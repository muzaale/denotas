{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08/01/2023\n",
    "\n",
    "### 825. [undergrads](https://jhutrc.github.io/fenagas/content/dramatispersonae/undergraduates/undergraduates.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number     Name                          \n",
      "1          Catherine Wood                \n",
      "2          Teresa Medina                 \n",
      "3          Deborah Kent                  \n",
      "4          Samuel Mccarthy               \n",
      "5          Victoria Walton               \n",
      "6          Jason Campbell                \n",
      "7          Jacqueline Hopkins            \n",
      "8          Christopher Baker             \n",
      "9          Shelia Miller                 \n",
      "10         Vincent Matthews              \n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "def generate_random_names(num_names):\n",
    "    fake = Faker()\n",
    "    names = [[fake.first_name(), fake.last_name()] for _ in range(num_names)]\n",
    "    return names\n",
    "\n",
    "def create_table(names):\n",
    "    header = [\"Number\", \"Name\"]\n",
    "    table = []\n",
    "    table.append(header)\n",
    "    for idx, name in enumerate(names, start=1):\n",
    "        full_name = \" \".join(name)\n",
    "        row = [idx, full_name]\n",
    "        table.append(row)\n",
    "\n",
    "    # Printing the table\n",
    "    for row in table:\n",
    "        print(f\"{row[0]:<10} {row[1]:<30}\")\n",
    "\n",
    "# Generate 10 random names and call the function\n",
    "random_names = generate_random_names(10)\n",
    "create_table(random_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 826. graçias &#128591;\n",
    "\n",
    "+ beta version of the fenagas webapp is up & running\n",
    "+ andrew & fawaz will be first to test it\n",
    "+ consider rolling it out to faculty in department \n",
    "+ philosophe, elliot, betsy, and with residents\n",
    "\n",
    "### 827. fenagas\n",
    "\n",
    "+ let ai say about `fena` \"annuŋŋamya mu makubo ago butukirivu\"\n",
    "+ ai can't produce such nuance on its own\n",
    "+ but, guided, it can\n",
    "\n",
    "### 828. music\n",
    "\n",
    "+ take me to church on apple music\n",
    "+ my fave playlist\n",
    "+ *savor it!*\n",
    "\n",
    "### 829. boards\n",
    "\n",
    "+ my scores are expired\n",
    "+ i need to retake them\n",
    "+ will be fun with ai \n",
    "+ consider a timeline\n",
    "+ and with fenagas,llc?\n",
    "+ just might have bandwidth\n",
    "+ but its the synthesis of the two\n",
    "+ that will be the most fun\n",
    "\n",
    "### 830. breakthru\n",
    "\n",
    "Act I: `Hypothesis` - Navigating the Realm of Ideas and Concepts\n",
    "\n",
    "In Act I, we embark on a journey of exploration, where ideas take center stage. We delve into the realm of hypotheses and concepts, laying the foundation for our scientific inquiry. From conceiving research questions to formulating testable propositions, Act I serves as the starting point of our intellectual pursuit. Through manuscripts, code, and Git, we learn to articulate and organize our ideas effectively, setting the stage for robust investigations and insightful discoveries.\n",
    "\n",
    "Act II: `Data` - Unveiling the Power of Information\n",
    "\n",
    "Act II unfolds as we dive into the realm of data, where raw information becomes the fuel for knowledge. Through the lenses of Python, AI, R, and Stata, we explore data collection, processing, and analysis. Act II empowers us to harness the potential of data and unleash its power in extracting meaningful insights. By mastering the tools to handle vast datasets and uncover patterns, Act II equips us to bridge the gap between theoretical hypotheses and empirical evidence.\n",
    "\n",
    "Act III: `Estimates` - Seeking Truth through Inference\n",
    "\n",
    "In Act III, we venture into the world of estimates, where statistical methods guide us in drawing meaningful conclusions. Nonparametric, semiparametric, parametric, and simulation techniques become our allies in the quest for truth. Act III enables us to infer population characteristics from sample data, making informed decisions and drawing reliable generalizations. Understanding the nuances of estimation empowers us to extract valuable information from limited observations, transforming data into actionable knowledge.\n",
    "\n",
    "Act IV: `Variance` - Grappling with Uncertainty\n",
    "\n",
    "Act IV brings us face to face with variance, where uncertainty and variability loom large. In the pursuit of truth, we encounter truth, rigor, error, sloppiness, and the unsettling specter of fraud. Act IV teaches us to navigate the intricacies of uncertainty, recognize the sources of variation, and identify potential pitfalls. By embracing variance, we fortify our methodologies, enhance the rigor of our analyses, and guard against errors and biases that may distort our findings.\n",
    "\n",
    "Act V: `Explanation` - Illuminating the \"Why\" behind the \"What\"\n",
    "\n",
    "Act V marks the pinnacle of our journey, where we seek to unravel the mysteries behind observed phenomena. Oneway, Twoway, Multivariable, Hierarchical, Clinical, and Public perspectives converge in a quest for understanding. Act V unfolds the rich tapestry of explanations, exploring causal relationships, uncovering hidden connections, and interpreting complex findings. By delving into the intricacies of explanation, Act V empowers us to communicate our discoveries, inspire new research avenues, and drive positive change in our scientific pursuits.\n",
    "\n",
    "Epilogue: Embracing the `Journey` of Knowledge\n",
    "\n",
    "In the Epilogue, we reflect on our expedition through Fenagas, celebrating the richness of knowledge and the evolution of our understanding. Open Science, Self-publishing, Published works, Grants, Proposals, and the interconnected world of Git & Spoke symbolize the culmination of our endeavors. Epilogue serves as a reminder of the ever-growing landscape of learning and the profound impact our contributions can have. Embracing the spirit of curiosity, we step forward, armed with newfound wisdom, to navigate the boundless seas of knowledge and ignite the flame of discovery in ourselves and others.\n",
    "\n",
    "### 831. fenagas\n",
    "\n",
    "+ each paper, manuscript, or project should have its own set of repos\n",
    "+ these will necessarily include a mixture of private and public repos\n",
    "+ private repos will be used for collaboration\n",
    "+ the public repos will be used for publication\n",
    "+ fenagas is a private company and recruitor\n",
    "+ so it will have its own set of repos as well\n",
    "+ but the science and research will have its own repos\n",
    "\n",
    "### 832. jerktaco\n",
    "\n",
    "+ oxtail\n",
    "+ jerk chicken\n",
    "+ `sweet` chilli-fried whole jerk snapper. is that a thing? quick google says yes.\n",
    "\n",
    "### 833. eddie\n",
    "\n",
    "Kadi and Mark…    \n",
    "+ The square root of the number of employees you employ will do most of the work… \n",
    "+ 5 classical composers created 95% of the classical music that’s played \n",
    "+ and yet if you look at their music, only 5% of their music is what’s played 95% of the time”…. \n",
    "+ Debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08/02/2023\n",
    "\n",
    "### 834. fena\n",
    "\n",
    "+ fawaz initally mistook persian and urdu for arabic\n",
    "+ and read them out but said they made no sense\n",
    "+ then recognized the \"middle one\" as arabic\n",
    "+ with the meaning that is intended\n",
    "+ but probably no idiomatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No.   Phrase                     Language         English Translation       \n",
      "----------------------------------------------------------------------------------\n",
      " 1     Eno yaffe ffena.           Luganda          Our and by us.            \n",
      " 2     Nuestro y por nosotros     Spanish          Ours and by us            \n",
      " 3     Le nôtre et par nous       French           Ours and by us            \n",
      " 4     Unser und von uns          German           Ours and by us            \n",
      " 5     Nostro e da noi            Italian          Ours and by us            \n",
      " 6     Nosso e por nós            Portuguese       Ours and by us            \n",
      " 7     Ons en door ons            Dutch            Ours and by us            \n",
      " 8     Наш и нами                 Russian          Ours and by us            \n",
      " 9     我们的，由我们提供                  Chinese          Ours and by us            \n",
      " 10    हमारा और हमसे              Nepali           Ours and by us            \n",
      " 11    نا و توسط ما               Persian          Ours and by us            \n",
      " 12    私たちのものであり、私たちによって          Japanese         Ours and by us            \n",
      " 13    لنا وبواسطتنا              Arabic           Ours and by us            \n",
      " 14    שלנו ועל ידינו             Hebrew           Ours and by us            \n",
      " 15    Yetu na kwa sisi           Swahili          Ours and by us            \n",
      " 16    Yetu futhi ngathi sisi     Zulu             Ours and like us          \n",
      " 17    Tiwa ni aṣẹ ati nipa wa    Yoruba           Ours and through us       \n",
      " 18    A ka na anyi               Igbo             Ours and by us            \n",
      " 19    Korean                     Korean           Ours and by us            \n",
      " 20    Meidän ja meidän toimesta  Finnish          Ours and by us            \n",
      " 21    ኦህድዎና በእኛ                  Amharic          Ours and by us            \n",
      " 22    Hinqabu fi hinqabu jechuun  Oromo            Ours and through us       \n",
      " 23    ምንም ነገርና እኛ በእኛ            Tigrinya         Nothing and by us         \n",
      " 24    हमारा और हमसे              Marathi          Ours and by us            \n",
      " 25    અમારા અને અમારા દ્વારા     Gujarati         Ours and by us            \n",
      " 26    ما و توسط ما               Urdu             Ours and by us            \n",
      " 27    우리 것이며, 우리에 의해             Korean           Ours and by us            \n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Eno yaffe ffena.\", \"Luganda\", \"Our and by us.\"),\n",
    "    (\"Nuestro y por nosotros\", \"Spanish\", \"Ours and by us\"),\n",
    "    (\"Le nôtre et par nous\", \"French\", \"Ours and by us\"),\n",
    "    (\"Unser und von uns\", \"German\", \"Ours and by us\"),\n",
    "    (\"Nostro e da noi\", \"Italian\", \"Ours and by us\"),\n",
    "    (\"Nosso e por nós\", \"Portuguese\", \"Ours and by us\"),\n",
    "    (\"Ons en door ons\", \"Dutch\", \"Ours and by us\"),\n",
    "    (\"Наш и нами\", \"Russian\", \"Ours and by us\"),\n",
    "    (\"我们的，由我们提供\", \"Chinese\", \"Ours and by us\"),\n",
    "    (\"हमारा और हमसे\", \"Nepali\", \"Ours and by us\"),\n",
    "    (\"نا و توسط ما\", \"Persian\", \"Ours and by us\"),\n",
    "    (\"私たちのものであり、私たちによって\", \"Japanese\", \"Ours and by us\"),\n",
    "    (\"لنا وبواسطتنا\", \"Arabic\", \"Ours and by us\"),\n",
    "    (\"שלנו ועל ידינו\", \"Hebrew\", \"Ours and by us\"),\n",
    "    (\"Yetu na kwa sisi\", \"Swahili\", \"Ours and by us\"),\n",
    "    (\"Yetu futhi ngathi sisi\", \"Zulu\", \"Ours and like us\"),\n",
    "    (\"Tiwa ni aṣẹ ati nipa wa\", \"Yoruba\", \"Ours and through us\"),\n",
    "    (\"A ka na anyi\", \"Igbo\", \"Ours and by us\"),\n",
    "    (\"Korean\", \"Korean\", \"Ours and by us\"),\n",
    "    (\"Meidän ja meidän toimesta\", \"Finnish\", \"Ours and by us\"),\n",
    "    (\"ኦህድዎና በእኛ\", \"Amharic\", \"Ours and by us\"),\n",
    "    (\"Hinqabu fi hinqabu jechuun\", \"Oromo\", \"Ours and through us\"),\n",
    "    (\"ምንም ነገርና እኛ በእኛ\", \"Tigrinya\", \"Nothing and by us\"),\n",
    "    (\"हमारा और हमसे\", \"Marathi\", \"Ours and by us\"),\n",
    "    (\"અમારા અને અમારા દ્વારા\", \"Gujarati\", \"Ours and by us\"),\n",
    "    (\"ما و توسط ما\", \"Urdu\", \"Ours and by us\"),\n",
    "    (\"우리 것이며, 우리에 의해\", \"Korean\", \"Ours and by us\"),  # New row for Korean\n",
    "]\n",
    "\n",
    "def print_table(data):\n",
    "    print(\" {:<4}  {:<25}  {:<15}  {:<25} \".format(\"No.\", \"Phrase\", \"Language\", \"English Translation\"))\n",
    "    print(\"\" + \"-\" * 6 + \"\" + \"-\" * 32 + \"\" + \"-\" * 17 + \"\" + \"-\" * 27 + \"\")\n",
    "    for idx, (phrase, language, translation) in enumerate(data, 1):\n",
    "        print(\" {:<4}  {:<25}  {:<15}  {:<25} \".format(idx, phrase, language, translation))\n",
    "\n",
    "print_table(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 835. kind\n",
    "\n",
    "The statement you've provided suggests that someone who enjoys being surrounded by like-minded individuals (kindred spirits) may not have the energy or drive to compete or challenge themselves against strong adversaries or opponents. This seems to express a perspective on complacency, comfort zones, and personal growth.     \n",
    "\n",
    "However, it's essential to understand that people have different paths to personal growth and self-fulfillment. While some thrive on challenge and adversity, finding it energizing and invigorating, others might find equal growth in collaboration, mutual understanding, and shared passions.     \n",
    "\n",
    "Realism, as the statement concludes, might refer to understanding and accepting these differences in human nature and personal paths, acknowledging that everyone has their unique journey and definition of success.   \n",
    "\n",
    "### 836. adversarial networks\n",
    "\n",
    "GANs pit two neural networks against each other in a kind of game, where one network (the generator) tries to create fake data that looks real, while the other (the discriminator) tries to tell real from fake.\n",
    "\n",
    "1. **Supervised Learning, $Y$**: Algorithm is trained on labeled data.\n",
    "2. **Unsupervised Learning, $X$**: Algorithm is trained on unlabeled data and looks for patterns.\n",
    "3. **Semi-Supervised Learning, $\\beta$**: Uses both labeled and unlabeled data for training.\n",
    "4. **Reinforcement Learning, $\\epsilon$**: Algorithm learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\n",
    "5. **Transfer Learning, $z$**: Using knowledge gained from one task to aid performance on a related, but different task.\n",
    "6. **Generative Adversarial Networks, $\\rho$**: A subset of unsupervised learning where two networks are trained together in a competitive fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of ML: Supervised\n",
      "Pros: Direct feedback, High accuracy with enough data\n",
      "Cons: Needs labeled data, Can overfit\n",
      "----------------------------------------\n",
      "Type of ML: Unsupervised\n",
      "Pros: Works with unlabeled data, Can uncover hidden patterns\n",
      "Cons: No feedback, Harder to verify results\n",
      "----------------------------------------\n",
      "Type of ML: Semi-Supervised\n",
      "Pros: Leverages large amounts of unlabeled data\n",
      "Cons: Needs some labeled data, Combines challenges of both supervised and unsupervised\n",
      "----------------------------------------\n",
      "Type of ML: Reinforcement\n",
      "Pros: Adapts to dynamic environments, Potential for real-time learning\n",
      "Cons: Requires careful reward design, Can be computationally expensive\n",
      "----------------------------------------\n",
      "Type of ML: Transfer\n",
      "Pros: Saves training time, Can leverage pre-trained models\n",
      "Cons: Not always straightforward, Domain differences can be an issue\n",
      "----------------------------------------\n",
      "Type of ML: GANs\n",
      "Pros: Generates new data, Can achieve impressive realism\n",
      "Cons: Training can be unstable, May require lots of data and time\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Type of ML\": [\"Supervised\", \"Unsupervised\", \"Semi-Supervised\", \"Reinforcement\", \"Transfer\", \"GANs\"],\n",
    "    \"Pros\": [\n",
    "        \"Direct feedback, High accuracy with enough data\",\n",
    "        \"Works with unlabeled data, Can uncover hidden patterns\",\n",
    "        \"Leverages large amounts of unlabeled data\",\n",
    "        \"Adapts to dynamic environments, Potential for real-time learning\",\n",
    "        \"Saves training time, Can leverage pre-trained models\",\n",
    "        \"Generates new data, Can achieve impressive realism\"\n",
    "    ],\n",
    "    \"Cons\": [\n",
    "        \"Needs labeled data, Can overfit\",\n",
    "        \"No feedback, Harder to verify results\",\n",
    "        \"Needs some labeled data, Combines challenges of both supervised and unsupervised\",\n",
    "        \"Requires careful reward design, Can be computationally expensive\",\n",
    "        \"Not always straightforward, Domain differences can be an issue\",\n",
    "        \"Training can be unstable, May require lots of data and time\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Type of ML: {row['Type of ML']}\\nPros: {row['Pros']}\\nCons: {row['Cons']}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 837. mbappé\n",
    "\n",
    "In the world of machine learning, there's an architecture called Generative Adversarial Networks (GANs). A GAN consists of two neural networks: a generator and a discriminator. The generator creates fake data, while the discriminator evaluates data to determine if it's real or generated by the generator. These networks are \"adversaries\", and they improve through their competition with one another. \n",
    "\n",
    "**Mbappé in Ligue 1 is like the generator in a GAN:**\n",
    "\n",
    "1. **Competitiveness (Lack of a Worthy Adversary)**: If the discriminator is too weak (akin to the other Ligue 1 teams compared to PSG), then the generator might produce data (or performance) that seems impressive in its context, but might not be as refined as it would be if it faced a stronger discriminator. Just as the EPL could serve as a more challenging discriminator for Mbappé, making him fine-tune his \"generation\" of skills, a stronger discriminator in a GAN forces the generator to produce higher-quality data.\n",
    "\n",
    "2. **Exposure to Challenges**: If Mbappé were in the EPL (a stronger discriminator), he'd face more frequent and varied challenges, pushing him to adapt and refine his skills, much like a generator improving its data generation when pitted against a robust discriminator.\n",
    "\n",
    "3. **Star Power & Champions League**: Just as Mbappé gets to face high-level competition in the Champions League and play alongside top talents in PSG, a generator can still produce high-quality data when trained with superior techniques or in combination with other skilled \"networks\", even if its regular discriminator isn't top-tier.\n",
    "\n",
    "4. **Future Moves & Evolution**: Over time, a GAN might be fine-tuned or paired with stronger discriminators. Similarly, Mbappé might move to a more competitive league in the future, facing \"stronger discriminators\" that challenge and refine his game further.\n",
    "\n",
    "In essence, for optimal growth and refinement, both a soccer player and a GAN benefit from being challenged regularly by worthy adversaries. PSG dominating Ligue 1 without a consistent worthy adversary might not push them to their absolute limits, just as a generator won't produce its best possible data without a strong discriminator to challenge it.\n",
    "\n",
    "### 838. lyrical\n",
    "\n",
    "+ kyrie eleison\n",
    "+ lord deliver me\n",
    "+ this is my exodus\n",
    "\n",
    "```unix\n",
    "He leads me beside still waters\n",
    "He restoreth my soul\n",
    "When you become a believer\n",
    "Your spirit is made right\n",
    "And sometimes, the soul doesn't get to notice\n",
    "It has a hole in it\n",
    "Due to things that's happened in the past\n",
    "Hurt, abuse, molestation\n",
    "But we wanna speak to you today and tell you\n",
    "That God wants to heal the hole in your soul\n",
    "Some people's actions are not because their spirit is wrong\n",
    "But it's because the past has left a hole in their soul\n",
    "May this wisdom help you get over your past\n",
    "And remind you that God wants to heal the hole in your soul\n",
    "I have my sister Le'Andria here\n",
    "She's gonna help me share this wisdom\n",
    "And tell this story\n",
    "Lord\n",
    "Deliver me, yeah\n",
    "'Cause all I seem to do is hurt me\n",
    "Hurt me, yeah\n",
    "Lord\n",
    "Deliver me\n",
    "'Cause all I seem to do is hurt me\n",
    "(Yes, sir)\n",
    "Hurt me, yeah, yeah\n",
    "(I know we should be finishing but)\n",
    "(Sing it for me two more times)\n",
    "Lord\n",
    "Deliver me, yeah\n",
    "'Cause all I seem to do is hurt me\n",
    "(Na-ha)\n",
    "Hurt me\n",
    "(One more time)\n",
    "Yeah\n",
    "Lord\n",
    "(Oh)\n",
    "Deliver me\n",
    "'Cause all I seem to do is hurt me, yeah\n",
    "Hurt me, yeah\n",
    "Whoa, yeah\n",
    "And my background said\n",
    "(Whoa-whoa, Lord)\n",
    "Oh yeah (deliver me)\n",
    "God rescued me from myself, from my overthinking\n",
    "If you're listening out there\n",
    "Just repeat after me if you're struggling with your past\n",
    "And say it\n",
    "(Oh, Lord, oh)\n",
    "Let the Lord know, just say it, oh\n",
    "(Oh, Lord, Lord)\n",
    "He wants to restore your soul\n",
    "He said\n",
    "(Deliver me)\n",
    "Hey\n",
    "If my people, who are called by my name\n",
    "Will move themselves and pray\n",
    "(Deliver me)\n",
    "Seek my face, turn from their wicked ways\n",
    "I will hear from Heaven\n",
    "Break it on down\n",
    "So it is\n",
    "It is so\n",
    "Amen\n",
    "Now when we pray\n",
    "Wanna end that with a declaration, a decree\n",
    "So I'm speaking for all of you listening\n",
    "Starting here, starting now\n",
    "The things that hurt you in the past won't control your future\n",
    "Starting now, this is a new day\n",
    "This is your exodus, you are officially released\n",
    "Now sing it for me Le'Andria\n",
    "Yeah\n",
    "(This is my Exodus)\n",
    "I'm saying goodbye\n",
    "(This is my Exodus)\n",
    "To the old me, yeah\n",
    "(This is my Exodus)\n",
    "Oh, oh, oh\n",
    "(Thank you, Lord)\n",
    "And I'm saying hello\n",
    "(Thank you, Lord)\n",
    "To the brand new me, yeah\n",
    "(Thank you, Lord)\n",
    "Yeah, yeah, yeah, yeah\n",
    "This is\n",
    "(This is my Exodus)\n",
    "I declare it\n",
    "(This is my Exodus)\n",
    "And I decree\n",
    "(This is my Exodus)\n",
    "Oh this is, this day, this day is why I thank you, Lord\n",
    "(This is my Exodus)\n",
    "(Thank you, Lord)\n",
    "Around\n",
    "(Thank you, Lord)\n",
    "For you and for me\n",
    "(Thank you, Lord)\n",
    "Yeah-hey-hey-yeah\n",
    "Now, Lord God\n",
    "(This is my Exodus)\n",
    "Now, Lord God\n",
    "(This is my Exodus)\n",
    "It is my\n",
    "(This is my Exodus)\n",
    "The things that sent to break me down\n",
    "(This is my Exodus)\n",
    "Hey-hey-hey, hey-hey-hey, hey-hey-hey, hey-yeah\n",
    "(Thank you, Lord)\n",
    "(Thank you, Lord)\n",
    "Every weapon\n",
    "(Thank you, Lord)\n",
    "God is you and to me, there for me\n",
    "Source: Musixmatch\n",
    "Songwriters: Donald Lawrence / Marshon Lewis / William James Stokes / Robert Woolridge / Desmond Davis\n",
    "```\n",
    "\n",
    "### 839. counterfeit\n",
    "\n",
    "In the context of competitive sports, the concept of \"generating fakes\" is indeed a fundamental aspect of gameplay. Athletes often use various techniques, such as dummies, side-steps, feints, or deceptive movements, to outwit their opponents and create opportunities for themselves or their teammates. These deceptive maneuvers act as the \"generator\" in the game, producing fake actions that challenge the opponent's perception and decision-making.\n",
    "\n",
    "Just like the generator in a GAN creates fake data to confuse the discriminator, athletes generate fake movements to deceive their opponents and gain an advantage. By presenting a range of possible actions, athletes keep their adversaries guessing and force them to make hasty decisions, potentially leading to mistakes or creating openings for an attack.\n",
    "\n",
    "The effectiveness of generating fakes lies in the balance between unpredictability and precision. Just as a GAN's generator must create data that is realistic enough to deceive the discriminator, athletes must execute their fakes with skill and timing to make them convincing and catch their opponents off guard.\n",
    "\n",
    "Moreover, much like how the discriminator in a GAN becomes stronger by learning from previous encounters, athletes also improve their \"discrimination\" skills over time by facing various opponents with different playing styles and tactics. The experience of playing against worthy adversaries enhances an athlete's ability to recognize and respond to deceptive movements, making them more refined in their decision-making and defensive actions.\n",
    "\n",
    "In summary, generating fakes in competitive sports is a crucial aspect that parallels the dynamics of Generative Adversarial Networks. Just as a GAN benefits from facing a strong discriminator to refine its data generation, athletes grow and excel when regularly challenged by worthy adversaries who can test their ability to produce deceptive movements and refine their gameplay to the highest level.\n",
    "\n",
    "### 840. music \n",
    "\n",
    "Composers in music, much like athletes in competitive sports and Generative Adversarial Networks (GANs), utilize the element of surprise and expectation to create captivating and emotionally engaging compositions. They play with the listener's anticipation, offering moments of tension and resolution, which add depth and excitement to the musical experience.\n",
    "\n",
    "In a musical composition, composers establish patterns, melodic motifs, and harmonic progressions that the listener subconsciously starts to expect. These expectations are the \"discriminator\" in this analogy, as they act as a reference point against which the composer can generate moments of tension and surprise, similar to the generator's role in a GAN.\n",
    "\n",
    "When a composer introduces a musical phrase that deviates from what the listener expects, it creates tension. This deviation can be through unexpected harmonies, dissonant intervals, rhythmic variations, or sudden changes in dynamics. This is akin to the \"fake data\" generated by the GAN's generator or the deceptive movements used by athletes to outwit their opponents.\n",
    "\n",
    "Just as a GAN's discriminator learns from previous encounters to recognize fake data better, listeners' musical discrimination skills improve over time as they become more familiar with different compositions and musical styles. As a result, composers must continually innovate and challenge the listener's expectations to keep the music engaging and fresh.\n",
    "\n",
    "The resolution in music, which ultimately satisfies the listener's expectations, is the equivalent of a GAN's generator producing data that appears realistic enough to deceive the discriminator successfully. Composers craft resolutions that give a sense of closure and fulfillment by returning to familiar themes, tonal centers, or melodic patterns.\n",
    "\n",
    "A well-composed musical piece strikes a balance between unexpected twists and satisfying resolutions. Too many surprises without resolution can leave listeners disoriented and unsatisfied, just as a GAN's generator may produce meaningless or unrealistic data. On the other hand, predictability without any element of surprise can result in boredom, both in music and in the world of sports.\n",
    "\n",
    "Let's illustrate this concept with a simple Python code snippet representing a musical script in the form of sheet music:\n",
    "\n",
    "```bash\n",
    "pip install music21\n",
    "```\n",
    "\n",
    "In this simple musical script, the notes and chords create an expected melody and progression in the key of C major. By introducing new harmonies or rhythms at strategic points, the composer can generate tension and surprise in the music, capturing the listener's attention. Ultimately, the music will return to familiar notes and chords, resolving the tension and providing a satisfying conclusion.\n",
    "\n",
    "In conclusion, just as GANs and competitive sports benefit from generating fakes and challenging adversaries, composers in music use the listener's expectations and create tension through deviations, only to resolve it with familiar elements, creating a rich and engaging musical experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0} <music21.tempo.MetronomeMark animato Quarter=120>\n",
      "{0.0} <music21.key.KeySignature of no sharps or flats>\n",
      "{0.0} <music21.meter.TimeSignature 4/4>\n",
      "{0.0} <music21.note.Note C>\n",
      "{1.0} <music21.note.Note D>\n",
      "{2.0} <music21.note.Note E>\n",
      "{3.0} <music21.note.Note F>\n",
      "{4.0} <music21.note.Note G>\n",
      "{5.0} <music21.note.Note A>\n",
      "{6.0} <music21.note.Note B>\n",
      "{7.0} <music21.note.Note C>\n",
      "{8.0} <music21.chord.Chord C E G>\n",
      "{9.0} <music21.chord.Chord F A C>\n",
      "{10.0} <music21.chord.Chord G B D>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from music21 import *\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set the path to the MuseScore executable\n",
    "musescore_path = '/Applications/MuseScore 4.app/Contents/MacOS/mscore'\n",
    "us = environment.UserSettings()\n",
    "us['musescoreDirectPNGPath'] = musescore_path\n",
    "\n",
    "# Create a score\n",
    "score = stream.Score()\n",
    "\n",
    "# Create a tempo\n",
    "tempo = tempo.MetronomeMark(number=120)\n",
    "\n",
    "# Create a key signature (C major)\n",
    "key_signature = key.KeySignature(0)\n",
    "\n",
    "# Create a time signature (4/4)\n",
    "time_signature = meter.TimeSignature('4/4')\n",
    "\n",
    "# Create a music stream\n",
    "music_stream = stream.Stream()\n",
    "\n",
    "# Add the tempo, key signature, and time signature to the music stream\n",
    "music_stream.append(tempo)\n",
    "music_stream.append(key_signature)\n",
    "music_stream.append(time_signature)\n",
    "\n",
    "# Define a list of note names\n",
    "notes = ['C', 'D', 'E', 'F', 'G', 'A', 'B', 'C5']\n",
    "\n",
    "# Create notes and add them to the music stream\n",
    "for note_name in notes:\n",
    "    new_note = note.Note(note_name, quarterLength=1)\n",
    "    music_stream.append(new_note)\n",
    "\n",
    "# Define a list of chords\n",
    "chords = [chord.Chord(['C', 'E', 'G']), chord.Chord(['F', 'A', 'C']), chord.Chord(['G', 'B', 'D'])]\n",
    "\n",
    "# Add chords to the music stream\n",
    "for c in chords:\n",
    "    music_stream.append(c)\n",
    "\n",
    "# Add the music stream to the score\n",
    "score.insert(0, music_stream)\n",
    "\n",
    "# Check the contents of the music_stream\n",
    "print(music_stream.show('text'))\n",
    "\n",
    "# Save the score as MusicXML and display it as a PNG image\n",
    "# musicxml_path = '/tmp/music21_example.musicxml'\n",
    "# png_path = '/tmp/music21_example.png'\n",
    "# score.write('musicxml.png', fp=musicxml_path)\n",
    "\n",
    "# Convert the MusicXML to a PNG image\n",
    "conv = converter.subConverters.ConverterMusicXML()\n",
    "# conv.write(score, 'png', png_path)\n",
    "\n",
    "# Display the PNG image\n",
    "# display(Image(filename=png_path))\n",
    "\n",
    "# Clean up temporary files\n",
    "# os.remove(musicxml_path)\n",
    "# os.remove(png_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 841. learning\n",
    "\n",
    "+ generative adversarial networks\n",
    "+ challenge-level, skill-level, and equiping students with the right tools to \"level up\"\n",
    "+ use this approach to create a \"learning\" GAN for any sort of course but starting with a course on Stata\n",
    "\n",
    "To design a Stata Programming class with the flexibility to adapt it into Python and R Programming classes, we can organize the content according to the provided headings in the `_toc.yml` file. We will structure the course into five acts, and each act will contain three to six scenes representing different chapters or topics. Each scene will be a learning module or topic that covers a specific aspect of Stata programming (and later Python and R programming).\n",
    "\n",
    "Let's begin by creating the `_toc.yml`:\n",
    "\n",
    "```yaml\n",
    "Skip to main content\n",
    "\n",
    "Have any feedback? Please participate in this survey\n",
    "Logo image\n",
    "Fenagas\n",
    "Prologue\n",
    "\n",
    "Act I\n",
    "Manuscripts\n",
    "Code\n",
    "Git\n",
    "\n",
    "Act II\n",
    "Python\n",
    "AI\n",
    "R\n",
    "Stata\n",
    "\n",
    "Act III\n",
    "Nonparametric\n",
    "Semiparametric\n",
    "Parametric\n",
    "Simulation\n",
    "Uses, abuses\n",
    "\n",
    "Act IV\n",
    "Truth\n",
    "Rigor\n",
    "Error\n",
    "Sloppiness\n",
    "Fraud\n",
    "Learning\n",
    "\n",
    "Act V\n",
    "Oneway\n",
    "Twoway\n",
    "Multivariable\n",
    "Hierarchical\n",
    "Clinical\n",
    "Public\n",
    "Epilogue\n",
    "\n",
    "Open Science\n",
    "Self publish\n",
    "Published\n",
    "Grants\n",
    "Proposals\n",
    "Git & Spoke\n",
    "\n",
    "Automate\n",
    "Bash\n",
    "Unix\n",
    "Courses\n",
    "\n",
    "Stata Programming\n",
    "```\n",
    "\n",
    "Now, let's create a brief description for each act and scene:\n",
    "\n",
    "Act I - Introduction to Research Manuscripts and Version Control\n",
    "\n",
    "Scene 1 - Understanding Research Manuscripts\n",
    "This scene will provide an overview of research manuscripts, their structure, and the importance of clear documentation in reproducible research.\n",
    "\n",
    "Scene 2 - Introduction to Code\n",
    "In this scene, we will introduce coding concepts, syntax, and the use of Stata, Python, and R as programming languages for data analysis.\n",
    "\n",
    "Scene 3 - Version Control with Git\n",
    "Students will learn the fundamentals of version control using Git, a powerful tool for tracking changes in code and collaborating with others.\n",
    "\n",
    "Act II - Exploring Data Analysis with Python, AI, R, and Stata\n",
    "\n",
    "Scene 1 - Python for Data Analysis\n",
    "This scene will cover basic data analysis tasks using Python, focusing on data manipulation, visualization, and statistical analysis.\n",
    "\n",
    "Scene 2 - Introduction to Artificial Intelligence (AI)\n",
    "Students will gain insights into AI concepts and applications, including machine learning, deep learning, and generative adversarial networks (GANs).\n",
    "\n",
    "Scene 3 - R for Data Science\n",
    "In this scene, we'll explore R's capabilities for data analysis, statistical modeling, and creating visualizations.\n",
    "\n",
    "Scene 4 - Introduction to Stata\n",
    "Students will be introduced to Stata programming, including data management, analysis, and graphing features.\n",
    "\n",
    "Act III - Advanced Topics in Data Analysis\n",
    "\n",
    "Scene 1 - Nonparametric Statistics\n",
    "This scene will delve into nonparametric statistical methods and their applications in various research scenarios.\n",
    "\n",
    "Scene 2 - Semiparametric Statistics\n",
    "Students will learn about semiparametric models and their advantages in handling complex data structures.\n",
    "\n",
    "Scene 3 - Parametric Modeling\n",
    "This scene will cover parametric statistical models and their assumptions, along with practical implementation in the chosen programming languages.\n",
    "\n",
    "Scene 4 - Simulation Techniques\n",
    "In this scene, students will learn about simulation methods to replicate observed data and explore \"what if\" scenarios in their analyses.\n",
    "\n",
    "Scene 5 - Data Analysis Uses and Abuses\n",
    "We will discuss common mistakes and pitfalls in data analysis, emphasizing the importance of data integrity and robustness.\n",
    "\n",
    "Act IV - Ensuring Data Quality and Integrity\n",
    "\n",
    "Scene 1 - Seeking Truth in Research\n",
    "This scene will highlight the importance of truth-seeking in research and the impact of biased results on scientific discoveries.\n",
    "\n",
    "Scene 2 - Rigorous Research Methods\n",
    "Students will learn about various rigorous research methodologies to ensure valid and reliable findings.\n",
    "\n",
    "Scene 3 - Identifying and Addressing Errors\n",
    "We will explore different types of errors in research and how to identify and correct them during the data analysis process.\n",
    "\n",
    "Scene 4 - Preventing Sloppiness in Analysis\n",
    "This scene will discuss best practices to avoid careless mistakes in data analysis that may compromise research outcomes.\n",
    "\n",
    "Scene 5 - Fraud Detection in Research\n",
    "Students will explore methods and approaches to detect and prevent fraud in clinical and public health research.\n",
    "\n",
    "Scene 6 - Learning from Data\n",
    "Drawing inspiration from Generative Adversarial Networks (GANs), this scene will encourage students to learn from data by simulating expected outcomes based on observed data.\n",
    "\n",
    "Act V - Advanced Data Visualization and Reporting\n",
    "\n",
    "Scene 1 - Oneway Plots and Scatterplots\n",
    "This scene will focus on creating oneway plots and scatterplots with jitter and overlapped mean and 95% CI bars to compare variables.\n",
    "\n",
    "Scene 2 - Twoway Plots and Multivariable Visualization\n",
    "We will cover twoway plots and multivariable visualizations to explore relationships between multiple variables.\n",
    "\n",
    "Scene 3 - Hierarchical Data Visualization\n",
    "Students will learn techniques for visualizing hierarchical data structures effectively.\n",
    "\n",
    "Scene 4 - Data Visualization in Clinical Research\n",
    "This scene will demonstrate visualization methods specifically tailored to clinical research scenarios.\n",
    "\n",
    "Scene 5 - Communicating Research to the Public\n",
    "In this final scene, we will explore effective ways to communicate research findings to the general public.\n",
    "\n",
    "Epilogue - Advancing Open Science and Professional Development\n",
    "\n",
    "Scene 1 - Embracing Open Science\n",
    "This scene will emphasize the importance of open science practices and promoting transparency in research.\n",
    "\n",
    "Scene 2 - Self-publishing and Open Access\n",
    "Students will learn about self-publishing options and the benefits of open access to research outputs.\n",
    "\n",
    "Scene 3 - Getting Published in Journals\n",
    "This scene will guide students through the process of getting research published in academic journals.\n",
    "\n",
    "Scene 4 - Grant Writing and Proposal Development\n",
    "We will cover essential aspects of grant writing and proposal development for research funding.\n",
    "\n",
    "Scene 5 - Git and Collaborative Workflows\n",
    "In this final scene, students will explore advanced Git workflows for collaborative programming projects.\n",
    "\n",
    "Automate - Bash and Unix Scripting for Automation\n",
    "This optional chapter will introduce students to automation using bash and Unix scripting.\n",
    "\n",
    "Courses - Explore Other Programming Courses\n",
    "Students will be provided with resources to explore more specialized programming courses beyond Stata, Python, and R.\n",
    "\n",
    "With this _toc.yml and the organized content for each part and scene, the Stata Programming class can be easily adapted into Python and R Programming classes by modifying the programming language-specific examples and exercises while keeping the underlying concepts and topics consistent.\n",
    "\n",
    "Based on the provided `_toc.yml` in the `jb-book` format, here is the redesigned Stata Programming class:\n",
    "\n",
    "---\n",
    "\n",
    "Please note that the content of each chapter may need to be written separately in the corresponding `.ipynb` or `.md` files. The organization of the class remains the same with five acts, each containing multiple scenes (chapters) covering different topics in Stata programming. You can add content and explanations in the respective files according to the intended class structure.\n",
    "\n",
    "---\n",
    "\n",
    "Sure, here's the redesigned Stata Programming class with all files as `.ipynb`:\n",
    "\n",
    "With this format, all chapters are now represented as `.ipynb` files, making it easier to create, manage, and access the content in Jupyter Notebook format. Please ensure that the content of each `.ipynb` file is written appropriately to deliver the Stata Programming class effectively.\n",
    "\n",
    "---\n",
    "\n",
    "Sure, here's the redesigned Stata Programming class with 10 scenes per act:\n",
    "\n",
    "```yaml\n",
    "Root: intro.ipynb\n",
    "Title: Fenagas\n",
    "\n",
    "Parts:\n",
    "- Caption:\n",
    "  Chapters:\n",
    "  - File: prologue.ipynb\n",
    "\n",
    "- Caption: Act I\n",
    "  Chapters:\n",
    "  - File: content/lessons/l1/act1_1.ipynb\n",
    "  - File: content/lessons/l1/act1_2.ipynb\n",
    "  - File: content/lessons/l1/act1_3.ipynb\n",
    "  - File: content/lessons/l1/act1_4.ipynb\n",
    "  - File: content/lessons/l1/act1_5.ipynb\n",
    "  - File: content/lessons/l1/act1_6.ipynb\n",
    "  - File: content/lessons/l1/act1_7.ipynb\n",
    "  - File: content/lessons/l1/act1_8.ipynb\n",
    "  - File: content/lessons/l1/act1_9.ipynb\n",
    "  - File: content/lessons/l1/act1_10.ipynb\n",
    "\n",
    "- Caption: Act II\n",
    "  Chapters:\n",
    "  - File: content/lessons/l2/act2_1.ipynb\n",
    "  - File: content/lessons/l2/act2_2.ipynb\n",
    "  - File: content/lessons/l2/act2_3.ipynb\n",
    "  - File: content/lessons/l2/act2_4.ipynb\n",
    "  - File: content/lessons/l2/act2_5.ipynb\n",
    "  - File: content/lessons/l2/act2_6.ipynb\n",
    "  - File: content/lessons/l2/act2_7.ipynb\n",
    "  - File: content/lessons/l2/act2_8.ipynb\n",
    "  - File: content/lessons/l2/act2_9.ipynb\n",
    "  - File: content/lessons/l2/act2_10.ipynb\n",
    "\n",
    "- Caption: Act III\n",
    "  Chapters:\n",
    "  - File: content/lessons/l3/act3_1.ipynb\n",
    "  - File: content/lessons/l3/act3_2.ipynb\n",
    "  - File: content/lessons/l3/act3_3.ipynb\n",
    "  - File: content/lessons/l3/act3_4.ipynb\n",
    "  - File: content/lessons/l3/act3_5.ipynb\n",
    "  - File: content/lessons/l3/act3_6.ipynb\n",
    "  - File: content/lessons/l3/act3_7.ipynb\n",
    "  - File: content/lessons/l3/act3_8.ipynb\n",
    "  - File: content/lessons/l3/act3_9.ipynb\n",
    "  - File: content/lessons/l3/act3_10.ipynb\n",
    "\n",
    "- Caption: Act IV\n",
    "  Chapters:\n",
    "  - File: content/lessons/l4/act4_1.ipynb\n",
    "  - File: content/lessons/l4/act4_2.ipynb\n",
    "  - File: content/lessons/l4/act4_3.ipynb\n",
    "  - File: content/lessons/l4/act4_4.ipynb\n",
    "  - File: content/lessons/l4/act4_5.ipynb\n",
    "  - File: content/lessons/l4/act4_6.ipynb\n",
    "  - File: content/lessons/l4/act4_7.ipynb\n",
    "  - File: content/lessons/l4/act4_8.ipynb\n",
    "  - File: content/lessons/l4/act4_9.ipynb\n",
    "  - File: content/lessons/l4/act4_10.ipynb\n",
    "\n",
    "- Caption: Act V\n",
    "  Chapters:\n",
    "  - File: content/lessons/l5/act5_1.ipynb\n",
    "  - File: content/lessons/l5/act5_2.ipynb\n",
    "  - File: content/lessons/l5/act5_3.ipynb\n",
    "  - File: content/lessons/l5/act5_4.ipynb\n",
    "  - File: content/lessons/l5/act5_5.ipynb\n",
    "  - File: content/lessons/l5/act5_6.ipynb\n",
    "  - File: content/lessons/l5/act5_7.ipynb\n",
    "  - File: content/lessons/l5/act5_8.ipynb\n",
    "  - File: content/lessons/l5/act5_9.ipynb\n",
    "  - File: content/lessons/l5/act5_10.ipynb\n",
    "\n",
    "- Caption: Epilogue\n",
    "  Chapters:\n",
    "  - File: content/lessons/l6/epi_1.ipynb\n",
    "  - File: content/lessons/l6/epi_2.ipynb\n",
    "  - File: content/lessons/l6/epi_3.ipynb\n",
    "  - File: content/lessons/l6/epi_4.ipynb\n",
    "  - File: content/lessons/l6/epi_5.ipynb\n",
    "  - File: content/lessons/l6/epi_6.ipynb\n",
    "  - File: content/lessons/l6/epi_7.ipynb\n",
    "  - File: content/lessons/l6/epi_8.ipynb\n",
    "  - File: content/lessons/l6/epi_9.ipynb\n",
    "  - File: content/lessons/l6/epi_10.ipynb\n",
    "\n",
    "- Caption: Git & Spoke\n",
    "  Chapters:\n",
    "  - File: content/lessons/l7/act7_1.ipynb\n",
    "  - File: content/lessons/l7/act7_2.ipynb\n",
    "  - File: content/lessons/l7/act7_3.ipynb\n",
    "  - File: content/lessons/l7/act7_4.ipynb\n",
    "  - File: content/lessons/l7/act7_5.ipynb\n",
    "  - File: content/lessons/l7/act7_6.ipynb\n",
    "  - File: content/lessons/l7/act7_7.ipynb\n",
    "  - File: content/lessons/l7/act7_8.ipynb\n",
    "  - File: content/lessons/l7/act7_9.ipynb\n",
    "  - File: content/lessons/l7/act7_10.ipynb\n",
    "\n",
    "- Caption: Feedback\n",
    "  Chapters:\n",
    "  - File: content/lessons/1_survey9.ipynb\n",
    "\n",
    "- Caption: About Fena\n",
    "  Chapters:\n",
    "  - File: content/lessons/about/cite.ipynb\n",
    "  - File: content/lessons/about/changelog.ipynb\n",
    "\n",
    "- Caption: Courses\n",
    "  Chapters:\n",
    "  - URL: https://publichealth.jhu.edu/courses\n",
    "    Title: Stata Programming\n",
    "  - File: content/dramatispersonae/high_school_students/high_school_students.ipynb\n",
    "  - File: content/dramatispersonae/undergraduates/undergraduates.ipynb\n",
    "  - File: content/dramatispersonae/graduate_students/graduate_students.ipynb\n",
    "  - File: content/dramatispersonae/medical_students/medical_students.ipynb\n",
    "```\n",
    "\n",
    "Sure! Below is a bash script named `stataclass.sh` that creates the folder structure and places the `.ipynb` files in the appropriate folders:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Create folders\n",
    "mkdir -p content/lessons/l1\n",
    "mkdir -p content/lessons/l2\n",
    "mkdir -p content/lessons/l3\n",
    "mkdir -p content/lessons/l4\n",
    "mkdir -p content/lessons/l5\n",
    "mkdir -p content/lessons/l6\n",
    "mkdir -p content/lessons/l7\n",
    "mkdir -p content/dramatispersonae/high_school_students\n",
    "mkdir -p content/dramatispersonae/undergraduates\n",
    "mkdir -p content/dramatispersonae/graduate_students\n",
    "mkdir -p content/dramatispersonae/medical_students\n",
    "mkdir -p content/dramatispersonae/residents\n",
    "mkdir -p content/dramatispersonae/fellows\n",
    "mkdir -p content/dramatispersonae/faculty\n",
    "mkdir -p content/dramatispersonae/analysts\n",
    "mkdir -p content/dramatispersonae/staff\n",
    "mkdir -p content/dramatispersonae/collaborators\n",
    "mkdir -p content/dramatispersonae/graduates\n",
    "mkdir -p content/dramatispersonae/high_school_students/calvin_mathews\n",
    "mkdir -p content/dramatispersonae/medical_students/andrew_arking\n",
    "mkdir -p content/lessons/l6\n",
    "mkdir -p content/lessons/l7\n",
    "\n",
    "# Place the .ipynb files in appropriate folders\n",
    "cp prologue.ipynb content/\n",
    "cp content/lessons/l1/act1_1.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_2.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_3.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_4.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_5.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_6.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_7.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_8.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_9.ipynb content/lessons/l1/\n",
    "cp content/lessons/l1/act1_10.ipynb content/lessons/l1/\n",
    "\n",
    "# Continue copying .ipynb files to other folders as required...\n",
    "\n",
    "echo \"Folder structure and files have been created successfully.\"\n",
    "```\n",
    "\n",
    "This script will create all the necessary folders and copy the `.ipynb` files to their respective locations, as specified in the `_toc.yml` format. Note that you'll need to manually complete the script by copying the remaining `.ipynb` files to the corresponding folders based on the `_toc.yml`. Save the script and run it to create the desired folder structure and files.\n",
    "\n",
    "Absolutely! You are right; there is a more elegant and efficient way to create the folders and files using a bash loop. Here's the updated version of the `stataclass.sh` script using your suggestion:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Create folders\n",
    "mkdir -p content/lessons/l{1..7}\n",
    "mkdir -p content/dramatispersonae/{high_school_students,undergraduates,graduate_students,medical_students,residents,fellows,faculty,analysts,staff,collaborators,graduates}\n",
    "mkdir -p content/dramatispersonae/high_school_students/calvin_mathews\n",
    "mkdir -p content/dramatispersonae/medical_students/andrew_arking\n",
    "\n",
    "# Place the .ipynb files in appropriate folders\n",
    "cp prologue.ipynb content/\n",
    "cp content/lessons/l1/act1_{1..10}.ipynb content/lessons/l1/\n",
    "\n",
    "# Continue copying .ipynb files to other folders as required...\n",
    "\n",
    "echo \"Folder structure and files have been created successfully.\"\n",
    "```\n",
    "\n",
    "This script uses brace expansion in the `mkdir` command to create the necessary folders in a more concise way. It also utilizes the bash loop with the array to create the `.ipynb` files for the `dramatispersonae` folder. It is both shorter and more readable, making it a more elegant solution. Just make sure to continue copying the remaining `.ipynb` files to the corresponding folders as specified in the `_toc.yml` format.\n",
    "\n",
    "### 842. act1\n",
    "\n",
    "+ give students a tool to learn how to learn\n",
    "+ a template of an entire manuscript.ipynb file\n",
    "+ the python, ai, r, and stata programming scripts that support the manuscript.ipynb file\n",
    "+ step-by-step instructions on creating a github account, a public, and private repository\n",
    "+ push content to the public repository and use gh-pages to publish the content\n",
    "\n",
    "### 843. streamline\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Change the working directory to the desired location\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Uncomment the following line if you need to create the \"three40\" directory\n",
    "# nano three40.sh & paste the contents of the three40.sh file\n",
    "# chmod +x three40.sh\n",
    "# mkdir three40\n",
    "# cd three40\n",
    "# nano _toc.yml & paste the contents of the _toc.yml file\n",
    "\n",
    "# Create the root folder\n",
    "# mkdir -p three40\n",
    "\n",
    "# Create the \"intro.ipynb\" file inside the \"root\" folder\n",
    "touch three40/intro.ipynb\n",
    "\n",
    "# Function to read the chapters from the YAML file using pure bash\n",
    "get_chapters_from_yaml() {\n",
    "  local part=\"$1\"\n",
    "  local toc_file=\"_toc.yml\"\n",
    "  local lines\n",
    "  local in_part=false\n",
    "\n",
    "  while read -r line; do\n",
    "    if [[ \"$line\" == *\"$part\"* ]]; then\n",
    "      in_part=true\n",
    "    elif [[ \"$line\" == *\"- File: \"* ]]; then\n",
    "      if \"$in_part\"; then\n",
    "        echo \"$line\" | awk -F': ' '{print $2}' | tr -d ' '\n",
    "      fi\n",
    "    elif [[ \"$line\" == *\"-\"* ]]; then\n",
    "      in_part=false\n",
    "    fi\n",
    "  done < \"$toc_file\"\n",
    "}\n",
    "\n",
    "# Create parts and chapters based on the _toc.yml structure\n",
    "parts=(\n",
    "  \"Act I\"\n",
    "  \"Act II\"\n",
    "  \"Act III\"\n",
    "  \"Act IV\"\n",
    "  \"Act V\"\n",
    "  \"Epilogue\"\n",
    "  \"Git & Spoke\"\n",
    "  \"Courses\"\n",
    ")\n",
    "\n",
    "# Loop through parts and create chapters inside each part folder\n",
    "for part in \"${parts[@]}\"; do\n",
    "  part_folder=\"three40/$part\"\n",
    "  mkdir -p \"$part_folder\"\n",
    "\n",
    "  # Get the chapters for the current part from the _toc.yml\n",
    "  chapters=($(get_chapters_from_yaml \"$part\"))\n",
    "\n",
    "  # Create chapter files inside the part folder\n",
    "  for chapter in \"${chapters[@]}\"; do\n",
    "    touch \"$part_folder/$chapter\"\n",
    "  done\n",
    "done\n",
    "\n",
    "# Create folders for dramatispersonae\n",
    "files=(\n",
    "  \"high_school_students/high_school_students.ipynb\"\n",
    "  \"undergraduates/undergraduates.ipynb\"\n",
    "  \"graduate_students/graduate_students.ipynb\"\n",
    "  \"medical_students/medical_students.ipynb\"\n",
    "  \"residents/residents.ipynb\"\n",
    "  \"fellows/fellows.ipynb\"\n",
    "  \"faculty/faculty.ipynb\"\n",
    "  \"analysts/analysts.ipynb\"\n",
    "  \"staff/staff.ipynb\"\n",
    "  \"collaborators/collaborators.ipynb\"\n",
    "  \"graduates/graduates.ipynb\"\n",
    "  \"high_school_students/calvin_mathews/calvin_mathews.ipynb\"\n",
    "  \"medical_students/andrew_arking/andrew_arking.ipynb\"\n",
    "  \"medical_students/andrew_arking/andrew_arking_1.ipynb\"\n",
    "  \"collaborators/fawaz_al_ammary/fawaz_al_ammary.ipynb\"\n",
    "  \"collaborators/fawaz_al_ammary/fawaz_al_ammary_1.ipynb\"\n",
    ")\n",
    "\n",
    "# Loop through the file paths and create the corresponding directories\n",
    "for file_path in \"${files[@]}\"; do\n",
    "  # Remove the common prefix \"content/dramatispersonae/\" from the file path\n",
    "  dir_path=${file_path#content/dramatispersonae/}\n",
    "  \n",
    "  # Create the directory\n",
    "  mkdir -p \"three40/content/dramatispersonae/$dir_path\"\n",
    "done\n",
    "\n",
    "echo \"Folder structure has been created successfully.\"\n",
    "\n",
    "```\n",
    "\n",
    "```yml\n",
    "Root: intro.ipynb\n",
    "Title: Fenagas\n",
    "\n",
    "Parts:\n",
    "- Caption:\n",
    "  Chapters:\n",
    "  - File: prologue.ipynb\n",
    "\n",
    "- Caption: Act I\n",
    "  Chapters:\n",
    "  - File: content/lessons/l1/act1_1.ipynb\n",
    "  - File: content/lessons/l1/act1_2.ipynb\n",
    "  - File: content/lessons/l1/act1_3.ipynb\n",
    "  - File: content/lessons/l1/act1_4.ipynb\n",
    "  - File: content/lessons/l1/act1_5.ipynb\n",
    "  - File: content/lessons/l1/act1_6.ipynb\n",
    "  - File: content/lessons/l1/act1_7.ipynb\n",
    "  - File: content/lessons/l1/act1_8.ipynb\n",
    "  - File: content/lessons/l1/act1_9.ipynb\n",
    "  - File: content/lessons/l1/act1_10.ipynb\n",
    "\n",
    "- Caption: Act II\n",
    "  Chapters:\n",
    "  - File: content/lessons/l2/act2_1.ipynb\n",
    "  - File: content/lessons/l2/act2_2.ipynb\n",
    "  - File: content/lessons/l2/act2_3.ipynb\n",
    "  - File: content/lessons/l2/act2_4.ipynb\n",
    "  - File: content/lessons/l2/act2_5.ipynb\n",
    "  - File: content/lessons/l2/act2_6.ipynb\n",
    "  - File: content/lessons/l2/act2_7.ipynb\n",
    "  - File: content/lessons/l2/act2_8.ipynb\n",
    "  - File: content/lessons/l2/act2_9.ipynb\n",
    "  - File: content/lessons/l2/act2_10.ipynb\n",
    "\n",
    "- Caption: Act III\n",
    "  Chapters:\n",
    "  - File: content/lessons/l3/act3_1.ipynb\n",
    "  - File: content/lessons/l3/act3_2.ipynb\n",
    "  - File: content/lessons/l3/act3_3.ipynb\n",
    "  - File: content/lessons/l3/act3_4.ipynb\n",
    "  - File: content/lessons/l3/act3_5.ipynb\n",
    "  - File: content/lessons/l3/act3_6.ipynb\n",
    "  - File: content/lessons/l3/act3_7.ipynb\n",
    "  - File: content/lessons/l3/act3_8.ipynb\n",
    "  - File: content/lessons/l3/act3_9.ipynb\n",
    "  - File: content/lessons/l3/act3_10.ipynb\n",
    "\n",
    "- Caption: Act IV\n",
    "  Chapters:\n",
    "  - File: content/lessons/l4/act4_1.ipynb\n",
    "  - File: content/lessons/l4/act4_2.ipynb\n",
    "  - File: content/lessons/l4/act4_3.ipynb\n",
    "  - File: content/lessons/l4/act4_4.ipynb\n",
    "  - File: content/lessons/l4/act4_5.ipynb\n",
    "  - File: content/lessons/l4/act4_6.ipynb\n",
    "  - File: content/lessons/l4/act4_7.ipynb\n",
    "  - File: content/lessons/l4/act4_8.ipynb\n",
    "  - File: content/lessons/l4/act4_9.ipynb\n",
    "  - File: content/lessons/l4/act4_10.ipynb\n",
    "\n",
    "- Caption: Act V\n",
    "  Chapters:\n",
    "  - File: content/lessons/l5/act5_1.ipynb\n",
    "  - File: content/lessons/l5/act5_2.ipynb\n",
    "  - File: content/lessons/l5/act5_3.ipynb\n",
    "  - File: content/lessons/l5/act5_4.ipynb\n",
    "  - File: content/lessons/l5/act5_5.ipynb\n",
    "  - File: content/lessons/l5/act5_6.ipynb\n",
    "  - File: content/lessons/l5/act5_7.ipynb\n",
    "  - File: content/lessons/l5/act5_8.ipynb\n",
    "  - File: content/lessons/l5/act5_9.ipynb\n",
    "  - File: content/lessons/l5/act5_10.ipynb\n",
    "\n",
    "- Caption: Epilogue\n",
    "  Chapters:\n",
    "  - File: content/lessons/l6/epi_1.ipynb\n",
    "  - File: content/lessons/l6/epi_2.ipynb\n",
    "  - File: content/lessons/l6/epi_3.ipynb\n",
    "  - File: content/lessons/l6/epi_4.ipynb\n",
    "  - File: content/lessons/l6/epi_5.ipynb\n",
    "  - File: content/lessons/l6/epi_6.ipynb\n",
    "  - File: content/lessons/l6/epi_7.ipynb\n",
    "  - File: content/lessons/l6/epi_8.ipynb\n",
    "  - File: content/lessons/l6/epi_9.ipynb\n",
    "  - File: content/lessons/l6/epi_10.ipynb\n",
    "\n",
    "- Caption: Git & Spoke\n",
    "  Chapters:\n",
    "  - File: content/lessons/l7/act7_1.ipynb\n",
    "  - File: content/lessons/l7/act7_2.ipynb\n",
    "  - File: content/lessons/l7/act7_3.ipynb\n",
    "  - File: content/lessons/l7/act7_4.ipynb\n",
    "  - File: content/lessons/l7/act7_5.ipynb\n",
    "  - File: content/lessons/l7/act7_6.ipynb\n",
    "  - File: content/lessons/l7/act7_7.ipynb\n",
    "  - File: content/lessons/l7/act7_8.ipynb\n",
    "  - File: content/lessons/l7/act7_9.ipynb\n",
    "  - File: content/lessons/l7/act7_10.ipynb\n",
    "\n",
    "- Caption: Courses\n",
    "  Chapters:\n",
    "  - URL: https://publichealth.jhu.edu/courses\n",
    "    Title: Stata Programming\n",
    "  - file: content/dramatispersonae/high_school_students/high_school_students.ipynb\n",
    "  - file: content/dramatispersonae/undergraduates/undergraduates.ipynb\n",
    "  - file: content/dramatispersonae/graduate_students/graduate_students.ipynb\n",
    "  - file: content/dramatispersonae/medical_students/medical_students.ipynb\n",
    "  - file: content/dramatispersonae/residents/residents.ipynb\n",
    "  - file: content/dramatispersonae/fellows/fellows.ipynb\n",
    "  - file: content/dramatispersonae/faculty/faculty.ipynb\n",
    "  - file: content/dramatispersonae/analysts/analysts.ipynb\n",
    "  - file: content/dramatispersonae/staff/staff.ipynb\n",
    "  - file: content/dramatispersonae/collaborators/collaborators.ipynb\n",
    "  - file: content/dramatispersonae/graduates/graduates.ipynb\n",
    "  - file: content/dramatispersonae/high_school_students/calvin_mathews/calvin_mathews.ipynb\n",
    "  - file: content/dramatispersonae/medical_students/andrew_arking/andrew_arking.ipynb\n",
    "  - file: content/dramatispersonae/medical_students/andrew_arking/andrew_arking_1.ipynb\n",
    "  - file: content/dramatispersonae/collaborators/fawaz_al_ammary/fawaz_al_ammary.ipynb\n",
    "  - file: content/dramatispersonae/collaborators/fawaz_al_ammary/fawaz_al_ammary_1.ipynb\n",
    "```\n",
    "\n",
    "### 844. revolution\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Step 1: Navigate to the '1f.ἡἔρις,κ' directory in the 'dropbox' folder\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Step 2: Create and edit the 'three40.sh' file using 'nano'\n",
    "nano three40.sh\n",
    "\n",
    "# Step 3: Add execute permissions to the 'three40.sh' script\n",
    "chmod +x three40.sh\n",
    "\n",
    "# Step 4: Run the 'three40.sh' script\n",
    "./three40.sh\n",
    "\n",
    "# Step 5: Create the 'three40' directory\n",
    "mkdir three40\n",
    "\n",
    "# Step 6: Navigate to the 'three40' directory\n",
    "cd three40\n",
    "\n",
    "# Step 7: Create and edit the '_toc.yml' file using 'nano'\n",
    "nano _toc.yml\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```md\n",
    "three40/\n",
    "├── intro.ipynb\n",
    "├── prologue.ipynb\n",
    "├── content/\n",
    "│   └── lessons/\n",
    "│       └── l1/\n",
    "│           ├── act1_1.ipynb\n",
    "│           ├── act1_2.ipynb\n",
    "│           ├── act1_3.ipynb\n",
    "│           └── ...\n",
    "│       └── l2/\n",
    "│           ├── act2_1.ipynb\n",
    "│           ├── act2_2.ipynb\n",
    "│           └── ...\n",
    "│       └── ...\n",
    "│       └── l7/\n",
    "│           ├── act7_1.ipynb\n",
    "│           ├── act7_2.ipynb\n",
    "│           └── ...\n",
    "├── dramatispersonae/\n",
    "│   └── high_school_students/\n",
    "│       └── ...\n",
    "│   └── undergraduates/\n",
    "│       └── ...\n",
    "│   └── ...\n",
    "│   └── graduates/\n",
    "│       └── ...\n",
    "└── ...\n",
    "\n",
    "```\n",
    "\n",
    "### 845. yml\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Change the working directory to the desired location\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Uncomment the following line if you need to create the \"three40\" directory\n",
    "# nano three40.sh & paste the contents of the three40.sh file\n",
    "# chmod +x three40.sh\n",
    "# mkdir three40\n",
    "# cd three40\n",
    "# Create the root folder\n",
    "mkdir -p three40\n",
    "\n",
    "# nano three40/_toc.yml & paste the contents of the _toc.yml file\n",
    "\n",
    "# Create the \"intro.ipynb\" file inside the \"root\" folder\n",
    "touch three40/intro.ipynb\n",
    "\n",
    "# Function to read the chapters from the YAML file using pure bash\n",
    "get_chapters_from_yaml() {\n",
    "  local part=\"$1\"\n",
    "  local toc_file=\"three40/_toc.yml\"\n",
    "  local lines\n",
    "  local in_part=false\n",
    "\n",
    "  while read -r line; do\n",
    "    if [[ \"$line\" == *\"$part\"* ]]; then\n",
    "      in_part=true\n",
    "    elif [[ \"$line\" == *\"- File: \"* ]]; then\n",
    "      if \"$in_part\"; then\n",
    "        echo \"$line\" | awk -F': ' '{print $2}' | tr -d ' '\n",
    "      fi\n",
    "    elif [[ \"$line\" == *\"-\"* ]]; then\n",
    "      in_part=false\n",
    "    fi\n",
    "  done < \"$toc_file\"\n",
    "}\n",
    "\n",
    "# Create parts and chapters based on the _toc.yml structure\n",
    "parts=(\n",
    "  \"Act I\"\n",
    "  \"Act II\"\n",
    "  \"Act III\"\n",
    "  \"Act IV\"\n",
    "  \"Act V\"\n",
    "  \"Epilogue\"\n",
    "  \"Git & Spoke\"\n",
    "  \"Courses\"\n",
    ")\n",
    "\n",
    "# Loop through parts and create chapters inside each part folder\n",
    "for part in \"${parts[@]}\"; do\n",
    "  part_folder=\"three40/$part\"\n",
    "  mkdir -p \"$part_folder\"\n",
    "\n",
    "  # Get the chapters for the current part from the _toc.yml\n",
    "  chapters=($(get_chapters_from_yaml \"$part\"))\n",
    "\n",
    "  # Create chapter files inside the part folder\n",
    "  for chapter in \"${chapters[@]}\"; do\n",
    "    touch \"$part_folder/$chapter.ipynb\"\n",
    "  done\n",
    "done\n",
    "\n",
    "echo \"Folder structure has been created successfully.\"\n",
    "\n",
    "```\n",
    "\n",
    "### 846. iteration~30\n",
    "\n",
    "#### 846.1. structure\n",
    "\n",
    "Based on the provided information and incorporating the details under the \"dramatispersonae\" folder, the entire \"three40/\" directory structure will look like this:\n",
    "\n",
    "```bash\n",
    "three40/\n",
    "├── intro.ipynb\n",
    "├── prologue.ipynb\n",
    "├── Act I/\n",
    "│   ├── act1_1.ipynb\n",
    "│   ├── act1_2.ipynb\n",
    "│   ├── act1_3.ipynb\n",
    "│   └── ...\n",
    "├── Act II/\n",
    "│   ├── act2_1.ipynb\n",
    "│   ├── act2_2.ipynb\n",
    "│   └── ...\n",
    "├── Act III/\n",
    "│   ├── act3_1.ipynb\n",
    "│   ├── act3_2.ipynb\n",
    "│   ├── act3_3.ipynb\n",
    "│   ├── act3_4.ipynb\n",
    "│   └── act3_5.ipynb\n",
    "├── Act IV/\n",
    "│   ├── act4_1.ipynb\n",
    "│   ├── act4_2.ipynb\n",
    "│   ├── act4_3.ipynb\n",
    "│   ├── act4_4.ipynb\n",
    "│   ├── act4_5.ipynb\n",
    "│   └── act4_6.ipynb\n",
    "├── Act V/\n",
    "│   ├── act5_1.ipynb\n",
    "│   ├── act5_2.ipynb\n",
    "│   ├── act5_3.ipynb\n",
    "│   ├── act5_4.ipynb\n",
    "│   ├── act5_5.ipynb\n",
    "│   └── act5_6.ipynb\n",
    "├── Epilogue/\n",
    "│   ├── epi_1.ipynb\n",
    "│   ├── epi_2.ipynb\n",
    "│   ├── epi_3.ipynb\n",
    "│   ├── epi_4.ipynb\n",
    "│   ├── epi_5.ipynb\n",
    "│   ├── epi_6.ipynb\n",
    "│   ├── epi_7.ipynb\n",
    "│   └── epi_8.ipynb\n",
    "├── Gas & Spoke/\n",
    "│   ├── gas_1.ipynb\n",
    "│   ├── gas_2.ipynb\n",
    "│   └── gas_3.ipynb\n",
    "└── dramatispersonae/\n",
    "    ├── high_school_students/\n",
    "    │   ├── high_school_students_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_4/\n",
    "    │   │   └── ...\n",
    "    │   └── high_school_students_5/\n",
    "    │       └── ...\n",
    "    ├── undergraduates/\n",
    "    │   ├── undergraduates_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── undergraduates_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── undergraduates_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── undergraduates_4/\n",
    "    │   │   └── ...\n",
    "    │   └── undergraduates_5/\n",
    "    │       └── ...\n",
    "    ├── graduates/\n",
    "    │   ├── graduates_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_4/\n",
    "    │   │   └── ...\n",
    "    │   └── graduates_5/\n",
    "    │       └── ...\n",
    "    ├── medical_students/\n",
    "    │   ├── medical_students_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_4/\n",
    "    │   │   └── ...\n",
    "    │   └── medical_students_5/\n",
    "    │       └── ...\n",
    "    ├── residents/\n",
    "    │   ├── residents_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_4/\n",
    "    │   │   └── ...\n",
    "    │   └── residents_5/\n",
    "    │       └── ...\n",
    "    ├── fellows/\n",
    "    │   ├── fellows_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_4/\n",
    "    │   │   └── ...\n",
    "    │   └── fellows_5/\n",
    "    │       └── ...\n",
    "    ├── faculty/\n",
    "    │   ├── faculty_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_4/\n",
    "    │   │   └── ...\n",
    "    │   └── faculty_5/\n",
    "    │       └── ...\n",
    "    ├── analysts/\n",
    "    │   ├── analysts_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_4/\n",
    "    │   │   └── ...\n",
    "    │   └── analysts_5/\n",
    "    │       └── ...\n",
    "    ├── staff/\n",
    "    │   ├── staff_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_4/\n",
    "    │   │   └── ...\n",
    "    │   └── staff_5/\n",
    "    │       └── ...\n",
    "    └── collaborators/\n",
    "        ├── collaborators_1/\n",
    "        │   └── ...\n",
    "        ├── collaborators_2/\n",
    "        │   └── ...\n",
    "        ├── collaborators_3/\n",
    "        │   └── ...\n",
    "        ├── collaborators_4/\n",
    "        │   └── ...\n",
    "        └── collaborators_5/\n",
    "            └── ...\n",
    "\n",
    "```\n",
    "\n",
    "Note: The ellipsis (...) is used to represent additional files and directories that may be present but are not explicitly shown in the provided structure.    \n",
    "\n",
    "#### 846.2. script\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Change the working directory to the desired location\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Uncomment the following line if you need to create the \"three40\" directory\n",
    "# nano three40.sh & paste the contents of the three40.sh file\n",
    "# chmod +x three40.sh\n",
    "# mkdir three40\n",
    "# cd three40\n",
    "# Create the root folder\n",
    "# mkdir -p three40\n",
    "\n",
    "# nano three40/_toc.yml & paste the contents of the _toc.yml file\n",
    "\n",
    "# Create the \"intro.ipynb\" file inside the \"root\" folder\n",
    "touch three40/intro.ipynb\n",
    "\n",
    "# Function to read the chapters from the YAML file using pure bash\n",
    "get_chapters_from_yaml() {\n",
    "  local part=\"$1\"\n",
    "  local toc_file=\"three40/_toc.yml\"\n",
    "  local lines\n",
    "  local in_part=false\n",
    "\n",
    "  while read -r line; do\n",
    "    if [[ \"$line\" == *\"$part\"* ]]; then\n",
    "      in_part=true\n",
    "    elif [[ \"$line\" == *\"- File: \"* ]]; then\n",
    "      if \"$in_part\"; then\n",
    "        echo \"$line\" | awk -F': ' '{print $2}' | tr -d ' '\n",
    "      fi\n",
    "    elif [[ \"$line\" == *\"-\"* ]]; then\n",
    "      in_part=false\n",
    "    fi\n",
    "  done < \"$toc_file\"\n",
    "}\n",
    "\n",
    "# Create parts and chapters based on the _toc.yml structure\n",
    "parts=(\n",
    "  \"Root\"\n",
    "  \"Act I\"\n",
    "  \"Act II\"\n",
    "  \"Act III\"\n",
    "  \"Act IV\"\n",
    "  \"Act V\"\n",
    "  \"Epilogue\"\n",
    "  \"Git & Spoke\"\n",
    "  \"Courses\"\n",
    ")\n",
    "\n",
    "# Loop through parts and create chapters inside each part folder\n",
    "for part in \"${parts[@]}\"; do\n",
    "  part_folder=\"three40/$part\"\n",
    "  mkdir -p \"$part_folder\"\n",
    "\n",
    "  if [[ \"$part\" == \"Root\" ]]; then\n",
    "    # Create the \"prologue.ipynb\" file inside the \"Root\" folder\n",
    "    touch \"$part_folder/prologue.ipynb\"\n",
    "  else\n",
    "    # Get the chapters for the current part from the _toc.yml\n",
    "    chapters=($(get_chapters_from_yaml \"$part\"))\n",
    "\n",
    "    # Create chapter files inside the part folder\n",
    "    for chapter in \"${chapters[@]}\"; do\n",
    "      # Extract the act number and create the act folder\n",
    "      act=$(echo \"$chapter\" | cut -d '/' -f 3)\n",
    "      act_folder=\"$part_folder/Act $act\"\n",
    "      mkdir -p \"$act_folder\"\n",
    "\n",
    "      # Create the chapter file inside the act folder\n",
    "      touch \"$act_folder/$(basename \"$chapter\" .ipynb).ipynb\"\n",
    "    done\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Create the \"dramatispersonae\" folder and its subdirectories with loop\n",
    "dramatispersonae_folders=(\n",
    "  \"high_school_students\"\n",
    "  \"undergraduates\"\n",
    "  \"graduates\"\n",
    "  \"medical_students\"\n",
    "  \"residents\"\n",
    "  \"fellows\"\n",
    "  \"faculty\"\n",
    "  \"analysts\"\n",
    "  \"staff\"\n",
    "  \"collaborators\"\n",
    ")\n",
    "\n",
    "for folder in \"${dramatispersonae_folders[@]}\"; do\n",
    "  mkdir -p \"three40/dramatispersonae/$folder\"\n",
    "  touch \"three40/dramatispersonae/$folder/$folder.ipynb\"\n",
    "done\n",
    "\n",
    "# Create additional .ipynb files inside specific subdirectories\n",
    "touch \"three40/dramatispersonae/high_school_students/calvin_mathews/calvin_mathews.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/andrew_arking/andrew_arking.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/andrew_arking/andrew_arking_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/fawaz_al_ammary/fawaz_al_ammary.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/fawaz_al_ammary/fawaz_al_ammary_1.ipynb\"\n",
    "\n",
    "echo \"Folder structure has been created successfully.\"\n",
    "\n",
    "  \n",
    "```\n",
    "\n",
    "#### 846.3. _toc.yaml\n",
    "\n",
    "```yml\n",
    "format: jb-book\n",
    "root: intro.ipynb\n",
    "title: Play\n",
    "\n",
    "parts:\n",
    "- caption: \n",
    "  chapters:\n",
    "  - file: prologue.ipynb\n",
    "\n",
    "- caption: Act I\n",
    "  chapters:\n",
    "  - file: Act I/act1_1.ipynb\n",
    "  - file: Act I/act1_2.ipynb\n",
    "  - file: Act I/act1_3.ipynb\n",
    "\n",
    "- caption: Act II\n",
    "  chapters:\n",
    "  - file: Act II/act2_1.ipynb\n",
    "  - file: Act II/act2_2.ipynb\n",
    "  - file: Act II/act2_3.ipynb\n",
    "  - file: Act II/act2_4.ipynb\n",
    "\n",
    "- caption: Act III\n",
    "  chapters:\n",
    "  - file: Act III/act3_1.ipynb\n",
    "  - file: Act III/act3_2.ipynb\n",
    "  - file: Act III/act3_3.ipynb\n",
    "  - file: Act III/act3_4.ipynb\n",
    "  - file: Act III/act3_5.ipynb\n",
    "\n",
    "- caption: Act IV\n",
    "  chapters:\n",
    "  - file: Act IV/act4_1.ipynb\n",
    "  - file: Act IV/act4_2.ipynb\n",
    "  - file: Act IV/act4_3.ipynb\n",
    "  - file: Act IV/act4_4.ipynb\n",
    "  - file: Act IV/act4_5.ipynb\n",
    "  - file: Act IV/act4_6.ipynb\n",
    "\n",
    "- caption: Act V\n",
    "  chapters:\n",
    "  - file: Act V/act5_1.ipynb\n",
    "  - file: Act V/act5_2.ipynb\n",
    "  - file: Act V/act5_3.ipynb\n",
    "  - file: Act V/act5_4.ipynb\n",
    "  - file: Act V/act5_5.ipynb\n",
    "  - file: Act V/act5_6.ipynb\n",
    "\n",
    "- caption: Epilogue\n",
    "  chapters:\n",
    "  - file: Epilogue/epi_1.ipynb\n",
    "  - file: Epilogue/epi_2.ipynb\n",
    "  - file: Epilogue/epi_3.ipynb\n",
    "  - file: Epilogue/epi_4.ipynb\n",
    "  - file: Epilogue/epi_5.ipynb\n",
    "  - file: Epilogue/epi_6.ipynb\n",
    "  - file: Epilogue/epi_7.ipynb\n",
    "  - file: Epilogue/epi_8.ipynb\n",
    "\n",
    "- caption: Gas & Spoke\n",
    "  chapters:\n",
    "  - file: Gas & Spoke/gas_1.ipynb\n",
    "  - file: Gas & Spoke/gas_2.ipynb\n",
    "  - file: Gas & Spoke/gas_3.ipynb\n",
    "\n",
    "- caption: Courses\n",
    "  chapters: \n",
    "  - url: https://publichealth.jhu.edu/courses\n",
    "    title: Stata Programming \n",
    "  - file: dramatis_personae/high_school_students/high_school_students.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_1.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_2.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_3.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_4.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_5.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_1.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_2.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_3.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_4.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_5.ipynb\n",
    "  - file: dramatis_personae/grad_students/grad_students.ipynb\n",
    "  - file: dramatis_personae/grad_students_1/grad_students_1.ipynb\n",
    "  - file: dramatis_personae/grad_students_2/grad_students_2.ipynb\n",
    "  - file: dramatis_personae/grad_students_3/grad_students_3.ipynb\n",
    "  - file: dramatis_personae/grad_students_4/grad_students_4.ipynb\n",
    "  - file: dramatis_personae/grad_students_5/grad_students_5.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_1.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_2.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_3.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_4.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_5.ipynb\n",
    "  - file: dramatis_personae/residents/residents.ipynb\n",
    "  - file: dramatis_personae/residents/residents_1.ipynb\n",
    "  - file: dramatis_personae/residents/residents_2.ipynb\n",
    "  - file: dramatis_personae/residents/residents_3.ipynb\n",
    "  - file: dramatis_personae/residents/residents_4.ipynb\n",
    "  - file: dramatis_personae/residents/residents_5.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_1.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_2.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_3.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_4.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_5.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_1.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_2.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_3.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_4.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_5.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_1.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_2.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_3.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_4.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_5.ipynb\n",
    "  - file: dramatis_personae/staff/staff.ipynb\n",
    "  - file: dramatis_personae/staff/staff_1.ipynb\n",
    "  - file: dramatis_personae/staff/staff_2.ipynb\n",
    "  - file: dramatis_personae/staff/staff_3.ipynb\n",
    "  - file: dramatis_personae/staff/staff_4.ipynb\n",
    "  - file: dramatis_personae/staff/staff_5.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_1.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_2.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_3.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_4.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_5.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_1.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_2.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_3.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_4.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_5.ipynb\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 847. in-a-nutshell\n",
    "\n",
    "+ do i just codify the entire 07/01/2006 - 07/02/2023?\n",
    "+ the entire 17 years of my jhu life?\n",
    "+ if so then this revolution will be televised!\n",
    "+ not another soul will be lost to the [abyss](https://www.gutenberg.org/files/1998/1998-h/1998-h.htm) of the unknowable\n",
    "+ let them find other ways to get lost, other lifetasks to complete\n",
    "\n",
    "### 848. notfancybutworks\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Change the working directory to the desired location\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Create the \"three40\" directory\n",
    "mkdir -p three40\n",
    "\n",
    "# Create the \"Root\" folder and the \"intro.ipynb\" file inside it\n",
    "mkdir -p \"three40/Root\"\n",
    "touch \"three40/Root/intro.ipynb\"\n",
    "\n",
    "# Create the \"prologue.ipynb\" file in the \"three40\" directory\n",
    "touch \"three40/prologue.ipynb\"\n",
    "\n",
    "# Create \"Act I\" folder and its subfiles\n",
    "mkdir -p \"three40/Act I\"\n",
    "touch \"three40/Act I/act1_1.ipynb\"\n",
    "touch \"three40/Act I/act1_2.ipynb\"\n",
    "touch \"three40/Act I/act1_3.ipynb\"\n",
    "\n",
    "# Create \"Act II\" folder and its subfiles\n",
    "mkdir -p \"three40/Act II\"\n",
    "touch \"three40/Act II/act2_1.ipynb\"\n",
    "touch \"three40/Act II/act2_2.ipynb\"\n",
    "touch \"three40/Act II/act2_3.ipynb\"\n",
    "touch \"three40/Act II/act2_4.ipynb\"\n",
    "\n",
    "# Create \"Act III\" folder and its subfiles\n",
    "mkdir -p \"three40/Act III\"\n",
    "touch \"three40/Act III/act3_1.ipynb\"\n",
    "touch \"three40/Act III/act3_2.ipynb\"\n",
    "touch \"three40/Act III/act3_3.ipynb\"\n",
    "touch \"three40/Act III/act3_4.ipynb\"\n",
    "touch \"three40/Act III/act3_5.ipynb\"\n",
    "\n",
    "# Create \"Act IV\" folder and its subfiles\n",
    "mkdir -p \"three40/Act IV\"\n",
    "touch \"three40/Act IV/act4_1.ipynb\"\n",
    "touch \"three40/Act IV/act4_2.ipynb\"\n",
    "touch \"three40/Act IV/act4_3.ipynb\"\n",
    "touch \"three40/Act IV/act4_4.ipynb\"\n",
    "touch \"three40/Act IV/act4_5.ipynb\"\n",
    "touch \"three40/Act IV/act4_6.ipynb\"\n",
    "\n",
    "# Create \"Act V\" folder and its subfiles\n",
    "mkdir -p \"three40/Act V\"\n",
    "touch \"three40/Act V/act5_1.ipynb\"\n",
    "touch \"three40/Act V/act5_2.ipynb\"\n",
    "touch \"three40/Act V/act5_3.ipynb\"\n",
    "touch \"three40/Act V/act5_4.ipynb\"\n",
    "touch \"three40/Act V/act5_5.ipynb\"\n",
    "touch \"three40/Act V/act5_6.ipynb\"\n",
    "\n",
    "# Create \"Epilogue\" folder and its subfiles\n",
    "mkdir -p \"three40/Epilogue\"\n",
    "touch \"three40/Epilogue/epi_1.ipynb\"\n",
    "touch \"three40/Epilogue/epi_2.ipynb\"\n",
    "touch \"three40/Epilogue/epi_3.ipynb\"\n",
    "touch \"three40/Epilogue/epi_4.ipynb\"\n",
    "touch \"three40/Epilogue/epi_5.ipynb\"\n",
    "touch \"three40/Epilogue/epi_6.ipynb\"\n",
    "touch \"three40/Epilogue/epi_7.ipynb\"\n",
    "touch \"three40/Epilogue/epi_8.ipynb\"\n",
    "\n",
    "# Create \"Git & Spoke\" folder and its subfiles\n",
    "mkdir -p \"three40/Git & Spoke\"\n",
    "touch \"three40/Git & Spoke/gas_1.ipynb\"\n",
    "touch \"three40/Git & Spoke/gas_2.ipynb\"\n",
    "touch \"three40/Git & Spoke/gas_3.ipynb\"\n",
    "\n",
    "# Create \"Courses\" folder and its subfiles\n",
    "mkdir -p \"three40/Courses\"\n",
    "touch \"three40/Courses/course1.ipynb\"\n",
    "touch \"three40/Courses/course2.ipynb\"\n",
    "\n",
    "# Create \"dramatispersonae\" folder and its subdirectories\n",
    "mkdir -p \"three40/dramatispersonae/high_school_students\"\n",
    "mkdir -p \"three40/dramatispersonae/undergraduates\"\n",
    "mkdir -p \"three40/dramatispersonae/graduates\"\n",
    "mkdir -p \"three40/dramatispersonae/medical_students\"\n",
    "mkdir -p \"three40/dramatispersonae/residents\"\n",
    "mkdir -p \"three40/dramatispersonae/fellows\"\n",
    "mkdir -p \"three40/dramatispersonae/faculty\"\n",
    "mkdir -p \"three40/dramatispersonae/analysts\"\n",
    "mkdir -p \"three40/dramatispersonae/staff\"\n",
    "mkdir -p \"three40/dramatispersonae/collaborators\"\n",
    "\n",
    "# Create \"dramatispersonae\" subdirectories with suffixes _1 to _5\n",
    "for branch in high_school_students undergraduates graduates medical_students residents fellows faculty analysts staff collaborators; do\n",
    "    for ((i=1; i<=5; i++)); do\n",
    "        mkdir -p \"three40/dramatispersonae/${branch}/${branch}_${i}\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create additional .ipynb files inside specific subdirectories\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_5.ipynb\"\n",
    "\n",
    "# Display the directory tree\n",
    "echo \"Directory Structure:\"\n",
    "echo \"-------------------\"\n",
    "tree three40\n",
    "\n",
    "echo \"Folder structure has been created successfully.\"\n",
    "\n",
    "```\n",
    "\n",
    "### 849. [stillgotit](https://www.youtube.com/watch?v=DGcVEE_hMu8)\n",
    "\n",
    "I just met my life, yeah       \n",
    "For the first time     \n",
    "I just met the prettiest girl, in the whole wide world   \n",
    "\n",
    "--- \n",
    "\n",
    "I just met my `wife`, yeah           \n",
    "For the first time         \n",
    "I just met the `partiest` girl, in the whole wide world    \n",
    "\n",
    "---\n",
    "\n",
    "talk of [misdirection](https://muzaale.github.io/denotas/bdn202308.html#id4)\n",
    "\n",
    "### 850. openai\n",
    "\n",
    "Prego! Di nulla! If you have any more questions or need further assistance, feel free to ask. Buona giornata! (Have a great day!)   \n",
    "\n",
    "$\\vdots$    \n",
    "\n",
    "Great to hear! If you ever need any help or have more questions in the future, don't hesitate to reach out. Have a wonderful day!\n",
    "\n",
    "### 851. babyface\n",
    "\n",
    "+ verbs:\n",
    "   + to do\n",
    "   + do be\n",
    "   + [to hide](https://www.youtube.com/watch?v=S-EHGhIBBP4)\n",
    "+ nemesis:\n",
    "   + guys whose verb is `to do`\n",
    "   + athletes\n",
    "   + yeah.. get the picture?\n",
    "\n",
    "### 852. [asante](https://en.wikipedia.org/wiki/Ashanti_Empire)\n",
    "\n",
    "+ ghanian food in gettysburg, maryland:\n",
    "  + rambow\n",
    "  + savannah\n",
    "+ what to order:\n",
    "  + banku with tilapia or roca\n",
    "  + fufu with goat meat\n",
    "  + jollof rice with chicken  \n",
    "  + peanut butter soup with rice balls (with mutu - rice gun)\n",
    "  + black pepper soup (shito) & okra soup\n",
    "  + kenkey with fish\n",
    "+ visit willo & phat:\n",
    "  + let phat rest\n",
    "  + no kitchen time\n",
    "  + all on me!\n",
    "+ courtesy of:\n",
    "  + (240) 516-4535\n",
    "  + james obour (stone)\n",
    "\n",
    "### 853. ghana\n",
    "\n",
    "+ thc-dn-64707106/etbjhmf_lx_rsdorhrsdq_hm_sgd_vghkd_gnxeqhdmc_cndrm_s_jmnv \n",
    "+ best west african food is from the following countries in order:\n",
    "   + ghana\n",
    "   + nigeria\n",
    "   + senegal\n",
    "    + ivory coast\n",
    "    + mali\n",
    "    + guinea\n",
    "    + burkina faso\n",
    "    + togo\n",
    "    + benin\n",
    "    + gambia\n",
    "    + sierra leone\n",
    "    + liberia\n",
    "    + guinea-bissau\n",
    "    + cape verde\n",
    "+ github co-pilot suggested invory coast as number 2\n",
    "+ i disagree, as do most west africans\n",
    "+ some of us have eaten food from all of these countries\n",
    "\n",
    "### 854. Stata\n",
    "\n",
    "Hi Laura,     \n",
    " \n",
    "I hope this finds you well. Just wanted to drop some ideas by you:       \n",
    " \n",
    "After careful consideration of the course evaluations over the last two years I’m of the opinion that there should be three Stata Programming classes\n",
    "These three classes would reflect the diverse backgrounds of the Bloomberg School students\n",
    "What would be the defining characteristics of each of these classes?        \n",
    " \n",
    "i)                    Observed data    \n",
    "ii)                   Expected data    \n",
    "iii)                 Simulated data    \n",
    " \n",
    "These may seem somewhat abstract concepts but they would allow me to better communicate some fundamental concepts with students. The idea would be to have Stata Programming I (observed data) as the basic class. Stata Programming II (Expected data) as the intermediate class. And Stata Programming III  (Simulated data) as the advanced class. I already have some additional syllabus material for each of these. But, in brief, all would be offered as hybrid. The basic class would be 2 credit units. And the intermediate and advanced classes would each be 1 credit unit.     \n",
    " \n",
    "Any thoughts?    \n",
    " \n",
    "Abi   \n",
    "\n",
    "### 855. chances\n",
    "\n",
    "+ migos\n",
    "+ yrn 2\n",
    "+ romantic\n",
    "+ dance\n",
    "+ panic\n",
    "+ fancy\n",
    "+ outstanding\n",
    "+ i took a whole lot.. \n",
    "+ bandos\n",
    "\n",
    "### 856. Atliens\n",
    "\n",
    "+ throw your hands in the air\n",
    "+ and if you like fish and grits?\n",
    "+ every body let me hear you say.. oh yeah yurr \n",
    "\n",
    "### 857. g.o.a.t.\n",
    "\n",
    "+ man's [greatest](https://muzaale.github.io/denotas/_downloads/f03a303c6fcab3ab1bb87b11a74a92be/goat.jpg) invention\n",
    "+ a mere 88 keys\n",
    "+ piano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08/03/2023\n",
    "\n",
    "### 858. [igotthat](https://www.youtube.com/watch?v=EdZWMKxJpDc)\n",
    "\n",
    "+ warryn campbell\n",
    "+ erica campbell\n",
    "+ still got it\n",
    "\n",
    "### 859. yesterday\n",
    "\n",
    "+ workflow: rollover `jupyter-book create .`\n",
    "+ herein we take `100%` control of the `toc.yml` file\n",
    "+ lets see how it all comes together:\n",
    "\n",
    "#### 859.1 terminal\n",
    "\n",
    "```unix\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "mkdir -p three40\n",
    "nano three40/_toc.yml\n",
    "```\n",
    "\n",
    "#### 859.2 paste\n",
    "\n",
    "```yaml\n",
    "format: jb-book\n",
    "root: intro.ipynb\n",
    "title: Play\n",
    "\n",
    "parts:\n",
    "- caption: Prologue\n",
    "  chapters:\n",
    "  - file: prologue.ipynb\n",
    "\n",
    "- caption: Act I\n",
    "  chapters:\n",
    "  - file: Act I/act1_1.ipynb\n",
    "  - file: Act I/act1_2.ipynb\n",
    "  - file: Act I/act1_3.ipynb\n",
    "\n",
    "- caption: Act II\n",
    "  chapters:\n",
    "  - file: Act II/act2_1.ipynb\n",
    "  - file: Act II/act2_2.ipynb\n",
    "  - file: Act II/act2_3.ipynb\n",
    "  - file: Act II/act2_4.ipynb\n",
    "\n",
    "- caption: Act III\n",
    "  chapters:\n",
    "  - file: Act III/act3_1.ipynb\n",
    "  - file: Act III/act3_2.ipynb\n",
    "  - file: Act III/act3_3.ipynb\n",
    "  - file: Act III/act3_4.ipynb\n",
    "  - file: Act III/act3_5.ipynb\n",
    "\n",
    "- caption: Act IV\n",
    "  chapters:\n",
    "  - file: Act IV/act4_1.ipynb\n",
    "  - file: Act IV/act4_2.ipynb\n",
    "  - file: Act IV/act4_3.ipynb\n",
    "  - file: Act IV/act4_4.ipynb\n",
    "  - file: Act IV/act4_5.ipynb\n",
    "  - file: Act IV/act4_6.ipynb\n",
    "\n",
    "- caption: Act V\n",
    "  chapters:\n",
    "  - file: Act V/act5_1.ipynb\n",
    "  - file: Act V/act5_2.ipynb\n",
    "  - file: Act V/act5_3.ipynb\n",
    "  - file: Act V/act5_4.ipynb\n",
    "  - file: Act V/act5_5.ipynb\n",
    "  - file: Act V/act5_6.ipynb\n",
    "\n",
    "- caption: Epilogue\n",
    "  chapters:\n",
    "  - file: Epilogue/epi_1.ipynb\n",
    "  - file: Epilogue/epi_2.ipynb\n",
    "  - file: Epilogue/epi_3.ipynb\n",
    "  - file: Epilogue/epi_4.ipynb\n",
    "  - file: Epilogue/epi_5.ipynb\n",
    "  - file: Epilogue/epi_6.ipynb\n",
    "  - file: Epilogue/epi_7.ipynb\n",
    "  - file: Epilogue/epi_8.ipynb\n",
    "\n",
    "- caption: Gas & Spoke\n",
    "  chapters:\n",
    "  - file: Gas & Spoke/gas_1.ipynb\n",
    "  - file: Gas & Spoke/gas_2.ipynb\n",
    "  - file: Gas & Spoke/gas_3.ipynb\n",
    "\n",
    "- caption: Courses\n",
    "  chapters: \n",
    "  - url: https://publichealth.jhu.edu/courses\n",
    "    title: Stata Programming \n",
    "  - file: dramatispersonae/high_school_students/high_school_students.ipynb\n",
    "  - file: dramatispersonae/high_school_students/high_school_students_1.ipynb\n",
    "  - file: dramatispersonae/high_school_students/high_school_students_2.ipynb\n",
    "  - file: dramatispersonae/high_school_students/high_school_students_3.ipynb\n",
    "  - file: dramatispersonae/high_school_students/high_school_students_4.ipynb\n",
    "  - file: dramatispersonae/high_school_students/high_school_students_5.ipynb\n",
    "  - file: dramatispersonae/undergraduates/undergraduates.ipynb\n",
    "  - file: dramatispersonae/undergraduates/undergraduates_1.ipynb\n",
    "  - file: dramatispersonae/undergraduates/undergraduates_2.ipynb\n",
    "  - file: dramatispersonae/undergraduates/undergraduates_3.ipynb\n",
    "  - file: dramatispersonae/undergraduates/undergraduates_4.ipynb\n",
    "  - file: dramatispersonae/undergraduates/undergraduates_5.ipynb\n",
    "  - file: dramatispersonae/graduate_students/graduate_students.ipynb\n",
    "  - file: dramatispersonae/graduate_students/graduate_students_1.ipynb\n",
    "  - file: dramatispersonae/graduate_students/graduate_students_2.ipynb\n",
    "  - file: dramatispersonae/graduate_students/graduate_students_3.ipynb\n",
    "  - file: dramatispersonae/graduate_students/graduate_students_4.ipynb\n",
    "  - file: dramatispersonae/graduate_students/graduate_students_5.ipynb\n",
    "  - file: dramatispersonae/medical_students/medical_students.ipynb\n",
    "  - file: dramatispersonae/medical_students/medical_students_1.ipynb\n",
    "  - file: dramatispersonae/medical_students/medical_students_2.ipynb\n",
    "  - file: dramatispersonae/medical_students/medical_students_3.ipynb\n",
    "  - file: dramatispersonae/medical_students/medical_students_4.ipynb\n",
    "  - file: dramatispersonae/medical_students/medical_students_5.ipynb\n",
    "  - file: dramatispersonae/residents/residents.ipynb\n",
    "  - file: dramatispersonae/residents/residents_1.ipynb\n",
    "  - file: dramatispersonae/residents/residents_2.ipynb\n",
    "  - file: dramatispersonae/residents/residents_3.ipynb\n",
    "  - file: dramatispersonae/residents/residents_4.ipynb\n",
    "  - file: dramatispersonae/residents/residents_5.ipynb\n",
    "  - file: dramatispersonae/fellows/fellows.ipynb\n",
    "  - file: dramatispersonae/fellows/fellows_1.ipynb\n",
    "  - file: dramatispersonae/fellows/fellows_2.ipynb\n",
    "  - file: dramatispersonae/fellows/fellows_3.ipynb\n",
    "  - file: dramatispersonae/fellows/fellows_4.ipynb\n",
    "  - file: dramatispersonae/fellows/fellows_5.ipynb\n",
    "  - file: dramatispersonae/faculty/faculty.ipynb\n",
    "  - file: dramatispersonae/faculty/faculty_1.ipynb\n",
    "  - file: dramatispersonae/faculty/faculty_2.ipynb\n",
    "  - file: dramatispersonae/faculty/faculty_3.ipynb\n",
    "  - file: dramatispersonae/faculty/faculty_4.ipynb\n",
    "  - file: dramatispersonae/faculty/faculty_5.ipynb\n",
    "  - file: dramatispersonae/analysts/analysts.ipynb\n",
    "  - file: dramatispersonae/analysts/analysts_1.ipynb\n",
    "  - file: dramatispersonae/analysts/analysts_2.ipynb\n",
    "  - file: dramatispersonae/analysts/analysts_3.ipynb\n",
    "  - file: dramatispersonae/analysts/analysts_4.ipynb\n",
    "  - file: dramatispersonae/analysts/analysts_5.ipynb\n",
    "  - file: dramatispersonae/staff/staff.ipynb\n",
    "  - file: dramatispersonae/staff/staff_1.ipynb\n",
    "  - file: dramatispersonae/staff/staff_2.ipynb\n",
    "  - file: dramatispersonae/staff/staff_3.ipynb\n",
    "  - file: dramatispersonae/staff/staff_4.ipynb\n",
    "  - file: dramatispersonae/staff/staff_5.ipynb\n",
    "  - file: dramatispersonae/collaborators/collaborators.ipynb\n",
    "  - file: dramatispersonae/collaborators/collaborators_1.ipynb\n",
    "  - file: dramatispersonae/collaborators/collaborators_2.ipynb\n",
    "  - file: dramatispersonae/collaborators/collaborators_3.ipynb\n",
    "  - file: dramatispersonae/collaborators/collaborators_4.ipynb\n",
    "  - file: dramatispersonae/collaborators/collaborators_5.ipynb\n",
    "  - file: dramatispersonae/graduates/graduates.ipynb\n",
    "  - file: dramatispersonae/graduates/graduates_1.ipynb\n",
    "  - file: dramatispersonae/graduates/graduates_2.ipynb\n",
    "  - file: dramatispersonae/graduates/graduates_3.ipynb\n",
    "  - file: dramatispersonae/graduates/graduates_4.ipynb\n",
    "  - file: dramatispersonae/graduates/graduates_5.ipynb\n",
    "\n",
    "```\n",
    "#### 859.3 bash\n",
    "\n",
    "```unix\n",
    "./three40.sh\n",
    "```\n",
    "\n",
    "#### 859.4 tree\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Change the working directory to the desired location\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Create the \"three40\" directory\n",
    "# mkdir -p three40\n",
    "# nano three40/_toc.yml\n",
    "\n",
    "# Create the \"Root\" folder and the \"intro.ipynb\" file inside it\n",
    "touch \"three40/intro.ipynb\"\n",
    "\n",
    "# Create the \"prologue.ipynb\" file in the \"three40\" directory\n",
    "touch \"three40/prologue.ipynb\"\n",
    "\n",
    "# Create \"Act I\" folder and its subfiles\n",
    "mkdir -p \"three40/Act I\"\n",
    "touch \"three40/Act I/act1_1.ipynb\"\n",
    "touch \"three40/Act I/act1_2.ipynb\"\n",
    "touch \"three40/Act I/act1_3.ipynb\"\n",
    "\n",
    "# Create \"Act II\" folder and its subfiles\n",
    "mkdir -p \"three40/Act II\"\n",
    "touch \"three40/Act II/act2_1.ipynb\"\n",
    "touch \"three40/Act II/act2_2.ipynb\"\n",
    "touch \"three40/Act II/act2_3.ipynb\"\n",
    "touch \"three40/Act II/act2_4.ipynb\"\n",
    "\n",
    "# Create \"Act III\" folder and its subfiles\n",
    "mkdir -p \"three40/Act III\"\n",
    "touch \"three40/Act III/act3_1.ipynb\"\n",
    "touch \"three40/Act III/act3_2.ipynb\"\n",
    "touch \"three40/Act III/act3_3.ipynb\"\n",
    "touch \"three40/Act III/act3_4.ipynb\"\n",
    "touch \"three40/Act III/act3_5.ipynb\"\n",
    "\n",
    "# Create \"Act IV\" folder and its subfiles\n",
    "mkdir -p \"three40/Act IV\"\n",
    "touch \"three40/Act IV/act4_1.ipynb\"\n",
    "touch \"three40/Act IV/act4_2.ipynb\"\n",
    "touch \"three40/Act IV/act4_3.ipynb\"\n",
    "touch \"three40/Act IV/act4_4.ipynb\"\n",
    "touch \"three40/Act IV/act4_5.ipynb\"\n",
    "touch \"three40/Act IV/act4_6.ipynb\"\n",
    "\n",
    "# Create \"Act V\" folder and its subfiles\n",
    "mkdir -p \"three40/Act V\"\n",
    "touch \"three40/Act V/act5_1.ipynb\"\n",
    "touch \"three40/Act V/act5_2.ipynb\"\n",
    "touch \"three40/Act V/act5_3.ipynb\"\n",
    "touch \"three40/Act V/act5_4.ipynb\"\n",
    "touch \"three40/Act V/act5_5.ipynb\"\n",
    "touch \"three40/Act V/act5_6.ipynb\"\n",
    "\n",
    "# Create \"Epilogue\" folder and its subfiles\n",
    "mkdir -p \"three40/Epilogue\"\n",
    "touch \"three40/Epilogue/epi_1.ipynb\"\n",
    "touch \"three40/Epilogue/epi_2.ipynb\"\n",
    "touch \"three40/Epilogue/epi_3.ipynb\"\n",
    "touch \"three40/Epilogue/epi_4.ipynb\"\n",
    "touch \"three40/Epilogue/epi_5.ipynb\"\n",
    "touch \"three40/Epilogue/epi_6.ipynb\"\n",
    "touch \"three40/Epilogue/epi_7.ipynb\"\n",
    "touch \"three40/Epilogue/epi_8.ipynb\"\n",
    "\n",
    "# Create \"Git & Spoke\" folder and its subfiles\n",
    "mkdir -p \"three40/Git & Spoke\"\n",
    "touch \"three40/Git & Spoke/gas_1.ipynb\"\n",
    "touch \"three40/Git & Spoke/gas_2.ipynb\"\n",
    "touch \"three40/Git & Spoke/gas_3.ipynb\"\n",
    "\n",
    "# Create \"Courses\" folder and its subfiles\n",
    "mkdir -p \"three40/Courses\"\n",
    "touch \"three40/Courses/course1.ipynb\"\n",
    "touch \"three40/Courses/course2.ipynb\"\n",
    "\n",
    "# Create \"dramatispersonae\" folder and its subdirectories\n",
    "mkdir -p \"three40/dramatispersonae/high_school_students\"\n",
    "mkdir -p \"three40/dramatispersonae/undergraduates\"\n",
    "mkdir -p \"three40/dramatispersonae/graduates\"\n",
    "mkdir -p \"three40/dramatispersonae/medical_students\"\n",
    "mkdir -p \"three40/dramatispersonae/residents\"\n",
    "mkdir -p \"three40/dramatispersonae/fellows\"\n",
    "mkdir -p \"three40/dramatispersonae/faculty\"\n",
    "mkdir -p \"three40/dramatispersonae/analysts\"\n",
    "mkdir -p \"three40/dramatispersonae/staff\"\n",
    "mkdir -p \"three40/dramatispersonae/collaborators\"\n",
    "\n",
    "# Create \"dramatispersonae\" subdirectories with suffixes _1 to _5\n",
    "for branch in high_school_students undergraduates graduates medical_students residents fellows faculty analysts staff collaborators; do\n",
    "    for ((i=1; i<=5; i++)); do\n",
    "        mkdir -p \"three40/dramatispersonae/${branch}/${branch}_${i}\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create additional .ipynb files inside specific subdirectories\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/high_school_students/high_school_students_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/undergraduates/undergraduates_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/graduates/graduates_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/medical_students/medical_students_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/residents/residents_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/fellows/fellows_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/faculty/faculty_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/analysts/analysts_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/staff/staff_5.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_1.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_2.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_3.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_4.ipynb\"\n",
    "touch \"three40/dramatispersonae/collaborators/collaborators_5.ipynb\"\n",
    "\n",
    "# Display the directory tree\n",
    "echo \"Directory Structure:\"\n",
    "echo \"-------------------\"\n",
    "echo \"three40/\n",
    "├── intro.ipynb\n",
    "├── prologue.ipynb\n",
    "├── Act I/\n",
    "│   ├── act1_1.ipynb\n",
    "│   ├── act1_2.ipynb\n",
    "│   ├── act1_3.ipynb\n",
    "│   └── ...\n",
    "├── Act II/\n",
    "│   ├── act2_1.ipynb\n",
    "│   ├── act2_2.ipynb\n",
    "│   └── ...\n",
    "├── Act III/\n",
    "│   ├── act3_1.ipynb\n",
    "│   ├── act3_2.ipynb\n",
    "│   ├── act3_3.ipynb\n",
    "│   ├── act3_4.ipynb\n",
    "│   └── act3_5.ipynb\n",
    "├── Act IV/\n",
    "│   ├── act4_1.ipynb\n",
    "│   ├── act4_2.ipynb\n",
    "│   ├── act4_3.ipynb\n",
    "│   ├── act4_4.ipynb\n",
    "│   ├── act4_5.ipynb\n",
    "│   └── act4_6.ipynb\n",
    "├── Act V/\n",
    "│   ├── act5_1.ipynb\n",
    "│   ├── act5_2.ipynb\n",
    "│   ├── act5_3.ipynb\n",
    "│   ├── act5_4.ipynb\n",
    "│   ├── act5_5.ipynb\n",
    "│   └── act5_6.ipynb\n",
    "├── Epilogue/\n",
    "│   ├── epi_1.ipynb\n",
    "│   ├── epi_2.ipynb\n",
    "│   ├── epi_3.ipynb\n",
    "│   ├── epi_4.ipynb\n",
    "│   ├── epi_5.ipynb\n",
    "│   ├── epi_6.ipynb\n",
    "│   ├── epi_7.ipynb\n",
    "│   └── epi_8.ipynb\n",
    "├── Gas & Spoke/\n",
    "│   ├── gas_1.ipynb\n",
    "│   ├── gas_2.ipynb\n",
    "│   └── gas_3.ipynb\n",
    "└── dramatispersonae/\n",
    "    ├── high_school_students/\n",
    "    │   ├── high_school_students_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_4/\n",
    "    │   │   └── ...\n",
    "    │   └── high_school_students_5/\n",
    "    │       └── ...\n",
    "    ├── undergraduates/\n",
    "    │   ├── undergraduates_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── undergraduates_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── undergraduates_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── undergraduates_4/\n",
    "    │   │   └── ...\n",
    "    │   └── undergraduates_5/\n",
    "    │       └── ...\n",
    "    ├── graduates/\n",
    "    │   ├── graduates_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_4/\n",
    "    │   │   └── ...\n",
    "    │   └── graduates_5/\n",
    "    │       └── ...\n",
    "    ├── medical_students/\n",
    "    │   ├── medical_students_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_4/\n",
    "    │   │   └── ...\n",
    "    │   └── medical_students_5/\n",
    "    │       └── ...\n",
    "    ├── residents/\n",
    "    │   ├── residents_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_4/\n",
    "    │   │   └── ...\n",
    "    │   └── residents_5/\n",
    "    │       └── ...\n",
    "    ├── fellows/\n",
    "    │   ├── fellows_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_4/\n",
    "    │   │   └── ...\n",
    "    │   └── fellows_5/\n",
    "    │       └── ...\n",
    "    ├── faculty/\n",
    "    │   ├── faculty_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_4/\n",
    "    │   │   └── ...\n",
    "    │   └── faculty_5/\n",
    "    │       └── ...\n",
    "    ├── analysts/\n",
    "    │   ├── analysts_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_4/\n",
    "    │   │   └── ...\n",
    "    │   └── analysts_5/\n",
    "    │       └── ...\n",
    "    ├── staff/\n",
    "    │   ├── staff_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_4/\n",
    "    │   │   └── ...\n",
    "    │   └── staff_5/\n",
    "    │       └── ...\n",
    "    └── collaborators/\n",
    "        ├── collaborators_1/\n",
    "        │   └── ...\n",
    "        ├── collaborators_2/\n",
    "        │   └── ...\n",
    "        ├── collaborators_3/\n",
    "        │   └── ...\n",
    "        ├── collaborators_4/\n",
    "        │   └── ...\n",
    "        └── collaborators_5/\n",
    "            └── ...\"\n",
    "echo \"Folder structure has been created successfully.\"\n",
    "mv three40.sh three40/three40.sh\n",
    "```\n",
    "\n",
    "#### 859.4 \n",
    "\n",
    "+ create a workdir-gitrepo .sh file\n",
    "+ build your .html\n",
    "+ push to github\n",
    "\n",
    "### 860. pwomd\n",
    "\n",
    "```unix1. \n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "mkdir -p three40\n",
    "nano three40/three40.sh\n",
    "chmod +x three40/three40.sh\n",
    "nano three40/_toc.yml\n",
    "nano three40/_config.yml\n",
    "./three40/three40.sh\n",
    "find . -name \"*.ipynb\" -exec cp \"notebook.ipynb\" {} \\;\n",
    "nano three40/three40.six100.sh\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "git clone https://github.com/jhustata/six100\n",
    "jb build three40\n",
    "cp -r three40/* six100\n",
    "cd six100\n",
    "git add ./*\n",
    "git commit -m \"first jb created manually\"\n",
    "git push\n",
    "ghp-import -n -p -f _build/html\n",
    "chmod +x three40/three40.six100.sh\n",
    "./three40/three40.six100.sh\n",
    "```\n",
    "\n",
    "### 861. bloc/githistory.sh\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Function to reset to a clean state.\n",
    "reset_state() {\n",
    "    # Abort any ongoing rebase.\n",
    "    git rebase --abort &> /dev/null && echo \"Aborted an ongoing rebase.\"\n",
    "\n",
    "    # Stash any unstaged changes to ensure operations can proceed.\n",
    "    git stash save \"Unstaged changes before running githis.sh\" && echo \"Stashed unsaved changes.\"\n",
    "\n",
    "    # Remove any lingering rebase directories.\n",
    "    if [ -d \".git/rebase-merge\" ] || [ -d \".git/rebase-apply\" ]; then\n",
    "        rm -rf .git/rebase-*\n",
    "        echo \"Removed lingering rebase directories.\"\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Navigate to the main working directory.\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Navigate to the six100 directory.\n",
    "cd six100 || { echo \"Directory six100 does not exist. Exiting.\"; exit 1; }\n",
    "\n",
    "# Reset to a clean state.\n",
    "reset_state\n",
    "\n",
    "# Fetch the latest changes from temp_og_repo using SSH.\n",
    "if git fetch git@github.com:afecdvi/temp_og_repo.git main; then\n",
    "    echo \"Successfully fetched changes via SSH.\"\n",
    "else\n",
    "    echo \"Failed to fetch changes using SSH. Exiting.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Reset the local branch to match the fetched changes.\n",
    "git reset --hard FETCH_HEAD\n",
    "echo \"Local branch reset to match fetched changes.\"\n",
    "\n",
    "# Check for network connection.\n",
    "if ! ping -c 1 google.com &> /dev/null; then\n",
    "    echo \"No internet connection. Exiting.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check repository size.\n",
    "REPO_SIZE=$(du -sh .git | cut -f1)\n",
    "echo \"Repository size: $REPO_SIZE\"\n",
    "\n",
    "# Adjust Git configurations.\n",
    "POST_BUFFER_SIZE=$(( (RANDOM % 200 + 300) * 1048576 ))\n",
    "LOW_SPEED_LIMIT=$(( RANDOM % 5000 + 2000 ))\n",
    "LOW_SPEED_TIME=$(( RANDOM % 60 + 30 ))\n",
    "\n",
    "git config http.postBuffer $POST_BUFFER_SIZE\n",
    "git config http.lowSpeedLimit $LOW_SPEED_LIMIT\n",
    "git config http.lowSpeedTime $LOW_SPEED_TIME\n",
    "echo \"Adjusted Git's buffer size to $POST_BUFFER_SIZE, low speed limit to $LOW_SPEED_LIMIT and low speed time to $LOW_SPEED_TIME.\"\n",
    "\n",
    "# Push the changes to the remote repository using SSH and verbose logging.\n",
    "if git push git@github.com:afecdvi/og.git main --force -v; then\n",
    "    echo \"Successfully pushed changes using SSH.\"\n",
    "    # Unstash any changes we stashed earlier.\n",
    "    git stash pop &> /dev/null && echo \"Restored previously stashed changes.\"\n",
    "    echo \"Script completed successfully!\"\n",
    "else\n",
    "    echo \"Failed to push changes even with SSH. Exiting.\"\n",
    "    git stash pop &> /dev/null && echo \"Restored previously stashed changes.\"\n",
    "    exit 1\n",
    "fi\n",
    "```\n",
    "\n",
    "### 862. conditionals\n",
    "\n",
    "+ `if` `then` `else` `fi`\n",
    "   + `if` refers to the condition \n",
    "    + `then` refers to the action\n",
    "    + `else` refers to the alternative action\n",
    "    + `fi` refers to the end of the conditional\n",
    "+ `case` `esac`\n",
    "    + `case` refers to the condition\n",
    "    + `esac` refers to the end of the conditional\n",
    "+ `for` `do` `done`\n",
    "    + `for` refers to the condition\n",
    "    + `do` refers to the action\n",
    "    + `done` refers to the end of the conditional\n",
    "+ `while` `do` `done`\n",
    "    + `while` refers to the condition\n",
    "    + `do` refers to the action\n",
    "    + `done` refers to the end of the conditional\n",
    "+ `until` `do` `done`\n",
    "    + `until` refers to the condition\n",
    "    + `do` refers to the action\n",
    "    + `done` refers to the end of the conditional\n",
    "\n",
    "### 863. stata\n",
    "\n",
    "+ let it be said that unconditional code is the most basic, conditional code is intermediate, and looping code is advanced\n",
    "+ so let's start with the most basic\n",
    "+ `if` `then` `else` `fi`\n",
    "    + `if` refers to the condition \n",
    "    + `then` refers to the action\n",
    "    + `else` refers to the alternative action\n",
    "    + `fi` refers to the end of the conditional\n",
    "\n",
    "1. unconditional\n",
    "\n",
    "```stata\n",
    "clear\n",
    "set obs 100\n",
    "gen x = runiform()\n",
    "display \"x is greater than 0.5\"\n",
    "```\n",
    "---\n",
    "\n",
    "2. conditional\n",
    "\n",
    "```stata\n",
    "if x > 0.5 {\n",
    "    display \"x is greater than 0.5\"\n",
    "}\n",
    "else if x > 0.25 {\n",
    "    display \"x is greater than 0.25\"\n",
    "}\n",
    "else if x > 0.125 {\n",
    "    display \"x is greater than 0.125\"\n",
    "}\n",
    "else {\n",
    "    display \"x is less than or equal to 0.125\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "3. looping\n",
    "\n",
    "```stata\n",
    "forvalues i = 1/10 {\n",
    "    if `i' < 5 {\n",
    "        display \"i = `i'\"\n",
    "    }\n",
    "    else {\n",
    "        display \"i is greater than or equal to 5\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "+ we'll build the three classes around these three basic concepts\n",
    "\n",
    "### 864. bloc/blocdenotas.sh\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Ask the user for the path to the SSH key\n",
    "read -p \"Please provide the path to your SSH key (e.g. ~/.ssh/id_blocdenotas): \" SSH_KEY_PATH\n",
    "\n",
    "# If no input is provided, exit the script\n",
    "if [[ -z \"$SSH_KEY_PATH\" ]]; then\n",
    "    echo \"No SSH key provided. Exiting.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check if the SSH key exists\n",
    "if [[ ! -f \"$SSH_KEY_PATH\" ]]; then\n",
    "    echo \"The provided SSH key does not exist. Exiting.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Change directory to ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Build Jupyter Book\n",
    "jb build bloc\n",
    "cp -r bloc/* denotas\n",
    "\n",
    "# Change directory to 'denotas'\n",
    "cd denotas\n",
    "\n",
    "# Add all files in the current directory to Git\n",
    "git add ./*\n",
    "\n",
    "# Commit changes to Git with the given commit message\n",
    "git commit -m \"introducing SSH keys to bloc/blocdenotas.sh\"\n",
    "\n",
    "# Use the provided SSH key for the upcoming Git commands\n",
    "export GIT_SSH_COMMAND=\"ssh -i $SSH_KEY_PATH\"\n",
    "\n",
    "# Ensure using the SSH URL for the repository\n",
    "git remote set-url origin git@github.com:muzaale/denotas.git\n",
    "\n",
    "# Push changes to GitHub\n",
    "git push origin main\n",
    "\n",
    "# Import the built HTML to gh-pages and push to GitHub\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "# Unset the custom GIT_SSH_COMMAND to avoid affecting other git operations\n",
    "unset GIT_SSH_COMMAND\n",
    "``` \n",
    "\n",
    "### 865. bloc/blocdenotas.sh\n",
    "\n",
    "```{margin}\n",
    "```bash\n",
    "(base) d@Poseidon 1.ontology % ssh-keygen -t ed25519 -C \"xxzxxxx@gmail.com\" \n",
    "Generating public/private ed25519 key pair.\n",
    "Enter file in which to save the key (/users/d/.ssh/id_ed25519): /users/d/.ssh/id_blocdenotas\n",
    "Enter passphrase (empty for no passphrase): bloc\n",
    "Enter same passphrase again: bloc\n",
    "Your identification has been saved in /users/d/.ssh/id_blocdenotas\n",
    "Your public key has been saved in /users/d/.ssh/id_blocdenotas.pub\n",
    "The key fingerprint is:\n",
    "SHA256:/fSyZhRqN3+8b1Ml7tuAqOLNcHIL/w8UW3uVjZKnQZw muzaale@gmail.com\n",
    "The key's randomart image is:\n",
    "+--[ED25519 256]--+\n",
    "|           ...   |\n",
    "|           .E. .o|\n",
    "|          . = o.o|\n",
    "|         . +.*...|\n",
    "|        S +.+o...|\n",
    "|         .o+++. .|\n",
    "|      + o.oo++o..|\n",
    "|      .X o .ooo++|\n",
    "|     ...*..+o .=*|\n",
    "+----[SHA256]-----+\n",
    "(base) d@Poseidon 1.ontology % cat /users/d/.ssh/id_blocdenotas.pub\n",
    "ssh-ed25519 xxzxxxxxxzxxxxxxzxxxxxxzxxxx muzaale@gmail.com\n",
    "(base) d@Poseidon 1.ontology % eval \"$(ssh-agent -s)\"\n",
    "Agent pid 98437\n",
    "(base) d@Poseidon 1.ontology % pbcopy < ~/.ssh/id_blocdenotas.pub\n",
    "(base) d@Poseidon 1.ontology % chmod 600 ~/.ssh/id_blocdenotas\n",
    "(base) d@Poseidon 1.ontology % git remote -v\n",
    "book    https://github.com/xxzxxxx/book (fetch)\n",
    "book    https://github.com/xxzxxxx/book (push)\n",
    "og      ../og (fetch)\n",
    "og      ../og (push)\n",
    "origin  git@github.com:xxzxxxx/repos (fetch)\n",
    "origin  git@github.com:xxzxxxx/repos (push)\n",
    "source-repo     ../source-repo (fetch)\n",
    "source-repo     ../source-repo (push)\n",
    "(base) d@Poseidon 1.ontology % ssh-add -D\n",
    "All identities removed.\n",
    "(base) d@Poseidon 1.ontology % git remote set-url origin git@github.com:xxzxxxx/denotas\n",
    "(base) d@Poseidon 1.ontology % ssh-add ~/.ssh/id_blocdenotas\n",
    "Enter passphrase for /Users/d/.ssh/id_blocdenotas: bloc \n",
    "Identity added: /Users/d/.ssh/id_blocdenotas (xxzxxxx@gmail.com)\n",
    "```\n",
    "\n",
    "### 866. mb/og \n",
    "\n",
    "```{margin}\n",
    "```bash\n",
    "(base) d@Poseidon 1.ontology % ssh-keygen -t ed25519 -C \"afecdvi@gmail.com\"\n",
    "Generating public/private ed25519 key pair.\n",
    "Enter file in which to save the key (/Users/d/.ssh/id_ed25519): /users/d/.ssh/id_mbog       \n",
    "Enter passphrase (empty for no passphrase): mb\n",
    "Enter same passphrase again: mb\n",
    "Your identification has been saved in /users/d/.ssh/id_mbog\n",
    "Your public key has been saved in /users/d/.ssh/id_mbog.pub\n",
    "The key fingerprint is:\n",
    "SHA256:axiI/wbr/zmV1yz2im1zduE8NklWqdasRtQ5NVNXzb8 afecdvi@gmail.com\n",
    "The key's randomart image is:\n",
    "+--[ED25519 256]--+\n",
    "|               .B|\n",
    "|               o=|\n",
    "|              . B|\n",
    "|   . .       . =o|\n",
    "|  . . . S  ..o+ +|\n",
    "|   ..  o .o ++oE |\n",
    "|    .o. o. ooo* o|\n",
    "|    ...... ooo+B.|\n",
    "|   ..oo.o...+=..o|\n",
    "+----[SHA256]-----+\n",
    "(base) d@Poseidon 1.ontology % cat /users/d/.ssh/id_mbog.pub       \n",
    "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBkT6Eke7PwVt8qj8wXLq+6JP+O6z89g/ksAAi+h3hZ3 afecdvi@gmail.com\n",
    "(base) d@Poseidon 1.ontology % eval \"$(ssh-agent -s)\"\n",
    "Agent pid 99276\n",
    "(base) d@Poseidon 1.ontology % pbcopy < ~/.ssh/id_mbog.pub       \n",
    "(base) d@Poseidon 1.ontology % chmod 600 ~/.ssh/id_mbog       \n",
    "(base) d@Poseidon 1.ontology % git remote -v\n",
    "book    https://github.com/mxxzxxxxx/book (fetch)\n",
    "book    https://github.com/mxxzxxxxx/book (push)\n",
    "og      ../og (fetch)\n",
    "og      ../og (push)\n",
    "origin  git@github.com:mxxzxxxxx/denotas (fetch)\n",
    "origin  git@github.com:mxxzxxxxx/denotas (push)\n",
    "source-repo     ../source-repo (fetch)\n",
    "source-repo     ../source-repo (push)\n",
    "(base) d@Poseidon 1.ontology % ssh-add -D\n",
    "All identities removed.\n",
    "(base) d@Poseidon 1.ontology % git remote set-url origin git@github.com:afecdvi/og     \n",
    "(base) d@Poseidon 1.ontology % ssh-add ~/.ssh/id_mbog       \n",
    "Enter passphrase for /Users/d/.ssh/id_mbog: \n",
    "Identity added: /Users/d/.ssh/id_mbog (afecdvi@gmail.com)\n",
    "(base) d@Poseidon 1.ontology % jb build mb\n",
    "``` \n",
    "\n",
    "### 867. refine\n",
    "\n",
    "+ workflow 6.0\n",
    "+ default ssh key\n",
    "+ lets try this again\n",
    "+ this hasn't worked\n",
    "+ get back to the basics\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Default SSH Key path\n",
    "DEFAULT_SSH_KEY_PATH=\"~/.ssh/id_blocdenotas\"\n",
    "\n",
    "# Prompt user for the path to their private SSH key\n",
    "read -p \"Enter the path to your private SSH key [default: $DEFAULT_SSH_KEY_PATH]: \" SSH_KEY_PATH\n",
    "\n",
    "# If user doesn't input anything, use the default\n",
    "SSH_KEY_PATH=${SSH_KEY_PATH:-$DEFAULT_SSH_KEY_PATH}\n",
    "\n",
    "if [[ ! -f \"$SSH_KEY_PATH\" ]]; then\n",
    "    echo \"Error: SSH key not found at $SSH_KEY_PATH.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Use the specified SSH key for git operations in this script\n",
    "export GIT_SSH_COMMAND=\"ssh -i $SSH_KEY_PATH\"\n",
    "\n",
    "# Change directory to ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Build Jupyter Book with 'gano' as the argument\n",
    "jb build bloc\n",
    "cp -r bloc/* denotas\n",
    "\n",
    "# Change directory to 'denotas'\n",
    "cd denotas\n",
    "\n",
    "# Add all files in the current directory to Git\n",
    "git add ./*\n",
    "\n",
    "# Commit changes to Git with the given commit message\n",
    "git commit -m \"automate updates to denotas\"\n",
    "\n",
    "# Ensure using the SSH URL for the repository\n",
    "git remote set-url origin git@github.com:muzaale/denotas\n",
    "\n",
    "# Push changes to GitHub\n",
    "git push \n",
    "\n",
    "# Import the built HTML to gh-pages and push to GitHub\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "```\n",
    "\n",
    "### 868. reboot\n",
    "\n",
    "+ workflow 7.0\n",
    "+ default ssh key\n",
    "+ lets try this again\n",
    "+ this hasn't worked\n",
    "+ but first status update\n",
    "\n",
    "#### 868.1. githistory.sh\n",
    "\n",
    "1. I have a new repo: https://github.com/jhustata/six100\n",
    "2. There's this file seasons.docx in its main branch\n",
    "3. Lets look at its git history:\n",
    "\n",
    "```\n",
    "History for six100/seasons.docx\n",
    "Commits on Aug 3, 2023\n",
    "import seasons.docx and later its .git history\n",
    "@muzaale\n",
    "muzaale committed 5 hours ago\n",
    "End of commit history for this file\n",
    "```\n",
    "\n",
    "4. Now I wish to transfer the git history from an older repo: https://github.com/afecdvi/og\n",
    "5. Here's what it looks like:\n",
    "\n",
    "```\n",
    "History for og/seasons.docx\n",
    "Commits on Aug 2, 2023\n",
    "send this version to fawaz for review\n",
    "@muzaale\n",
    "muzaale committed yesterday\n",
    "Commits on Aug 1, 2023\n",
    "1. jon synder added as co-author \n",
    "@muzaale\n",
    "muzaale committed 2 days ago\n",
    "Commits on Jul 25, 2023\n",
    "Feedback from Abi on 07/25/2023: mostly stylistic. consider Fourier s… \n",
    "@muzaale\n",
    "muzaale committed last week\n",
    "Commits on Jul 20, 2023\n",
    "first & a half substantive edit of preface, hub/papers, seasons_*.doc… \n",
    "@muzaale\n",
    "muzaale committed 2 weeks ago\n",
    "End of commit history for this file\n",
    "```\n",
    "\n",
    "6. Here's my local machine: \n",
    "\n",
    "```\n",
    "(base) d@Poseidon 1.ontology % pwd\n",
    "/Users/d/Dropbox (Personal)/1f.ἡἔρις,κ/1.ontology\n",
    "(base) d@Poseidon 1.ontology % ls -l\n",
    "total 0\n",
    "drwxr-xr-x@  28 d  staff   896 Aug  3 16:56 _six100_\n",
    "drwxr-xr-x@  21 d  staff   672 Jul 30 17:41 amagunju\n",
    "drwxr-xr-x@ 276 d  staff  8832 Aug  3 15:54 bloc\n",
    "drwxr-xr-x@  18 d  staff   576 Jul 18 04:47 buch\n",
    "drwxr-xr-x@   4 d  staff   128 Aug  2 07:43 content\n",
    "drwxr-xr-x@ 280 d  staff  8960 Aug  3 18:46 denotas\n",
    "drwxr-xr-x@  80 d  staff  2560 Jul 29 08:52 fena\n",
    "drwxr-xr-x@  15 d  staff   480 Aug  1 14:43 fenagas\n",
    "drwxr-xr-x@  13 d  staff   416 Jul 28 20:00 ffena\n",
    "drwxr-xr-x@  22 d  staff   704 Jul 30 16:26 gano\n",
    "drwxr-xr-x@  13 d  staff   416 Jul 27 17:13 kelele\n",
    "drwxr-xr-x@  29 d  staff   928 Jul 20 20:26 libro\n",
    "drwxr-xr-x@ 144 d  staff  4608 Jun 23 23:20 livre\n",
    "drwxr-xr-x@  14 d  staff   448 Aug  3 18:03 llc\n",
    "drwxr-xr-x@  20 d  staff   640 Aug  2 13:18 mb\n",
    "drwxr-xr-x@  12 d  staff   384 Jul 27 16:22 ngoma\n",
    "drwxr-xr-x@  22 d  staff   704 Aug  1 12:59 og\n",
    "drwxr-xr-x@  15 d  staff   480 Jul 31 01:05 repos\n",
    "drwxr-xr-x@  42 d  staff  1344 Aug  3 20:41 six100\n",
    "drwxr-xr-x@  18 d  staff   576 Jul 18 10:57 spring\n",
    "drwxr-xr-x@ 139 d  staff  4448 Jun 25 08:29 summer\n",
    "drwxr-xr-x@  22 d  staff   704 Aug  3 16:51 temp_og_repo\n",
    "drwxr-xr-x@  26 d  staff   832 Aug  3 15:54 three40\n",
    "drwxr-xr-x@  14 d  staff   448 Jul 31 06:24 track\n",
    "drwxr-xr-x@ 102 d  staff  3264 Jul 29 09:28 tusirike\n",
    "drwxr-xr-x@  25 d  staff   800 Jul 20 20:21 verano\n",
    "drwxr-xr-x@  12 d  staff   384 Jul 28 19:59 yaffe\n",
    "(base) d@Poseidon 1.ontology % \n",
    "```\n",
    "\n",
    "7. I want to transfer the git history from og/seasons.docx to six100/seasons.docx\n",
    "8. the directories corresponding are old (og) and new (six100)\n",
    "9. I'll use the following command:\n",
    "\n",
    "```\n",
    "git filter-branch --index-filter \\\n",
    "'git ls-files -s | sed \"s-\\t\\\"*-&six100/-\" |\n",
    "GIT_INDEX_FILE=$GIT_INDEX_FILE.new \\\n",
    "git update-index --index-info &&\n",
    "mv \"$GIT_INDEX_FILE.new\" \"$GIT_INDEX_FILE\"' HEAD\n",
    "```\n",
    "\n",
    "#### 868.2. housekeeping\n",
    "\n",
    "```\n",
    "(base) d@Poseidon 1.ontology % ls -l\n",
    "total 0\n",
    "drwxr-xr-x@  20 d  staff   640 Aug  4 00:20 blank\n",
    "drwxr-xr-x@ 276 d  staff  8832 Aug  3 15:54 bloc\n",
    "drwxr-xr-x@  21 d  staff   672 Aug  4 00:23 canvas\n",
    "drwxr-xr-x@ 280 d  staff  8960 Aug  3 18:46 denotas\n",
    "drwxr-xr-x@  15 d  staff   480 Aug  1 14:43 fenagas\n",
    "drwxr-xr-x@  29 d  staff   928 Jul 20 20:26 libro\n",
    "drwxr-xr-x@ 144 d  staff  4608 Jun 23 23:20 livre\n",
    "drwxr-xr-x@  14 d  staff   448 Aug  3 18:03 llc\n",
    "drwxr-xr-x@  20 d  staff   640 Aug  2 13:18 mb\n",
    "drwxr-xr-x@  22 d  staff   704 Aug  1 12:59 og\n",
    "drwxr-xr-x@  15 d  staff   480 Jul 31 01:05 repos\n",
    "drwxr-xr-x@  18 d  staff   576 Jul 18 10:57 spring\n",
    "drwxr-xr-x@ 139 d  staff  4448 Jun 25 08:29 summer\n",
    "drwxr-xr-x@  14 d  staff   448 Jul 31 06:24 track\n",
    "drwxr-xr-x@  25 d  staff   800 Jul 20 20:21 verano\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08/04/2023\n",
    "\n",
    "### 869. [victory](https://www.youtube.com/watch?v=755whUTJtN4)\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Ensure the script stops on first error\n",
    "set -e\n",
    "\n",
    "# 1. Remove the \"og\" directory\n",
    "rm -rf og\n",
    "\n",
    "# 2. Clone the \"og\" repository\n",
    "git clone https://github.com/afecdvi/og\n",
    "\n",
    "# 3. Navigate to \"og\" and generate patches for seasons.docx\n",
    "cd og\n",
    "echo \"Generating patches for seasons.docx...\"\n",
    "git log --pretty=email --patch-with-stat --reverse -- seasons.docx > seasons.docx.patch\n",
    "\n",
    "# 4. Remove the \"canvas\" directory and clone the new repository\n",
    "cd ..\n",
    "rm -rf canvas\n",
    "git clone https://github.com/muzaale/canvas\n",
    "\n",
    "# 5. Apply patches to the \"canvas\" repository\n",
    "cd canvas\n",
    "echo \"Applying patches to canvas repository...\"\n",
    "git am < ../og/seasons.docx.patch\n",
    "\n",
    "# 6. Setup for SSH push to \"canvas\" repository\n",
    "echo \"Setting up SSH for secure push...\"\n",
    "chmod 600 ~/.ssh/id_blankcanvas\n",
    "ssh-add -D\n",
    "git remote set-url origin git@github.com:muzaale/canvas\n",
    "ssh-add ~/.ssh/id_blankcanvas\n",
    "\n",
    "# Optional: If you're using a remote, push changes to canvas\n",
    "echo \"Pushing changes to remote repository...\"\n",
    "git push\n",
    "\n",
    "# 7. Clean up\n",
    "cd ..\n",
    "rm ../og/seasons.docx.patch\n",
    "rm -rf og\n",
    "\n",
    "echo \"Migration completed successfully!\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 870. gh.sh\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "set -e  # Stop on any error\n",
    "\n",
    "# Variables\n",
    "OG_REPO=\"https://github.com/afecdvi/og\"\n",
    "CANVAS_REPO=\"git@github.com:muzaale/canvas\"\n",
    "SSH_KEY=\"$HOME/.ssh/id_blankcanvas\"\n",
    "FILENAME=\"seasons.docx\"\n",
    "BRANCH_NAME=\"merge_branch\"\n",
    "\n",
    "# Ensure git is installed\n",
    "if ! command -v git &> /dev/null; then\n",
    "    echo \"git could not be found. Please install git.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Navigate to the working directory\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "# mkdir -p workspace_for_merge && cd workspace_for_merge\n",
    "\n",
    "# Set up SSH\n",
    "echo \"Setting up SSH...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "\n",
    "# Clone the 'og' repository\n",
    "echo \"Cloning 'og' repository...\"\n",
    "rm -rf og\n",
    "git clone $OG_REPO og\n",
    "\n",
    "# Navigate to the cloned 'og' repository to fetch commits related to the desired file\n",
    "echo \"Fetching commits related to $FILENAME...\"\n",
    "cd og\n",
    "commits_to_cherry_pick=$(git log --reverse --pretty=format:\"%H\" -- $FILENAME)\n",
    "\n",
    "# Navigate back to the workspace and clone the 'canvas' repository if not already cloned\n",
    "cd ..\n",
    "rm -rf canvas\n",
    "if [ ! -d \"canvas\" ]; then\n",
    "  echo \"Cloning 'canvas' repository...\"\n",
    "  git clone $CANVAS_REPO canvas\n",
    "fi\n",
    "\n",
    "# Navigate to the 'canvas' repository\n",
    "cd canvas\n",
    "\n",
    "# Ensure that we're on the right branch or create one\n",
    "if git show-ref --verify --quiet refs/heads/$BRANCH_NAME; then\n",
    "    git checkout $BRANCH_NAME\n",
    "else\n",
    "    git checkout -b $BRANCH_NAME\n",
    "fi\n",
    "\n",
    "# Cherry-pick commits related to the desired file into the 'canvas' repository\n",
    "for commit in $commits_to_cherry_pick; do\n",
    "    # Cherry-pick each commit\n",
    "    git cherry-pick $commit\n",
    "\n",
    "    # Check for conflicts specifically related to the FILENAME\n",
    "    CONFLICTS=$(git diff --name-only --diff-filter=U | grep $FILENAME)\n",
    "\n",
    "    # If there are conflicts in FILENAME\n",
    "    if [ ! -z \"$CONFLICTS\" ]; then\n",
    "        echo \"Conflict detected in $FILENAME. Please resolve manually.\"\n",
    "        exit 1\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Push the changes\n",
    "echo \"Pushing changes to the 'canvas' repository...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "git remote set-url origin $CANVAS_REPO\n",
    "ssh-add $SSH_KEY\n",
    "git push origin $BRANCH_NAME\n",
    "\n",
    "echo \"Script executed successfully!\"\n",
    "\n",
    "```\n",
    "\n",
    "### 871. gitlog\n",
    "\n",
    "```bash\n",
    "(base) d@Poseidon workspace_for_merge % git log\n",
    "commit e2ca8dc57bb1d35332ad87719e70fb21edec7c77 (HEAD -> merge_branch, main)\n",
    "Author: jhustata <muzaale@jhmi.edu>\n",
    "Date:   Fri Aug 4 00:54:16 2023 -0400\n",
    "\n",
    "    seasons.docx\n",
    "\n",
    "commit 2bdcaf21290f2a34d8aa7177088bbc52296308d2\n",
    "Author: muzaale <muzaale@gmail.com>\n",
    "Date:   Wed Aug 2 13:21:34 2023 -0400\n",
    "\n",
    "    send this version to fawaz for review\n",
    "\n",
    "commit 546f62634d35902e5a03d2a422829ff6d612e728\n",
    "Author: muzaale <muzaale@gmail.com>\n",
    "Date:   Sat Jul 15 11:55:29 2023 -0400\n",
    "\n",
    "    vaughn j brathwaite zoom call\n",
    "\n",
    "commit ac1397deac6cc7cdeca7a207ebe60bd682956846\n",
    "Merge: 2fe97594 228a1c8b\n",
    "Author: muzaale <muzaale@gmail.com>\n",
    "Date:   Sat Jun 24 14:47:32 2023 -0400\n",
    "\n",
    "    cdf of z = 1-sided p\n",
    "(base) d@Poseidon workspace_for_merge % \n",
    "```\n",
    "\n",
    "### 872. workflow7.0\n",
    "\n",
    "#### 872.1. bc.sh\n",
    "\n",
    "```bash\n",
    "pwd\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "# nano bc.sh\n",
    "# chmod +x bc.sh\n",
    "jb build blank \n",
    "git clone https://github.com/muzaale/canvas\n",
    "cp -r blank/* canvas\n",
    "cd canvas\n",
    "git add ./*\n",
    "git commit -m \"iteration... 1\"\n",
    "ssh-keygen -t ed25519 -C \"muzaale@gmail.com\"\n",
    "/users/d/.ssh/id_blankcanvas\n",
    "y\n",
    "blank\n",
    "blank\n",
    "cat /users/d/.ssh/id_blankcanvas.pub\n",
    "eval \"$(ssh-agent -s)\"\n",
    "pbcopy < ~/.ssh/id_blankcanvas.pub\n",
    "chmod 600 ~/.ssh/id_blankcanvas\n",
    "git remote -v\n",
    "ssh-add -D\n",
    "git remote set-url origin git@github.com:muzaale/canvas \n",
    "ssh-add ~/.ssh/id_blankcanvas\n",
    "blank\n",
    "git push \n",
    "ghp-import -n -p -f _build/html\n",
    "cd ..\n",
    "./gh.sh \n",
    "\n",
    "```\n",
    "\n",
    "#### 872.2. gh.sh\n",
    "\n",
    "```bash \n",
    "#!/bin/bash\n",
    "set -e  # Stop on any error\n",
    "\n",
    "# Variables\n",
    "OG_REPO=\"https://github.com/afecdvi/og\"\n",
    "CANVAS_REPO=\"git@github.com:muzaale/canvas\"\n",
    "SSH_KEY=\"$HOME/.ssh/id_blankcanvas\"\n",
    "FILENAME=\"seasons.docx\"\n",
    "BRANCH_NAME=\"merge_branch\"\n",
    "\n",
    "# Ensure git is installed\n",
    "if ! command -v git &> /dev/null; then\n",
    "    echo \"git could not be found. Please install git.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Navigate to the working directory\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "# mkdir -p workspace_for_merge && cd workspace_for_merge\n",
    "\n",
    "# Set up SSH\n",
    "echo \"Setting up SSH...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "\n",
    "# Clone the 'og' repository\n",
    "echo \"Cloning 'og' repository...\"\n",
    "rm -rf og\n",
    "git clone $OG_REPO og\n",
    "\n",
    "# Navigate to the cloned 'og' repository to fetch commits related to the desired file\n",
    "echo \"Fetching commits related to $FILENAME...\"\n",
    "cd og\n",
    "commits_to_cherry_pick=$(git log --reverse --pretty=format:\"%H\" -- $FILENAME)\n",
    "\n",
    "# Navigate back to the workspace and clone the 'canvas' repository if not already cloned\n",
    "cd ..\n",
    "rm -rf canvas\n",
    "if [ ! -d \"canvas\" ]; then\n",
    "  echo \"Cloning 'canvas' repository...\"\n",
    "  git clone $CANVAS_REPO canvas\n",
    "fi\n",
    "\n",
    "# Navigate to the 'canvas' repository\n",
    "cd canvas\n",
    "\n",
    "# Ensure that we're on the right branch or create one\n",
    "if git show-ref --verify --quiet refs/heads/$BRANCH_NAME; then\n",
    "    git checkout $BRANCH_NAME\n",
    "else\n",
    "    git checkout -b $BRANCH_NAME\n",
    "fi\n",
    "\n",
    "# Cherry-pick commits related to the desired file into the 'canvas' repository\n",
    "for commit in $commits_to_cherry_pick; do\n",
    "    # Cherry-pick each commit\n",
    "    git cherry-pick $commit\n",
    "\n",
    "    # Check for conflicts specifically related to the FILENAME\n",
    "    CONFLICTS=$(git diff --name-only --diff-filter=U | grep $FILENAME)\n",
    "\n",
    "    # If there are conflicts in FILENAME\n",
    "    if [ ! -z \"$CONFLICTS\" ]; then\n",
    "        echo \"Conflict detected in $FILENAME. Please resolve manually.\"\n",
    "        exit 1\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Push the changes\n",
    "echo \"Pushing changes to the 'canvas' repository...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "git remote set-url origin $CANVAS_REPO\n",
    "ssh-add $SSH_KEY\n",
    "git push origin $BRANCH_NAME\n",
    "\n",
    "echo \"Script executed successfully!\"\n",
    "\n",
    "```\n",
    "\n",
    "#### 872.3. gitlog\n",
    "\n",
    "+ see above\n",
    "+ make it pretty\n",
    "+ with cherry-pick on top\n",
    "\n",
    "#### 872.4. gc.sh\n",
    "\n",
    "correct, but will seasons.docx in canvas have the commit history of seasons.docx in afecdvi/og? that's my humble plea and goal   \n",
    "\n",
    "---\n",
    "\n",
    "Understood. The answer is no, by using patches or the cherry-picking method described earlier, you will effectively be creating new commits in `canvas` that have the same changes as the original commits in `og`. These new commits will have a different commit hash and won't directly preserve the original commit messages, dates, or authors.\n",
    "\n",
    "However, if you want to *literally* transfer the entire commit history of `seasons.docx` from `afecdvi/og` to `canvas`, it's a bit more involved. Here's a rough outline of one method:\n",
    "\n",
    "1. Create a branch in `og` that contains only the history of `seasons.docx`.\n",
    "2. Turn that branch into its own standalone repository.\n",
    "3. Add the new standalone repository as a remote to `canvas`.\n",
    "4. Fetch and merge the branch from the standalone repository into `canvas`.\n",
    "\n",
    "Here's how you can do this:\n",
    "\n",
    "1. In `og`, filter out the history of `seasons.docx`:\n",
    "\n",
    "```bash\n",
    "cd og\n",
    "git checkout --orphan temp_branch\n",
    "git add seasons.docx\n",
    "git commit -m \"Init seasons.docx\"\n",
    "git rebase --onto temp_branch --root master\n",
    "```\n",
    "\n",
    "2. Clean up and turn `og` into a standalone repo for `seasons.docx`:\n",
    "\n",
    "```bash\n",
    "git gc --prune=all\n",
    "```\n",
    "\n",
    "3. Add the standalone repo as a remote to `canvas` and fetch:\n",
    "\n",
    "```bash\n",
    "cd ../canvas\n",
    "git remote add og_temp ../og\n",
    "git fetch og_temp\n",
    "```\n",
    "\n",
    "4. Merge the history of `seasons.docx` into `canvas`:\n",
    "\n",
    "```bash\n",
    "git merge og_temp/master --allow-unrelated-histories\n",
    "```\n",
    "\n",
    "This will preserve the actual commits and their metadata, but it will effectively merge the histories of the two repositories. This is a non-trivial action and may cause issues if other people are also working on `canvas`. Ensure you understand the ramifications and maybe test on a separate branch or repository first.\n",
    "\n",
    "If this is what you want, I can help further by creating a script. Let me know.\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "set -e  # Stop on any error\n",
    "\n",
    "# Variables\n",
    "OG_REPO=\"https://github.com/afecdvi/og\"\n",
    "CANVAS_REPO=\"git@github.com:muzaale/canvas\"\n",
    "SSH_KEY=\"$HOME/.ssh/id_blankcanvas\"\n",
    "FILENAME=\"seasons.docx\"\n",
    "BRANCH_NAME=\"merge_seasons_docx\"\n",
    "\n",
    "# Ensure git is installed\n",
    "if ! command -v git &> /dev/null; then\n",
    "    echo \"git could not be found. Please install git.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Set up SSH\n",
    "echo \"Setting up SSH...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "\n",
    "# Navigate to the working directory\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Clone the 'og' repository\n",
    "echo \"Cloning 'og' repository...\"\n",
    "rm -rf og_temp\n",
    "git clone $OG_REPO og_temp\n",
    "\n",
    "# Navigate to the cloned 'og' repository and filter out the history of $FILENAME\n",
    "echo \"Filtering out the history of $FILENAME in 'og' repository...\"\n",
    "cd og_temp\n",
    "git checkout --orphan temp_branch\n",
    "git add $FILENAME\n",
    "git commit -m \"Init $FILENAME\"\n",
    "git rebase --onto temp_branch --root master\n",
    "git gc --prune=all\n",
    "\n",
    "# Navigate back to the working directory and clone the 'canvas' repository if not already cloned\n",
    "cd ..\n",
    "if [ ! -d \"canvas\" ]; then\n",
    "  echo \"Cloning 'canvas' repository...\"\n",
    "  git clone $CANVAS_REPO canvas\n",
    "fi\n",
    "\n",
    "# Navigate to the 'canvas' repository\n",
    "cd canvas\n",
    "\n",
    "# Add the filtered 'og' repo as a remote and fetch its commits\n",
    "echo \"Fetching the commits from 'og' repository...\"\n",
    "git remote add og_temp ../og_temp\n",
    "git fetch og_temp\n",
    "\n",
    "# Merge the commits into 'canvas' repo\n",
    "echo \"Merging the commits into 'canvas' repository...\"\n",
    "git checkout -b $BRANCH_NAME\n",
    "git merge og_temp/temp_branch --allow-unrelated-histories\n",
    "\n",
    "# Push the changes to the 'canvas' repository\n",
    "echo \"Pushing changes to the 'canvas' repository...\"\n",
    "git remote set-url origin $CANVAS_REPO\n",
    "ssh-add $SSH_KEY\n",
    "git push origin $BRANCH_NAME\n",
    "\n",
    "echo \"Script executed successfully!\"\n",
    "\n",
    "```\n",
    "\n",
    "#### 873. workflow7.3\n",
    "\n",
    "Here's a revised script that consolidates the instructions and incorporates your feedback:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "set -e  # Stop on any error\n",
    "\n",
    "# Variables\n",
    "OG_REPO=\"https://github.com/afecdvi/og\"\n",
    "CANVAS_REPO=\"git@github.com:muzaale/canvas\"\n",
    "SSH_KEY=\"$HOME/.ssh/id_blankcanvas\"\n",
    "FILENAME=\"seasons.docx\"\n",
    "BRANCH_NAME=\"merge_seasons_docx\"\n",
    "\n",
    "# Ensure git is installedå\n",
    "if ! command -v git &> /dev/null; then\n",
    "    echo \"git could not be found. Please install git.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Set up SSH\n",
    "echo \"Setting up SSH...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "\n",
    "# Navigate to the working directory\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Clone the 'og' repository\n",
    "echo \"Cloning 'og' repository...\"\n",
    "rm -rf og_temp\n",
    "git clone $OG_REPO og_temp\n",
    "\n",
    "# Navigate to the cloned 'og' repository and filter out the history of $FILENAME\n",
    "echo \"Filtering out the history of $FILENAME in 'og' repository...\"\n",
    "cd og_temp\n",
    "git checkout --orphan temp_branch\n",
    "git add $FILENAME\n",
    "git commit -m \"Init $FILENAME\"\n",
    "git rebase --onto temp_branch --root master\n",
    "git branch -D master\n",
    "git branch -m temp_branch master\n",
    "git gc --prune=all\n",
    "\n",
    "# Navigate back to the working directory \n",
    "cd ..\n",
    "\n",
    "# Clone the 'canvas' repository if not already cloned\n",
    "if [ ! -d \"canvas\" ]; then\n",
    "  echo \"Cloning 'canvas' repository...\"\n",
    "  git clone $CANVAS_REPO canvas\n",
    "fi\n",
    "\n",
    "# Navigate to the 'canvas' repository and set up for the merge\n",
    "cd canvas\n",
    "\n",
    "# Add the filtered 'og' repo as a remote and fetch its commits\n",
    "echo \"Fetching the commits from 'og' repository...\"\n",
    "git remote add og_temp ../og_temp\n",
    "git fetch og_temp\n",
    "\n",
    "# Ensure you're on the main branch of canvas before creating a new branch\n",
    "git checkout main\n",
    "\n",
    "# Create a new branch for the merge\n",
    "git checkout -b $BRANCH_NAME\n",
    "\n",
    "# Merge the commits from the 'og' repository\n",
    "echo \"Merging the commits into 'canvas' repository...\"\n",
    "git merge og_temp/master --allow-unrelated-histories\n",
    "\n",
    "# Push the new branch to the 'canvas' repository\n",
    "echo \"Pushing changes to the 'canvas' repository...\"\n",
    "git push origin $BRANCH_NAME\n",
    "\n",
    "echo \"Script executed successfully!\"\n",
    "```\n",
    "\n",
    "The script should now be able to:\n",
    "\n",
    "1. Setup SSH.\n",
    "2. Clone the 'og' repository.\n",
    "3. Filter out the history of the specified file (`FILENAME`) from 'og'.\n",
    "4. Clone the 'canvas' repository (if not already cloned).\n",
    "5. Fetch the filtered history of 'og'.\n",
    "6. Merge the filtered history into a new branch in 'canvas'.\n",
    "7. Push the new branch to the 'canvas' remote.\n",
    "\n",
    "Do make sure you have permissions to push to the 'canvas' repo and that the SSH key (`$SSH_KEY`) has been added to your GitHub account or the respective account hosting the repositories.\n",
    "\n",
    "### 874. [blankcanvas.sh](https://muzaale.github.io/canvas/intro.html)\n",
    "\n",
    "+ literally what it is\n",
    "+ but its header segues into a delightful place\n",
    "+ and it's a good place to be\n",
    "\n",
    "### 875. database \n",
    "\n",
    "Design a database schema for an online merch store.\n",
    "\n",
    "---\n",
    "\n",
    "Certainly! Below is a basic database schema for an online merchandise store. This schema covers key features but can be expanded or adjusted based on specific requirements.\n",
    "\n",
    "### 1. Tables:\n",
    "\n",
    "**1. Users**\n",
    "- UserID (Primary Key, Auto-increment)\n",
    "- Username\n",
    "- Email (Unique)\n",
    "- Password (hashed)\n",
    "- Firstname\n",
    "- Lastname\n",
    "- DateJoined\n",
    "- LastLoginDate\n",
    "\n",
    "**2. Products**\n",
    "- ProductID (Primary Key, Auto-increment)\n",
    "- ProductName\n",
    "- Description\n",
    "- Price\n",
    "- StockQuantity\n",
    "- CategoryID (Foreign Key)\n",
    "- ImageURL\n",
    "\n",
    "**3. Categories**\n",
    "- CategoryID (Primary Key, Auto-increment)\n",
    "- CategoryName\n",
    "- Description\n",
    "\n",
    "**4. Orders**\n",
    "- OrderID (Primary Key, Auto-increment)\n",
    "- UserID (Foreign Key)\n",
    "- OrderDate\n",
    "- ShippingAddressID (Foreign Key)\n",
    "- OrderStatus\n",
    "\n",
    "**5. OrderDetails**\n",
    "- OrderDetailID (Primary Key, Auto-increment)\n",
    "- OrderID (Foreign Key)\n",
    "- ProductID (Foreign Key)\n",
    "- QuantityOrdered\n",
    "- PriceEach\n",
    "\n",
    "**6. Addresses**\n",
    "- AddressID (Primary Key, Auto-increment)\n",
    "- UserID (Foreign Key)\n",
    "- StreetAddress1\n",
    "- StreetAddress2\n",
    "- City\n",
    "- State\n",
    "- PostalCode\n",
    "- Country\n",
    "- IsDefault\n",
    "\n",
    "**7. Cart**\n",
    "- CartID (Primary Key, Auto-increment)\n",
    "- UserID (Foreign Key)\n",
    "\n",
    "**8. CartItems**\n",
    "- CartItemID (Primary Key, Auto-increment)\n",
    "- CartID (Foreign Key)\n",
    "- ProductID (Foreign Key)\n",
    "- Quantity\n",
    "\n",
    "**9. Reviews**\n",
    "- ReviewID (Primary Key, Auto-increment)\n",
    "- UserID (Foreign Key)\n",
    "- ProductID (Foreign Key)\n",
    "- Rating (e.g., 1-5)\n",
    "- Comment\n",
    "- ReviewDate\n",
    "\n",
    "### 2. Relationships:\n",
    "\n",
    "- **Users** and **Orders**: One-to-Many\n",
    "  - A user can place multiple orders, but each order is placed by one user.\n",
    "  \n",
    "- **Users** and **Addresses**: One-to-Many\n",
    "  - A user can have multiple shipping addresses, but each address belongs to one user.\n",
    "  \n",
    "- **Products** and **Categories**: Many-to-One\n",
    "  - A product belongs to one category, but each category can have multiple products.\n",
    "  \n",
    "- **Orders** and **OrderDetails**: One-to-Many\n",
    "  - An order can have multiple products (order details), but each order detail refers to one order.\n",
    "  \n",
    "- **Users** and **Reviews**: One-to-Many\n",
    "  - A user can leave multiple reviews, but each review is written by one user.\n",
    "  \n",
    "- **Products** and **Reviews**: One-to-Many\n",
    "  - A product can have multiple reviews, but each review pertains to one product.\n",
    "  \n",
    "- **Users** and **Cart**: One-to-One\n",
    "  - A user has one cart, and each cart belongs to one user.\n",
    "  \n",
    "- **Cart** and **CartItems**: One-to-Many\n",
    "  - A cart can have multiple cart items, but each cart item belongs to one cart.\n",
    "\n",
    "### 3. Indexes:\n",
    "\n",
    "Indexes should be created on columns that are frequently searched or joined to enhance query performance. For example:\n",
    "- `Email` in **Users**\n",
    "- `ProductName` and `CategoryID` in **Products**\n",
    "- `UserID` in **Orders**, **Addresses**, **Reviews**, and **Cart**\n",
    "\n",
    "This schema provides a foundational structure for an online merch store. Depending on the specific requirements and features needed (e.g., wish lists, promotional codes, affiliate tracking, etc.), additional tables and fields may be necessary.\n",
    "\n",
    "---\n",
    "\n",
    "bring this to life even by simulation if necessary: given me python code\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Creating a real-world application with the above schema involves multiple layers including front-end, back-end, and database interactions. I'll provide a simplified simulation in Python that focuses on the backend aspect, using SQLite as the database for ease of demonstration.\n",
    "\n",
    "1. **Setting Up the Database**:\n",
    "```python\n",
    "import sqlite3\n",
    "\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect(\"merch_store.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Users table\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS Users (\n",
    "        UserID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Username TEXT NOT NULL,\n",
    "        Email TEXT UNIQUE NOT NULL,\n",
    "        Password TEXT NOT NULL,\n",
    "        Firstname TEXT,\n",
    "        Lastname TEXT,\n",
    "        DateJoined DATE,\n",
    "        LastLoginDate DATE\n",
    "    )''')\n",
    "    \n",
    "    # Remaining tables ... (similarly create tables for Products, Categories, etc.)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "```\n",
    "\n",
    "2. **Inserting Data** (Using Users as an example):\n",
    "```python\n",
    "def add_user(username, email, password, firstname, lastname):\n",
    "    conn = sqlite3.connect(\"merch_store.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "    INSERT INTO Users (Username, Email, Password, Firstname, Lastname, DateJoined, LastLoginDate)\n",
    "    VALUES (?, ?, ?, ?, ?, CURRENT_DATE, CURRENT_DATE)\n",
    "    ''', (username, email, password, firstname, lastname))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "```\n",
    "\n",
    "3. **Fetching Data** (Again, using Users as an example):\n",
    "```python\n",
    "def fetch_user(email):\n",
    "    conn = sqlite3.connect(\"merch_store.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"SELECT * FROM Users WHERE Email=?\", (email,))\n",
    "    user = cursor.fetchone()\n",
    "    \n",
    "    conn.close()\n",
    "    return user\n",
    "```\n",
    "\n",
    "4. **Main Program Flow**:\n",
    "```python\n",
    "def main():\n",
    "    setup_database()\n",
    "    \n",
    "    # Adding a sample user\n",
    "    add_user(\"john_doe\", \"john.doe@example.com\", \"securepassword\", \"John\", \"Doe\")\n",
    "    \n",
    "    # Fetching and displaying the user\n",
    "    user = fetch_user(\"john.doe@example.com\")\n",
    "    print(user)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "This is a very basic setup. In a real-world application:\n",
    "\n",
    "- Passwords would be hashed (e.g., using `bcrypt`).\n",
    "- Error handling and input validation would be essential.\n",
    "- Interaction with other tables and relationships would be implemented.\n",
    "- A RESTful API, frontend, and more advanced backend features would be added.\n",
    "- ORM (Object Relational Mapping) tools like SQLAlchemy could be used to simplify database operations.\n",
    "\n",
    "Always remember to be cautious with handling sensitive information, especially passwords. Always use industry-approved cryptographic methods for storage and transmission.\n",
    "\n",
    "### 876. adversarial-learning\n",
    "\n",
    "```bash\n",
    "User\n",
    "brief schpiel on my gc.sh file: #!/bin/bash\n",
    "\n",
    "set -e  # Stop on any error\n",
    "\n",
    "# Variables\n",
    "OG_REPO=\"https://github.com/afecdvi/og\"\n",
    "CANVAS_REPO=\"git@github.com:muzaale/canvas\"\n",
    "SSH_KEY=\"$HOME/.ssh/id_blankcanvas\"\n",
    "FILENAME=\"seasons.docx\"\n",
    "BRANCH_NAME=\"merge_seasons_docx\"\n",
    "\n",
    "# Ensure git is installedå\n",
    "if ! command -v git &> /dev/null; then\n",
    "    echo \"git could not be found. Please install git.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Set up SSH\n",
    "echo \"Setting up SSH...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "\n",
    "# Navigate to the working directory\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Clone the 'og' repository\n",
    "echo \"Cloning 'og' repository...\"\n",
    "rm -rf og_temp\n",
    "git clone $OG_REPO og_temp\n",
    "\n",
    "# Navigate to the cloned 'og' repository and filter out the history of $FILENAME\n",
    "echo \"Filtering out the history of $FILENAME in 'og' repository...\"\n",
    "cd og_temp\n",
    "git checkout --orphan temp_branch\n",
    "git add $FILENAME\n",
    "git commit -m \"Init $FILENAME\"\n",
    "git rebase --onto temp_branch --root master\n",
    "git branch -D master\n",
    "git branch -m temp_branch master\n",
    "git gc --prune=all\n",
    "\n",
    "# Navigate back to the working directory \n",
    "cd ..\n",
    "\n",
    "# Clone the 'canvas' repository if not already cloned\n",
    "if [ ! -d \"canvas\" ]; then\n",
    "  echo \"Cloning 'canvas' repository...\"\n",
    "  git clone $CANVAS_REPO canvas\n",
    "fi\n",
    "\n",
    "# Navigate to the 'canvas' repository and set up for the merge\n",
    "cd canvas\n",
    "\n",
    "# Add the filtered 'og' repo as a remote and fetch its commits\n",
    "echo \"Fetching the commits from 'og' repository...\"\n",
    "git remote add og_temp ../og_temp\n",
    "git fetch og_temp\n",
    "\n",
    "# Ensure you're on the main branch of canvas before creating a new branch\n",
    "git checkout main\n",
    "\n",
    "# Create a new branch for the merge\n",
    "git checkout -b $BRANCH_NAME\n",
    "\n",
    "# Merge the commits from the 'og' repository\n",
    "echo \"Merging the commits into 'canvas' repository...\"\n",
    "git merge og_temp/master --allow-unrelated-histories\n",
    "\n",
    "# Push the new branch to the 'canvas' repository\n",
    "echo \"Pushing changes to the 'canvas' repository...\"\n",
    "git push origin $BRANCH_NAME\n",
    "\n",
    "echo \"Script executed successfully!\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "The provided `gc.sh` script seems to be designed to merge the history of a specific file (`seasons.docx`) from one Git repository (`og`) into another Git repository (`canvas`). Below is a breakdown of what the script does:\n",
    "\n",
    "1. **Stop on Any Error**: Using `set -e`, the script will halt if any command fails.\n",
    "   \n",
    "2. **Variables**: There are several predefined variables that configure the script:\n",
    "   - `OG_REPO`: The original repository's URL.\n",
    "   - `CANVAS_REPO`: The destination repository's URL.\n",
    "   - `SSH_KEY`: The SSH key location for Git authentication.\n",
    "   - `FILENAME`: The file name to filter from the original repo.\n",
    "   - `BRANCH_NAME`: The name of the new branch where the file's history will be merged into the canvas repository.\n",
    "\n",
    "3. **Git Installation Check**: Checks if Git is installed on the machine. If not, it throws an error.\n",
    "\n",
    "4. **Setting up SSH**: Sets up SSH by starting the agent, making sure the provided SSH key is only readable by the owner (for security), and adding the key to the agent.\n",
    "\n",
    "5. **Navigation**: The script then navigates to a specified working directory.\n",
    "\n",
    "6. **Clone and Filter `og` Repository**: \n",
    "   - Clones the original repository into a temporary directory (`og_temp`).\n",
    "   - Then, it navigates into this temporary directory and uses a combination of Git commands to isolate the history of `seasons.docx` while getting rid of the other files and their histories.\n",
    "\n",
    "7. **Clone `canvas` Repository**: If the `canvas` repository hasn't been cloned yet, it will clone it.\n",
    "\n",
    "8. **Set Up for Merge in `canvas` Repository**:\n",
    "   - Navigates to the `canvas` repo directory.\n",
    "   - Adds the filtered `og_temp` repo as a remote source.\n",
    "   - Fetches its commits.\n",
    "   - Checks out the main branch and creates a new branch (`merge_seasons_docx`).\n",
    "\n",
    "9. **Merge**: The script merges the isolated `seasons.docx` history from `og_temp` repository into the new branch of `canvas` repository.\n",
    "\n",
    "10. **Push**: Pushes the changes of the new branch to the remote `canvas` repository.\n",
    "\n",
    "11. **Completion**: A message \"Script executed successfully!\" is displayed once all the commands are executed without error.\n",
    "\n",
    "### Takeaways:\n",
    "- **Purpose**: The script's primary purpose is to transplant the history of a specific file from one repository into another repository, maintaining its commit history.\n",
    "  \n",
    "- **Usability**: The script is well-structured with echo statements which guide the user through its steps, making it user-friendly.\n",
    "  \n",
    "- **Security**: The script uses SSH for Git operations. Make sure that the provided SSH key has the right permissions on GitHub or whichever platform hosts the repositories.\n",
    "\n",
    "- **Precautions**: The script makes several changes that are hard or impossible to revert, like force cloning over directories and altering Git history. It's always a good practice to backup your repositories before running such scripts.\n",
    "\n",
    "--- \n",
    "\n",
    "how would you improve gc.sh? my work directory:   \n",
    "\n",
    "```\n",
    "(base) d@Poseidon 1.ontology % pwd\n",
    "/Users/d/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "(base) d@Poseidon 1.ontology % ls -l\n",
    "total 24\n",
    "-rw-r--r--@   1 d  staff   601 Aug  4 08:06 bc.sh\n",
    "drwxr-xr-x@  21 d  staff   672 Aug  4 00:38 blank\n",
    "drwxr-xr-x@ 276 d  staff  8832 Aug  3 15:54 bloc\n",
    "drwxr-xr-x@  22 d  staff   704 Aug  4 08:16 canvas\n",
    "drwxr-xr-x@ 280 d  staff  8960 Aug  3 18:46 denotas\n",
    "drwxr-xr-x@  15 d  staff   480 Aug  1 14:43 fenagas\n",
    "-rwxr-xr-x@   1 d  staff  1952 Aug  4 08:41 gc.sh\n",
    "-rwxr-xr-x@   1 d  staff  1952 Aug  4 08:41 gh.sh\n",
    "drwxr-xr-x@  29 d  staff   928 Jul 20 20:26 libro\n",
    "drwxr-xr-x@ 144 d  staff  4608 Jun 23 23:20 livre\n",
    "drwxr-xr-x@  14 d  staff   448 Aug  4 09:47 llc\n",
    "drwxr-xr-x@  20 d  staff   640 Aug  2 13:18 mb\n",
    "drwxr-xr-x@  22 d  staff   704 Aug  4 08:16 og\n",
    "drwxr-xr-x@  15 d  staff   480 Jul 31 01:05 repos\n",
    "drwxr-xr-x@  18 d  staff   576 Jul 18 10:57 spring\n",
    "drwxr-xr-x@ 139 d  staff  4448 Jun 25 08:29 summer\n",
    "drwxr-xr-x@  14 d  staff   448 Jul 31 06:24 track\n",
    "drwxr-xr-x@  25 d  staff   800 Jul 20 20:21 verano\n",
    "(base) d@Poseidon 1.ontology % \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Given the structure of the current `gc.sh` script and the layout of your working directory, here are some improvements you can consider:\n",
    "\n",
    "1. **Parameterization**: Instead of hardcoding values in the script, accept them as arguments. This makes your script more versatile. For instance, if you need to merge a different file or branch in the future, you can easily do that without modifying the script.\n",
    "\n",
    "2. **Error Handling**: Improve error handling. For instance, ensure directories exist before navigating into them and handle the case where they don't.\n",
    "\n",
    "3. **Consistent Directory Naming**: Instead of hardcoding directory names like \"canvas\" and \"og_temp\", consider using consistent naming or even generating temporary directory names to avoid any clashes or unintentional overwrites.\n",
    "\n",
    "4. **Check Existing Remotes**: Before adding a new remote, check if it already exists to avoid errors.\n",
    "\n",
    "5. **Cleanup**: At the end of your script, consider cleaning up any temporary changes, like removing temporary directories or remotes.\n",
    "\n",
    "6. **Logging**: Introduce logging so you can troubleshoot or verify actions later on. \n",
    "\n",
    "7. **Comments**: While your script is quite self-explanatory, especially with the echo commands, adding comments for complex operations can aid future readers or yourself when revisiting the script.\n",
    "\n",
    "8. **Avoid Hard Exit**: Instead of directly exiting the script when a certain condition isn't met, consider printing a warning and then proceed with the rest of the script or provide alternatives.\n",
    "\n",
    "Here's a revised version of the script with some of these suggestions implemented:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "set -e  # Stop on any error\n",
    "\n",
    "# Variables\n",
    "OG_REPO=${1:-\"https://github.com/afecdvi/og\"}\n",
    "CANVAS_REPO=${2:-\"git@github.com:muzaale/canvas\"}\n",
    "SSH_KEY=${3:-\"$HOME/.ssh/id_blankcanvas\"}\n",
    "FILENAME=${4:-\"seasons.docx\"}\n",
    "BRANCH_NAME=${5:-\"merge_seasons_docx\"}\n",
    "\n",
    "# Ensure git is installed\n",
    "if ! command -v git &> /dev/null; then\n",
    "    echo \"WARNING: git could not be found. Please install git.\"\n",
    "else\n",
    "    # Set up SSH\n",
    "    echo \"Setting up SSH...\"\n",
    "    eval \"$(ssh-agent -s)\"\n",
    "    chmod 600 $SSH_KEY\n",
    "    ssh-add -D\n",
    "    ssh-add $SSH_KEY\n",
    "\n",
    "    # Navigate to the working directory\n",
    "    cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "    # Clone the 'og' repository\n",
    "    echo \"Cloning 'og' repository...\"\n",
    "    TEMP_DIR=\"og_temp_$(date +%s)\"\n",
    "    rm -rf $TEMP_DIR\n",
    "    git clone $OG_REPO $TEMP_DIR\n",
    "\n",
    "    # Filter out the history of $FILENAME\n",
    "    cd $TEMP_DIR\n",
    "    git checkout --orphan temp_branch\n",
    "    git add $FILENAME\n",
    "    git commit -m \"Init $FILENAME\"\n",
    "    git rebase --onto temp_branch --root master\n",
    "    git branch -D master\n",
    "    git branch -m temp_branch master\n",
    "    git gc --prune=all\n",
    "\n",
    "    # Navigate back to the working directory \n",
    "    cd ..\n",
    "\n",
    "    # Clone the 'canvas' repository if not already cloned\n",
    "    if [ ! -d \"canvas\" ]; then\n",
    "        echo \"Cloning 'canvas' repository...\"\n",
    "        git clone $CANVAS_REPO canvas\n",
    "    fi\n",
    "\n",
    "    cd canvas\n",
    "\n",
    "    # Add the filtered 'og' repo as a remote and fetch its commits\n",
    "    if ! git remote | grep -q \"og_temp\"; then\n",
    "        git remote add og_temp ../$TEMP_DIR\n",
    "    fi\n",
    "    git fetch og_temp\n",
    "\n",
    "    git checkout main\n",
    "    git checkout -b $BRANCH_NAME\n",
    "    git merge og_temp/master --allow-unrelated-histories\n",
    "\n",
    "    git push origin $BRANCH_NAME\n",
    "\n",
    "    # Cleanup\n",
    "    git remote remove og_temp\n",
    "    cd ..\n",
    "    rm -rf $TEMP_DIR\n",
    "fi\n",
    "\n",
    "echo \"Script executed successfully!\"\n",
    "```\n",
    "\n",
    "Remember, the changes above are illustrative. Your actual requirements and constraints will dictate which changes make sense and which don't. Always back up your data before running modified scripts or other operations.\n",
    "\n",
    "### 877. fatal-error\n",
    "\n",
    "It looks like you're trying to run some Git operations and ran into the error: `fatal: no such branch/commit 'master'`. This is likely because the default branch is not named `master`.\n",
    "\n",
    "Here are some steps to troubleshoot and resolve the issue:\n",
    "\n",
    "1. **Check Current Branches**: First, let's see the list of branches you have. Run:\n",
    "    ```bash\n",
    "    git branch -a\n",
    "    ```\n",
    "   This command will list all the branches in your local repo as well as remote-tracking branches.\n",
    "\n",
    "2. **Default Branch Name**: Git repositories created after October 1, 2020, on platforms like GitHub use `main` as the default branch name instead of `master`. If you see `main` in your branch list and no `master`, this is likely the issue. Replace any reference to `master` in your scripts or commands with `main`.\n",
    "\n",
    "3. **Reference the Correct Branch**: If neither `master` nor `main` are listed in your branches, identify the correct default branch from the output of the `git branch -a` command and use that instead.\n",
    "\n",
    "4. **Update Your Script**: If you're using a script similar to the `gc.sh` you shared earlier, find the line where `master` is referenced (like in the `git merge og_temp/master --allow-unrelated-histories` command) and replace it with the correct branch name.\n",
    "\n",
    "5. **Consider Renaming**: If you prefer to stick with the `master` naming convention (or any other name), and if you have the necessary permissions, you can rename the default branch:\n",
    "    ```bash\n",
    "    git branch -m old_branch_name new_branch_name\n",
    "    ```\n",
    "   Replace `old_branch_name` with the current default branch name (like `main`) and `new_branch_name` with the desired name (like `master`).\n",
    "\n",
    "6. **Sync with Remote**: If you renamed the branch, don't forget to push the changes to the remote repository and set the upstream branch:\n",
    "    ```bash\n",
    "    git push -u origin new_branch_name\n",
    "    ```\n",
    "\n",
    "Make sure to update any scripts, CI/CD configurations, or documentation that refers to the old branch name after renaming.\n",
    "\n",
    "### 878. onemoreiteration\n",
    "\n",
    "Certainly, I can see you're trying to simplify and get back to the version of `gc.sh` that had worked well for you previously, with just the necessary changes to accommodate the \"Not a valid object name HEAD\" error.\n",
    "\n",
    "The main issue seems to be the occurrence of merge conflicts due to your current working branch having changes (or a different history) than what you're trying to merge in. Since this script automates a significant set of operations, when something goes wrong in between, the outcome can be tricky.\n",
    "\n",
    "Let's update the original `gc.sh` to address the aforementioned error and potential merge conflicts:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "set -e  # Stop on any error\n",
    "\n",
    "# Variables\n",
    "OG_REPO=${1:-\"https://github.com/afecdvi/og\"}\n",
    "CANVAS_REPO=${2:-\"git@github.com:muzaale/canvas\"}\n",
    "SSH_KEY=${3:-\"$HOME/.ssh/id_blankcanvas\"}\n",
    "FILENAME=${4:-\"seasons.docx\"}\n",
    "BRANCH_NAME=${5:-\"merge_seasons_docx\"}\n",
    "\n",
    "# Ensure git is installed\n",
    "if ! command -v git &> /dev/null; then\n",
    "    echo \"WARNING: git could not be found. Please install git.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Set up SSH\n",
    "echo \"Setting up SSH...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 $SSH_KEY\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEY\n",
    "\n",
    "# Navigate to the working directory\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Clone the 'og' repository and filter its history\n",
    "echo \"Cloning 'og' repository and filtering history for $FILENAME...\"\n",
    "TEMP_DIR=\"og_temp_$(date +%s)\"\n",
    "rm -rf $TEMP_DIR\n",
    "git clone $OG_REPO $TEMP_DIR\n",
    "cd $TEMP_DIR\n",
    "git filter-branch --prune-empty --index-filter \"\n",
    "    git rm --cached --ignore-unmatch *;\n",
    "    if [ -f '$FILENAME' ]; then\n",
    "        git add $FILENAME;\n",
    "    fi\n",
    "\" main\n",
    "\n",
    "# Return to main directory\n",
    "cd ../\n",
    "\n",
    "# Check if the 'canvas' repository is present and if not, clone it\n",
    "echo \"Processing 'canvas' repository...\"\n",
    "if [ ! -d \"canvas\" ]; then\n",
    "    git clone $CANVAS_REPO canvas\n",
    "fi\n",
    "\n",
    "cd canvas\n",
    "\n",
    "# Recovery steps if HEAD is disconnected\n",
    "if ! git symbolic-ref HEAD &> /dev/null; then\n",
    "    echo \"Reconnecting HEAD...\"\n",
    "    echo ref: refs/heads/main > .git/HEAD\n",
    "fi\n",
    "\n",
    "# Add the filtered 'og' repo as a remote and fetch its commits\n",
    "if ! git remote | grep -q \"og_temp\"; then\n",
    "    git remote add og_temp ../$TEMP_DIR\n",
    "fi\n",
    "git fetch og_temp\n",
    "\n",
    "# Merge the filtered branch into a new branch\n",
    "git checkout -b $BRANCH_NAME\n",
    "git merge og_temp/main --allow-unrelated-histories\n",
    "\n",
    "# If merge conflict occurs\n",
    "if [ \"$(git ls-files -u | wc -l)\" -ne 0 ]; then\n",
    "    echo \"ERROR: Merge conflict detected. Please resolve manually.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "git push origin $BRANCH_NAME\n",
    "\n",
    "# Cleanup\n",
    "git remote remove og_temp\n",
    "cd ..\n",
    "rm -rf $TEMP_DIR\n",
    "\n",
    "echo \"Script executed successfully!\"\n",
    "```\n",
    "\n",
    "Changes made:\n",
    "\n",
    "1. Reintroduced the check for disconnected HEAD and reconnected it if needed.\n",
    "2. Introduced a simple check for merge conflicts right after the merge. If conflicts occur, the script stops and advises you to solve them manually.\n",
    "\n",
    "This script should be more stable, especially in your current working context. However, given the complexity of operations and potential variances in the state of repositories, you might still run into challenges. Always backup your work and run the script in a test environment if possible.\n",
    "\n",
    "### 879. alpha-beta\n",
    "\n",
    "0. cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "1. track [repos](https://muzaale.github.io/repos/intro.html)\n",
    "2. edit track\n",
    "3. commit in `./track/trackrepos.sh`\n",
    "4. vigilante\n",
    "5. jhutrc/beta\n",
    "6. https://github.com/jhutrc/beta\n",
    "7. ssh-keygen -t ed25519 -C \"muzaale@icloud.com\"\n",
    "8. /users/d/.ssh/id_alphabeta\n",
    "9. alpha\n",
    "10. alpha\n",
    "11. cat /users/d/.ssh/id_alphabeta.pub\n",
    "12. eval \"$(ssh-agent -s)\"\n",
    "13. pbcopy < ~/.ssh/id_alphabeta.pub\n",
    "14. chmod 600 ~/.ssh/id_alphabeta\n",
    "15. git remote -v\n",
    "16. ssh-add -D\n",
    "17. git remote set-url origin git@github.com:jhutrc/beta\n",
    "18. ssh-add ~/.ssh/id_alphabeta\n",
    "19. alpha\n",
    "20. https://muzaale.github.io/denotas/bdn202308.html#yesterday\n",
    "21. mkdir -p alpha\n",
    "22. nano alpha/_toc.yml\n",
    "23. chatgpt\n",
    "24. nano ./alpha.sh\n",
    "25. chmod +x alpha.sh\n",
    "26. ./alpha.sh\n",
    "27. nano alpha/_config.yml\n",
    "28. alpha/hub_and_spoke.jpg\n",
    "29. nano alpha/alphabeta.sh\n",
    "30. chmod +x alpha/alphabeta.sh\n",
    "31. cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "32. chmod +x alpha/alphabeta.sh\n",
    "33. cd alpha\n",
    "34. vscode > file > new file > alpha/alpha.ipynb\n",
    "35. find . -name \"*.ipynb\" -exec cp \"alpha.ipynb\" {} \\;\n",
    "36. cd ..\n",
    "37. cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "38. git clone https://github.com/jhutrc/beta\n",
    "39. jb build alpha\n",
    "40. cp -r alpha/* beta\n",
    "41. cd beta\n",
    "42. git add ./*\n",
    "43. git commit -m \"third jb created manually\"\n",
    "44. chmod 600 ~/.ssh/id_alphabeta\n",
    "45. ssh-add -D\n",
    "46. git remote set-url origin git@github.com:jhutrc/beta\n",
    "47. ssh-add ~/.ssh/id_alphabeta\n",
    "48. alpha\n",
    "49. git push\n",
    "50. ghp-import -n -p -f _build/html\n",
    "\n",
    "### 880. directory\n",
    "\n",
    "```bash\n",
    "   8d4a34c..465ee23  gh-pages -> gh-pages\n",
    "(base) d@Poseidon 1.ontology % ls -l alpha\n",
    "total 120\n",
    "drwxr-xr-x@  5 d  staff    160 Aug  4 13:55 Act I\n",
    "drwxr-xr-x@  6 d  staff    192 Aug  4 13:55 Act II\n",
    "drwxr-xr-x@  7 d  staff    224 Aug  4 13:55 Act III\n",
    "drwxr-xr-x@  8 d  staff    256 Aug  4 13:55 Act IV\n",
    "drwxr-xr-x@  8 d  staff    256 Aug  4 13:55 Act V\n",
    "drwxr-xr-x@  4 d  staff    128 Aug  4 13:55 Courses\n",
    "drwxr-xr-x@ 10 d  staff    320 Aug  4 13:55 Epilogue\n",
    "drwxr-xr-x@  5 d  staff    160 Aug  4 13:55 Git & Spoke\n",
    "drwxr-xr-x@  5 d  staff    160 Aug  4 14:18 _build\n",
    "-rw-r--r--@  1 d  staff    950 Aug  4 14:05 _config.yml\n",
    "-rw-r--r--@  1 d  staff   5429 Aug  4 13:40 _toc.yml\n",
    "-rw-r--r--@  1 d  staff    228 Aug  4 14:11 alpha.ipynb\n",
    "-rwxr-xr-x@  1 d  staff  11723 Aug  4 13:55 alpha.sh\n",
    "-rwxr-xr-x@  1 d  staff    308 Aug  4 14:27 alphabeta.sh\n",
    "drwxr-xr-x@ 12 d  staff    384 Aug  4 13:55 dramatispersonae\n",
    "-rw-r--r--@  1 d  staff  17905 Aug  3 18:03 hub_and_spoke.jpg\n",
    "-rw-r--r--@  1 d  staff    228 Aug  4 14:13 intro.ipynb\n",
    "-rw-r--r--@  1 d  staff    228 Aug  4 14:13 prologue.ipynb\n",
    "(base) d@Poseidon 1.ontology % \n",
    "```\n",
    "\n",
    "### 881. [intro](https://jhutrc.github.io/beta/intro.html)\n",
    "\n",
    "51. populate directories\n",
    "52. alpha/alphabeta.sh\n",
    "53. fix image paths\n",
    "54. .ipynb per dpersonae\n",
    "55. _toc.yml bugs\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Change the working directory to the desired location\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "\n",
    "# Create the \"alpha\" directory\n",
    "# mkdir -p alpha\n",
    "# nano alpha/_toc.yml\n",
    "\n",
    "# Create the \"Root\" folder and the \"intro.ipynb\" file inside it\n",
    "touch \"alpha/intro.ipynb\"\n",
    "\n",
    "# Create the \"prologue.ipynb\" file in the \"alpha\" directory\n",
    "touch \"alpha/prologue.ipynb\"\n",
    "\n",
    "# Create \"Act I\" folder and its subfiles\n",
    "mkdir -p \"alpha/Act I\"\n",
    "touch \"alpha/Act I/act1_1.ipynb\"\n",
    "touch \"alpha/Act I/act1_2.ipynb\"\n",
    "touch \"alpha/Act I/act1_3.ipynb\"\n",
    "\n",
    "# Create \"Act II\" folder and its subfiles\n",
    "mkdir -p \"alpha/Act II\"\n",
    "touch \"alpha/Act II/act2_1.ipynb\"\n",
    "touch \"alpha/Act II/act2_2.ipynb\"\n",
    "touch \"alpha/Act II/act2_3.ipynb\"\n",
    "touch \"alpha/Act II/act2_4.ipynb\"\n",
    "\n",
    "# Create \"Act III\" folder and its subfiles\n",
    "mkdir -p \"alpha/Act III\"\n",
    "touch \"alpha/Act III/act3_1.ipynb\"\n",
    "touch \"alpha/Act III/act3_2.ipynb\"\n",
    "touch \"alpha/Act III/act3_3.ipynb\"\n",
    "touch \"alpha/Act III/act3_4.ipynb\"\n",
    "touch \"alpha/Act III/act3_5.ipynb\"\n",
    "\n",
    "# Create \"Act IV\" folder and its subfiles\n",
    "mkdir -p \"alpha/Act IV\"\n",
    "touch \"alpha/Act IV/act4_1.ipynb\"\n",
    "touch \"alpha/Act IV/act4_2.ipynb\"\n",
    "touch \"alpha/Act IV/act4_3.ipynb\"\n",
    "touch \"alpha/Act IV/act4_4.ipynb\"\n",
    "touch \"alpha/Act IV/act4_5.ipynb\"\n",
    "touch \"alpha/Act IV/act4_6.ipynb\"\n",
    "\n",
    "# Create \"Act V\" folder and its subfiles\n",
    "mkdir -p \"alpha/Act V\"\n",
    "touch \"alpha/Act V/act5_1.ipynb\"\n",
    "touch \"alpha/Act V/act5_2.ipynb\"\n",
    "touch \"alpha/Act V/act5_3.ipynb\"\n",
    "touch \"alpha/Act V/act5_4.ipynb\"\n",
    "touch \"alpha/Act V/act5_5.ipynb\"\n",
    "touch \"alpha/Act V/act5_6.ipynb\"\n",
    "\n",
    "# Create \"Epilogue\" folder and its subfiles\n",
    "mkdir -p \"alpha/Epilogue\"\n",
    "touch \"alpha/Epilogue/epi_1.ipynb\"\n",
    "touch \"alpha/Epilogue/epi_2.ipynb\"\n",
    "touch \"alpha/Epilogue/epi_3.ipynb\"\n",
    "touch \"alpha/Epilogue/epi_4.ipynb\"\n",
    "touch \"alpha/Epilogue/epi_5.ipynb\"\n",
    "touch \"alpha/Epilogue/epi_6.ipynb\"\n",
    "touch \"alpha/Epilogue/epi_7.ipynb\"\n",
    "touch \"alpha/Epilogue/epi_8.ipynb\"\n",
    "\n",
    "# Create \"Git & Spoke\" folder and its subfiles\n",
    "mkdir -p \"alpha/Git & Spoke\"\n",
    "touch \"alpha/Git & Spoke/gas_1.ipynb\"\n",
    "touch \"alpha/Git & Spoke/gas_2.ipynb\"\n",
    "touch \"alpha/Git & Spoke/gas_3.ipynb\"\n",
    "\n",
    "# Create \"Courses\" folder and its subfiles\n",
    "mkdir -p \"alpha/Courses\"\n",
    "touch \"alpha/Courses/course1.ipynb\"\n",
    "touch \"alpha/Courses/course2.ipynb\"\n",
    "\n",
    "# Create \"dramatis_personae\" folder and its subdirectories\n",
    "mkdir -p \"alpha/dramatis_personae/high_school_students\"\n",
    "mkdir -p \"alpha/dramatis_personae/under_grads\"\n",
    "mkdir -p \"alpha/dramatis_personae/grad_students”\n",
    "mkdir -p \"alpha/dramatis_personae/graduates\"\n",
    "mkdir -p \"alpha/dramatis_personae/medical_students\"\n",
    "mkdir -p \"alpha/dramatis_personae/residents\"\n",
    "mkdir -p \"alpha/dramatis_personae/fellows\"\n",
    "mkdir -p \"alpha/dramatis_personae/faculty\"\n",
    "mkdir -p \"alpha/dramatis_personae/analysts\"\n",
    "mkdir -p \"alpha/dramatis_personae/staff\"\n",
    "mkdir -p \"alpha/dramatis_personae/collaborators\"\n",
    "\n",
    "# ... (rest of the script follows the same pattern) ...\n",
    "\n",
    "# Create \"dramatis_personae\" subdirectories with suffixes _1 to _5\n",
    "for branch in high_school_students under_grads graduates medical_students residents fellows faculty analysts staff collaborators; do\n",
    "    for ((i=1; i<=5; i++)); do\n",
    "        mkdir -p \"alpha/dramatis_personae/${branch}/${branch}_${i}\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create additional .ipynb files inside specific subdirectories\n",
    "touch \"alpha/dramatis_personae/high_school_students/high_school_students.ipynb\"\n",
    "touch \"alpha/dramatis_personae/under_grads/under_grads.ipynb\"\n",
    "touch \"alpha/dramatis_personae/grad_students/grad_students.ipynb\"\n",
    "touch \"alpha/dramatis_personae/graduates/graduates.ipynb\"\n",
    "touch \"alpha/dramatis_personae/medical_students/medical_students.ipynb\"\n",
    "touch \"alpha/dramatis_personae/residents/residents.ipynb\"\n",
    "touch \"alpha/dramatis_personae/fellows/fellows.ipynb\"\n",
    "touch \"alpha/dramatis_personae/faculty/faculty.ipynb\"\n",
    "touch \"alpha/dramatis_personae/analysts/analysts.ipynb\"\n",
    "touch \"alpha/dramatis_personae/staff/staff.ipynb\"\n",
    "touch \"alpha/dramatis_personae/collaborators/collaborators.ipynb\"\n",
    "touch \"alpha/dramatis_personae/high_school_students/high_school_students_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/high_school_students/high_school_students_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/high_school_students/high_school_students_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/high_school_students/high_school_students_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/high_school_students/high_school_students_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/under_grads/under_grads_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/under_grads/under_grads_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/under_grads/under_grads_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/under_grads/under_grads_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/under_grads/under_grads_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/grad_students/grad_students_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/grad_students/grad_students_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/grad_students/grad_students_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/grad_students/grad_students_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/grad_students/grad_students_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/graduates/graduates_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/graduates/graduates_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/graduates/graduates_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/graduates/graduates_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/graduates/graduates_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/medical_students/medical_students_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/medical_students/medical_students_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/medical_students/medical_students_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/medical_students/medical_students_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/medical_students/medical_students_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/residents/residents_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/residents/residents_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/residents/residents_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/residents/residents_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/residents/residents_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/fellows/fellows_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/fellows/fellows_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/fellows/fellows_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/fellows/fellows_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/fellows/fellows_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/faculty/faculty_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/faculty/faculty_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/faculty/faculty_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/faculty/faculty_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/faculty/faculty_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/analysts/analysts_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/analysts/analysts_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/analysts/analysts_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/analysts/analysts_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/analysts/analysts_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/staff/staff_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/staff/staff_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/staff/staff_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/staff/staff_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/staff/staff_5.ipynb\"\n",
    "touch \"alpha/dramatis_personae/collaborators/collaborators_1.ipynb\"\n",
    "touch \"alpha/dramatis_personae/collaborators/collaborators_2.ipynb\"\n",
    "touch \"alpha/dramatis_personae/collaborators/collaborators_3.ipynb\"\n",
    "touch \"alpha/dramatis_personae/collaborators/collaborators_4.ipynb\"\n",
    "touch \"alpha/dramatis_personae/collaborators/collaborators_5.ipynb\"\n",
    "\n",
    "# Display the directory tree\n",
    "echo \"Directory Structure:\"\n",
    "echo \"-------------------\"\n",
    "echo \"alpha/\n",
    "├── intro.ipynb\n",
    "├── prologue.ipynb\n",
    "├── Act I/\n",
    "│   ├── act1_1.ipynb\n",
    "│   ├── act1_2.ipynb\n",
    "│   ├── act1_3.ipynb\n",
    "│   └── ...\n",
    "├── Act II/\n",
    "│   ├── act2_1.ipynb\n",
    "│   ├── act2_2.ipynb\n",
    "│   └── ...\n",
    "├── Act III/\n",
    "│   ├── act3_1.ipynb\n",
    "│   ├── act3_2.ipynb\n",
    "│   ├── act3_3.ipynb\n",
    "│   ├── act3_4.ipynb\n",
    "│   └── act3_5.ipynb\n",
    "├── Act IV/\n",
    "│   ├── act4_1.ipynb\n",
    "│   ├── act4_2.ipynb\n",
    "│   ├── act4_3.ipynb\n",
    "│   ├── act4_4.ipynb\n",
    "│   ├── act4_5.ipynb\n",
    "│   └── act4_6.ipynb\n",
    "├── Act V/\n",
    "│   ├── act5_1.ipynb\n",
    "│   ├── act5_2.ipynb\n",
    "│   ├── act5_3.ipynb\n",
    "│   ├── act5_4.ipynb\n",
    "│   ├── act5_5.ipynb\n",
    "│   └── act5_6.ipynb\n",
    "├── Epilogue/\n",
    "│   ├── epi_1.ipynb\n",
    "│   ├── epi_2.ipynb\n",
    "│   ├── epi_3.ipynb\n",
    "│   ├── epi_4.ipynb\n",
    "│   ├── epi_5.ipynb\n",
    "│   ├── epi_6.ipynb\n",
    "│   ├── epi_7.ipynb\n",
    "│   └── epi_8.ipynb\n",
    "├── Gas & Spoke/\n",
    "│   ├── gas_1.ipynb\n",
    "│   ├── gas_2.ipynb\n",
    "│   └── gas_3.ipynb\n",
    "└── dramatis_personae/\n",
    "    ├── high_school_students/\n",
    "    │   ├── high_school_students_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── high_school_students_4/\n",
    "    │   │   └── ...\n",
    "    │   └── high_school_students_5/\n",
    "    │       └── ...\n",
    "    ├── under_grads/\n",
    "    │   ├── under_grads_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── under_grads_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── under_grads_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── under_grads_4/\n",
    "    │   │   └── ...\n",
    "    │   └── under_grads_5/\n",
    "    │       └── ...\n",
    "    ├── grad_students/\n",
    "    │   ├── grad_students_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── grad_students_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── grad_students_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── grad_students_4/\n",
    "    │   │   └── ...\n",
    "    │   └── grad_students_5/\n",
    "    │       └── ...\n",
    "    ├── graduates/\n",
    "    │   ├── graduates_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── graduates_4/\n",
    "    │   │   └── ...\n",
    "    │   └── graduates_5/\n",
    "    │       └── ...\n",
    "    ├── medical_students/\n",
    "    │   ├── medical_students_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── medical_students_4/\n",
    "    │   │   └── ...\n",
    "    │   └── medical_students_5/\n",
    "    │       └── ...\n",
    "    ├── residents/\n",
    "    │   ├── residents_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── residents_4/\n",
    "    │   │   └── ...\n",
    "    │   └── residents_5/\n",
    "    │       └── ...\n",
    "    ├── fellows/\n",
    "    │   ├── fellows_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── fellows_4/\n",
    "    │   │   └── ...\n",
    "    │   └── fellows_5/\n",
    "    │       └── ...\n",
    "    ├── faculty/\n",
    "    │   ├── faculty_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── faculty_4/\n",
    "    │   │   └── ...\n",
    "    │   └── faculty_5/\n",
    "    │       └── ...\n",
    "    ├── analysts/\n",
    "    │   ├── analysts_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── analysts_4/\n",
    "    │   │   └── ...\n",
    "    │   └── analysts_5/\n",
    "    │       └── ...\n",
    "    ├── staff/\n",
    "    │   ├── staff_1/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_2/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_3/\n",
    "    │   │   └── ...\n",
    "    │   ├── staff_4/\n",
    "    │   │   └── ...\n",
    "    │   └── staff_5/\n",
    "    │       └── ...\n",
    "    └── collaborators/\n",
    "        ├── collaborators_1/\n",
    "        │   └── ...\n",
    "        ├── collaborators_2/\n",
    "        │   └── ...\n",
    "        ├── collaborators_3/\n",
    "        │   └── ...\n",
    "        ├── collaborators_4/\n",
    "        │   └── ...\n",
    "        └── collaborators_5/\n",
    "            └── ...\"\n",
    "echo \"Folder structure has been created successfully.\"\n",
    "mv alpha.sh alpha/alpha.sh\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "format: jb-book\n",
    "root: intro.ipynb\n",
    "title: Play\n",
    "\n",
    "parts:\n",
    "- caption: \n",
    "  chapters:\n",
    "  - file: prologue.ipynb\n",
    "\n",
    "- caption: Act I\n",
    "  chapters:\n",
    "  - file: Act I/act1_1.ipynb\n",
    "  - file: Act I/act1_2.ipynb\n",
    "  - file: Act I/act1_3.ipynb\n",
    "\n",
    "- caption: Act II\n",
    "  chapters:\n",
    "  - file: Act II/act2_1.ipynb\n",
    "  - file: Act II/act2_2.ipynb\n",
    "  - file: Act II/act2_3.ipynb\n",
    "  - file: Act II/act2_4.ipynb\n",
    "\n",
    "- caption: Act III\n",
    "  chapters:\n",
    "  - file: Act III/act3_1.ipynb\n",
    "  - file: Act III/act3_2.ipynb\n",
    "  - file: Act III/act3_3.ipynb\n",
    "  - file: Act III/act3_4.ipynb\n",
    "  - file: Act III/act3_5.ipynb\n",
    "\n",
    "- caption: Act IV\n",
    "  chapters:\n",
    "  - file: Act IV/act4_1.ipynb\n",
    "  - file: Act IV/act4_2.ipynb\n",
    "  - file: Act IV/act4_3.ipynb\n",
    "  - file: Act IV/act4_4.ipynb\n",
    "  - file: Act IV/act4_5.ipynb\n",
    "  - file: Act IV/act4_6.ipynb\n",
    "\n",
    "- caption: Act V\n",
    "  chapters:\n",
    "  - file: Act V/act5_1.ipynb\n",
    "  - file: Act V/act5_2.ipynb\n",
    "  - file: Act V/act5_3.ipynb\n",
    "  - file: Act V/act5_4.ipynb\n",
    "  - file: Act V/act5_5.ipynb\n",
    "  - file: Act V/act5_6.ipynb\n",
    "\n",
    "- caption: Epilogue\n",
    "  chapters:\n",
    "  - file: Epilogue/epi_1.ipynb\n",
    "  - file: Epilogue/epi_2.ipynb\n",
    "  - file: Epilogue/epi_3.ipynb\n",
    "  - file: Epilogue/epi_4.ipynb\n",
    "  - file: Epilogue/epi_5.ipynb\n",
    "  - file: Epilogue/epi_6.ipynb\n",
    "  - file: Epilogue/epi_7.ipynb\n",
    "  - file: Epilogue/epi_8.ipynb\n",
    "\n",
    "- caption: Gas & Spoke\n",
    "  chapters:\n",
    "  - file: Gas & Spoke/gas_1.ipynb\n",
    "  - file: Gas & Spoke/gas_2.ipynb\n",
    "  - file: Gas & Spoke/gas_3.ipynb\n",
    "\n",
    "- caption: Courses\n",
    "  chapters: \n",
    "  - url: https://publichealth.jhu.edu/courses\n",
    "    title: Stata Programming \n",
    "  - file: dramatis_personae/high_school_students/high_school_students.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_1/high_school_students_1.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_1/high_school_students_1_1.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_2.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_3.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_4.ipynb\n",
    "  - file: dramatis_personae/high_school_students/high_school_students_5.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_1.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_2.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_3.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_4.ipynb\n",
    "  - file: dramatis_personae/under_grads/under_grads_5.ipynb\n",
    "  - file: dramatis_personae/grad_students/grad_students.ipynb\n",
    "  - file: dramatis_personae/grad_students_1/grad_students_1.ipynb\n",
    "  - file: dramatis_personae/grad_students_2/grad_students_2.ipynb\n",
    "  - file: dramatis_personae/grad_students_3/grad_students_3.ipynb\n",
    "  - file: dramatis_personae/grad_students_4/grad_students_4.ipynb\n",
    "  - file: dramatis_personae/grad_students_5/grad_students_5.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_1/medical_students_1.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_1/medical_students_1_1.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_1/medical_students_1_2.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_2.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_3.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_4.ipynb\n",
    "  - file: dramatis_personae/medical_students/medical_students_5.ipynb\n",
    "  - file: dramatis_personae/residents/residents.ipynb\n",
    "  - file: dramatis_personae/residents/residents_1.ipynb\n",
    "  - file: dramatis_personae/residents/residents_2.ipynb\n",
    "  - file: dramatis_personae/residents/residents_3.ipynb\n",
    "  - file: dramatis_personae/residents/residents_4.ipynb\n",
    "  - file: dramatis_personae/residents/residents_5.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_1.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_2.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_3.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_4.ipynb\n",
    "  - file: dramatis_personae/fellows/fellows_5.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_1/faculty_1.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_2/faculty_2.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_3/faculty_3.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_4/faculty_4.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_5/faculty_5.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_6/faculty_6.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_7/faculty_7.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_8/faculty_8.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_9/faculty_9.ipynb\n",
    "  - file: dramatis_personae/faculty/faculty_9/faculty_9_1.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_1.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_2.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_3.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_4.ipynb\n",
    "  - file: dramatis_personae/analysts/analysts_5.ipynb\n",
    "  - file: dramatis_personae/staff/staff.ipynb\n",
    "  - file: dramatis_personae/staff/staff_1.ipynb\n",
    "  - file: dramatis_personae/staff/staff_2.ipynb\n",
    "  - file: dramatis_personae/staff/staff_3.ipynb\n",
    "  - file: dramatis_personae/staff/staff_4.ipynb\n",
    "  - file: dramatis_personae/staff/staff_5.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_1/collaborators_1.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_1/collaborators_1_1.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_1/collaborators_1_2.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_2.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_3.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_4.ipynb\n",
    "  - file: dramatis_personae/collaborators/collaborators_5.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_1.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_2.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_3.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_4.ipynb\n",
    "  - file: dramatis_personae/graduates/graduates_5.ipynb\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "[High School Students](./dramatis_personae/high_school_students/high_school_students.ipynb)          \n",
    "[Undergraduates](./dramatis_personae/under_grads/under_grads.ipynb)    \n",
    "[Graduate Students](./dramatis_personae/grad_students/grad_students.ipynb)      \n",
    "[Medical Students](./dramatis_personae/medical_students/medical_students.ipynb)   \n",
    "[Residents](./dramatis_personae/residents/residents.ipynb)     \n",
    "[Fellows](./dramatis_personae/fellows/fellows.ipynb)    \n",
    "[Faculty](./dramatis_personae/faculty/faculty.ipynb)    \n",
    "[Analysts](./dramatis_personae/analysts/analysts.ipynb)    \n",
    "[Staff](./dramatis_personae/staff/staff.ipynb)    \n",
    "[Collaborators](./dramatis_personae/collaborators/collaborators.ipynb)    \n",
    "[Graduates](./dramatis_personae/graduates/graduates.ipynb) \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "jb build alpha\n",
    "cp -r alpha/* beta\n",
    "cd beta\n",
    "git add ./*\n",
    "git commit -m \"evolution of COVID-19 variants\"\n",
    "chmod 600 ~/.ssh/id_alphabeta\n",
    "ssh-add -D\n",
    "git remote set-url origin git@github.com:jhutrc/beta\n",
    "ssh-add ~/.ssh/id_alphabeta\n",
    "alpha\n",
    "git push\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08/05/2023\n",
    "\n",
    "### 882. workflow 8.0\n",
    "\n",
    "I see, you'd like to automate the creation of a directory and file structure based on some predefined parameters, such as the number of acts, the number of files in each act, and the categories under \"dramatispersonae\". \n",
    "\n",
    "You can make your script more generic and less manual by using loops and arrays to generate these structures. Here's a simplified version:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "root_dir=\"alpha\"\n",
    "acts=( \"Act I\" \"Act II\" \"Act III\" \"Act IV\" \"Act V\" )\n",
    "act_files=( 3 4 5 6 6 ) # change these values according to the number of files in each act\n",
    "branches=( \"high_school_students\" \"undergraduates\" \"graduates\" \"medical_students\" \"residents\" \"fellows\" \"faculty\" \"analysts\" \"staff\" \"collaborators\" )\n",
    "\n",
    "# Create the root directory\n",
    "mkdir -p $root_dir\n",
    "\n",
    "# Create intro.ipynb and prologue.ipynb\n",
    "touch \"$root_dir/intro.ipynb\"\n",
    "touch \"$root_dir/prologue.ipynb\"\n",
    "\n",
    "# Create Act directories and their files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    act_dir=\"$root_dir/${acts[$i]}\"\n",
    "    mkdir -p $act_dir\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        touch \"$act_dir/act${i+1}_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create the dramatispersonae directories and their files\n",
    "for branch in \"${branches[@]}\"; do\n",
    "    for ((i=1; i<=5; i++)); do\n",
    "        sub_dir=\"$root_dir/dramatispersonae/$branch/${branch}_$i\"\n",
    "        mkdir -p $sub_dir\n",
    "        touch \"$sub_dir/${branch}_$i.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# additional directories and files can be generated following similar pattern\n",
    "# ...\n",
    "```\n",
    "\n",
    "With this script, if you need to change the number of files in each Act, or add/remove Acts, or add/remove branches, you just need to modify the corresponding array at the top of the script, rather than adding/removing lines for each file/directory.\n",
    "\n",
    "Please note that you'll need to add additional lines for other folders like \"Epilogue\", \"Git & Spoke\", \"Courses\", etc., following similar pattern to the \"Act\" folders or the \"dramatispersonae\" folders as appropriate. \n",
    "\n",
    "I tried to cover the main points in your script and simplify the logic for those parts. If you have some more specific requirements, let me know and I can help you adjust the script.\n",
    "\n",
    "### 881. Calvin\n",
    "\n",
    "1. Stanford med school for the summer \n",
    "2. Pediatric cardiocular engineering\n",
    "3. Wet lab in this? Apprently so \n",
    "4. Focusing on congenital heart disease\n",
    "5. Creating extension to monitor heart function \n",
    "6. Glucose monitoring \n",
    "7. Here till August\n",
    "8. \"Our school is very weird\": hurricane season causes school to start in August\n",
    "9. Andrew busy with immunology and is unable to meet & mentor Calvin\n",
    "10. Calvin is a good guy (according to `GitHub CoPilot`)\n",
    "11. I guess I concurred with CoPilot\n",
    "12. Because of `Fena` and `CoPilot`, I should be able to spend more time with Calvin\n",
    "\n",
    "### 882. workflow 8.1\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# inflection point!\n",
    "\n",
    "# Set root directory\n",
    "root_dir=\"$HOME/dropbox/1f.ἡἔρις,κ/1.ontology/be\"\n",
    "\n",
    "acts=(\"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\")\n",
    "act_files=(5 5 5 5 5) # change these values according to the number of files in each act\n",
    "dramatis_personae=(\"dp_1\" \"dp_2\" \"dp_3\" \"dp_4\" \"dp_5\" \"dp_6\" \"dp_7\" \"dp_8\" \"dp_9\")\n",
    "\n",
    "# Create the root directory if not exists\n",
    "mkdir -p $root_dir\n",
    "\n",
    "# Create intro.ipynb and prologue.ipynb\n",
    "touch \"${root_dir}/be_0_0.ipynb\"\n",
    "touch \"${root_dir}/be_0_1.ipynb\"\n",
    "\n",
    "# Create Act directories and their files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    act_dir=\"${root_dir}/${acts[$i]}\"\n",
    "    mkdir -p $act_dir\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        touch \"${act_dir}/${acts[$i]}_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create the dramatis_personae directories and their files\n",
    "for ((i=0; i<${#dramatis_personae[@]}; i++)); do\n",
    "    for ((j=1; j<=5; j++)); do\n",
    "        dp_dir=\"${root_dir}/dramatis_personae/${dramatis_personae[$i]}\"\n",
    "        mkdir -p $dp_dir\n",
    "        touch \"${dp_dir}/${dramatis_personae[$i]}_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# additional directories and files can be generated following similar pattern\n",
    "```\n",
    "\n",
    "### 883. workflow 8.2\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# inflection point!\n",
    "# chmod +x ip.sh\n",
    "# folders appeared in parent directory\n",
    "# this should be fixed to be in the ./be/ directory\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology\n",
    "root_dir=\"be\"\n",
    "acts=( \"be_0\" \"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\" \"be_6\")\n",
    "act_files=(2 3 4 5 6 7 8) # change these values according to the number of files in each act\n",
    "# dramatis_personae=( \"dp_1\" \"dp_2\" \"dp_3\" \"dp_4\" \"dp_5\" \"dp_6\" \"dp_7\" \"dp_8\" \"dp_9\")\n",
    "\n",
    "# Create the root directory\n",
    "mkdir -p $root_dir\n",
    "# cd $root_dir\n",
    "\n",
    "# Create intro.ipynb and prologue.ipynb\n",
    "touch \"$root_dir/be_0_0.ipynb\"\n",
    "touch \"$root_dir/be_0_1.ipynb\"\n",
    "\n",
    "# Create Act directories and their files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    act_dir=\"$root_dir/${acts[$i]}\"\n",
    "    mkdir -p $act_dir\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        touch \"$act_dir/be_i${i+1}_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create the dramatispersonae directories and their files\n",
    "for branch in \"${branches[@]}\"; do\n",
    "    for ((i=1; i<=5; i++)); do\n",
    "        sub_dir=\"$root_dir/dp/$branch/${branch}_$i\"\n",
    "        mkdir -p $sub_dir\n",
    "        touch \"$sub_dir/${branch}_$i.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# additional directories and files can be generated following similar pattern\n",
    "# ...\n",
    "\n",
    "```\n",
    "\n",
    "Herein we populate an empty directory with the following structure:\n",
    "\n",
    "```bash \n",
    "be\n",
    "├── be_0\n",
    "│   ├── be_0_0.ipynb\n",
    "│   └── be_0_1.ipynb\n",
    "├── be_1\n",
    "│   ├── be_i1_1.ipynb\n",
    "│   ├── be_i1_2.ipynb\n",
    "│   └── be_i1_3.ipynb\n",
    "├── be_2\n",
    "│   ├── be_i2_1.ipynb\n",
    "│   ├── be_i2_2.ipynb\n",
    "│   ├── be_i2_3.ipynb\n",
    "│   └── be_i2_4.ipynb\n",
    "├── be_3\n",
    "│   ├── be_i3_1.ipynb\n",
    "│   ├── be_i3_2.ipynb\n",
    "│   ├── be_i3_3.ipynb\n",
    "│   ├── be_i3_4.ipynb\n",
    "│   └── be_i3_5.ipynb\n",
    "├── be_4\n",
    "│   ├── be_i4_1.ipynb\n",
    "│   ├── be_i4_2.ipynb\n",
    "│   ├── be_i4_3.ipynb\n",
    "│   ├── be_i4_4.ipynb\n",
    "│   ├── be_i4_5.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "So the _config.yml will look something like this:\n",
    "\n",
    "```yaml\n",
    "\n",
    "# Site settings\n",
    "title: \"ἡἔρις,κ\"\n",
    "description: \"ἡἔρις,κ\"\n",
    "baseurl: \"/1f.ἡἔρις,κ\"\n",
    "url: \"\n",
    "\n",
    "# Build settings\n",
    "markdown: kramdown\n",
    "theme: jekyll-theme-cayman\n",
    "plugins:\n",
    "  - jekyll-feed\n",
    "  - jekyll-seo-tag\n",
    "  - jekyll-sitemap\n",
    "  - jekyll-remote-theme\n",
    "remote_theme: \"mmistakes/minimal-mistakes\"\n",
    "```\n",
    "\n",
    "And the _toc.yml will look something like this:\n",
    "\n",
    "```yaml\n",
    "\n",
    "- title: \"ἡἔρις,κ\"\n",
    "  url: /1f.ἡἔρις,κ/\n",
    "  output: web\n",
    "  folderitems:\n",
    "    - title: \"be\"\n",
    "      url: /1f.ἡἔρις,κ/be/\n",
    "      output: web\n",
    "      folderitems:\n",
    "        - title: \"be_0\"\n",
    "          url: /1f.ἡἔρις,κ/be/be_0/\n",
    "          output: web\n",
    "          folderitems:\n",
    "            - title: \"be_0_0\"\n",
    "              url: /1f.ἡἔρις,κ/be/be_0/be_0_0/\n",
    "              output: web\n",
    "            - title: \"be_0_1\"\n",
    "              url: /1f.ἡἔρις,κ/be/be_0/be_0_1/\n",
    "              output: web\n",
    "        - title: \"be_1\"\n",
    "          url: /1f.ἡἔρις,κ/be/be_1/\n",
    "          output: web\n",
    "          folderitems:\n",
    "            - title: \"be_i1_1\"\n",
    "              url: /1f.ἡἔρις,κ/be/be_1/be_i1_1/\n",
    "              output: web\n",
    "            - title: \"be_i1_2\"\n",
    "              url: /1f.ἡἔρις,κ/be/be_1/be_i1_2/\n",
    "              output: web\n",
    "            - title: \"be_i1_3\"\n",
    "              url: /1f.ἡἔρις,κ/be/be_1/be_i1_3/\n",
    "              output: web\n",
    "        - title: \"be_2\"\n",
    "          url: /1f.ἡἔρις,κ/be/be_2/\n",
    "          output: web\n",
    "          folderitems:\n",
    "            - title: \"be_i2_1\"\n",
    "              url: /1f.ἡἔρις,κ/be/be_2/be_i2_1/\n",
    "              output: web\n",
    "            - title: \"be_i2_2\"\n",
    "              url: /1f.ἡἔρις,κ/be/be_\n",
    "```\n",
    "\n",
    "All the above are 100% suggestions from `Fena` and `CoPilot`. I have no idea what I am doing. (The last statement is `100%` from me. -- no... from `CoPilot`)\n",
    "\n",
    "### 884. ip.sh\n",
    "\n",
    "- this signals an inflection point in human history\n",
    "- so I am going to call it `ip.sh`\n",
    "- `chmod +x ip.sh`\n",
    "\n",
    "Certainly! I apologize for the confusion. Below is a complete and continuous script, combining everything you need to create directories, files, authentication keys, and push HTML content to the `gh-pages` branch of your repository.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# Generate a new SSH key (replace with your email)\n",
    "ssh-keygen -t ed25519 -C \"muzaale@gmail.com\" -f ~/.ssh/id_trackrepos\n",
    "\n",
    "# Start the ssh-agent and add the key\n",
    "eval \"$(ssh-agent -s)\"\n",
    "ssh-add ~/.ssh/id_trackrepos\n",
    "\n",
    "# Copy the public key to your clipboard\n",
    "pbcopy < ~/.ssh/id_trackrepos.pub\n",
    "echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "\n",
    "# Define arrays for acts and the number of files for each act\n",
    "acts=(\"be_0\" \"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\" \"be_6\")\n",
    "act_files=(2 3 4 5 6 7 8)\n",
    "\n",
    "# Create Act directories and their corresponding files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    mkdir -p ${acts[$i]}\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        touch \"${acts[$i]}/be_i$(($i + 1))_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _toc.yml file\n",
    "toc_file=\"_toc.yml\"\n",
    "echo \"\" > $toc_file\n",
    "\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    echo \"- part: Part $(($i + 1))\" >> $toc_file\n",
    "    echo \"  chapters:\" >> $toc_file\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        echo \"    - file: ${acts[$i]}/be_i$(($i + 1))_$j\" >> $toc_file\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create __config.yml file\n",
    "config_file=\"__config.yml\"\n",
    "echo \"title: Your Book Title\" > $config_file\n",
    "echo \"author: Your Name\" >> $config_file\n",
    "echo \"logo: logo.png\" >> $config_file\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "jupyter-book build .\n",
    "\n",
    "# Clone the repository\n",
    "git clone git@github.com:muzaale/fe.git ~/dropbox/1f.ἡἔρις,κ/1.ontology/fe\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology/fe\n",
    "\n",
    "# Create or switch to the gh-pages branch\n",
    "git checkout -b gh-pages\n",
    "\n",
    "# Copy the HTML files from the build directory to the repository\n",
    "cp -r ~/dropbox/1f.ἡἔρις,κ/1.ontology/be/_build/html/* .\n",
    "\n",
    "# Add, commit, and push changes to GitHub\n",
    "git add .\n",
    "git commit -m \"0.0 Update Jupyter Book content\"\n",
    "git push -u origin gh-pages\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to muzaale/fe repository!\"\n",
    "```\n",
    "\n",
    "You can save this script as a `.sh` file and run it using `bash yourscript.sh`. Make sure to follow the printed instruction to add the SSH public key to your GitHub account. Ensure that the necessary tools like `jupyter-book`, `ssh-keygen`, `pbcopy`, and `git` are installed on your system.\n",
    "\n",
    "### 885. ip.sh-i2\n",
    "\n",
    "+ creates directories and files\n",
    "+ creates _toc.yml\n",
    "+ creates __config.yml\n",
    "+ `fails to build` the book with Jupyter Book \n",
    "+ clones the repository\n",
    "+ creates or switches to the gh-pages branch\n",
    "+ copies the HTML files from the build directory to the repository\n",
    "+ adds, commits, and pushes changes to GitHub \n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# Generate a new SSH key (replace with your email)\n",
    "ssh-keygen -t ed25519 -C \"muzaale@gmail.com\" -f ~/.ssh/id_befe\n",
    "be \n",
    "be \n",
    "cat ~/.ssh/id_befe.pub\n",
    "# Start the ssh-agent and add the key\n",
    "eval \"$(ssh-agent -s)\"\n",
    "ssh-add ~/.ssh/id_befe\n",
    "be\n",
    "# Copy the public key to your clipboard\n",
    "pbcopy < ~/.ssh/id_befe.pub\n",
    "echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "\n",
    "# Define arrays for acts and the number of files for each act\n",
    "acts=(\"be_0\" \"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\" \"be_6\")\n",
    "act_files=(2 3 4 5 6 7 8)\n",
    "\n",
    "# Create Act directories and their corresponding files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    mkdir -p ${acts[$i]}\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        touch \"${acts[$i]}/be_i$(($i + 1))_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _toc.yml file\n",
    "toc_file=\"_toc.yml\"\n",
    "echo \"\" > $toc_file\n",
    "\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    echo \"- part: Part $(($i + 1))\" >> $toc_file\n",
    "    echo \"  chapters:\" >> $toc_file\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        echo \"    - file: ${acts[$i]}/be_i$(($i + 1))_$j\" >> $toc_file\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create __config.yml file\n",
    "config_file=\"__config.yml\"\n",
    "echo \"title: Your Book Title\" > $config_file\n",
    "echo \"author: Your Name\" >> $config_file\n",
    "echo \"logo: logo.png\" >> $config_file\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "jupyter-book build .\n",
    "\n",
    "# Clone the repository\n",
    "git clone git@github.com:muzaale/fe ~/dropbox/1f.ἡἔρις,κ/1.ontology/fe\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology/fe\n",
    "\n",
    "# Create or switch to the gh-pages branch\n",
    "git checkout -b gh-pages\n",
    "\n",
    "# Copy the HTML files from the build directory to the repository\n",
    "cp -r ~/dropbox/1f.ἡἔρις,κ/1.ontology/be/_build/html/* .\n",
    "\n",
    "# Add, commit, and push changes to GitHub\n",
    "git add .\n",
    "git commit -m \"0.0 Update Jupyter Book content\"\n",
    "git push -u origin gh-pages\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to muzaale/fe repository!\"\n",
    "\n",
    "```\n",
    "\n",
    "### 886. ip.sh-i3\n",
    "\n",
    "Certainly! Here's the entire updated script, with the modification to check if the SSH keys already exist:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# Check if SSH keys already exist, and if not, generate a new one\n",
    "SSH_KEY_PATH=\"$HOME/.ssh/id_befe\"\n",
    "if [ ! -f \"$SSH_KEY_PATH\" ]; then\n",
    "    ssh-keygen -t ed25519 -C \"muzaale@gmail.com\" -f ~/.ssh/id_befe\n",
    "    # Additional commands if needed\n",
    "    cat ~/.ssh/id_befe.pub\n",
    "    # Start the ssh-agent and add the key\n",
    "    eval \"$(ssh-agent -s)\"\n",
    "    ssh-add ~/.ssh/id_befe\n",
    "    # Copy the public key to your clipboard\n",
    "    pbcopy < ~/.ssh/id_befe.pub\n",
    "    echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "else\n",
    "    echo \"SSH keys already exist for this repository. Skipping key generation.\"\n",
    "fi\n",
    "\n",
    "# Define arrays for acts and the number of files for each act\n",
    "acts=(\"be_0\" \"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\" \"be_6\")\n",
    "act_files=(2 3 4 5 6 7 8)\n",
    "\n",
    "# Create Act directories and their corresponding files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    mkdir -p ${acts[$i]}\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        touch \"${acts[$i]}/be_i$(($i + 1))_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _toc.yml file\n",
    "toc_file=\"_toc.yml\"\n",
    "echo \"\" > $toc_file\n",
    "\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    echo \"- part: Part $(($i + 1))\" >> $toc_file\n",
    "    echo \"  chapters:\" >> $toc_file\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        echo \"    - file: ${acts[$i]}/be_i$(($i + 1))_$j\" >> $toc_file\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _config.yml file\n",
    "config_file=\"_config.yml\"\n",
    "echo \"title: Your Book Title\" > $config_file\n",
    "echo \"author: Your Name\" >> $config_file\n",
    "echo \"logo: logo.png\" >> $config_file\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "jupyter-book build .\n",
    "\n",
    "# Clone the repository\n",
    "git clone git@github.com:muzaale/fe ~/dropbox/1f.ἡἔρις,κ/1.ontology/fe\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology/fe\n",
    "\n",
    "# Create or switch to the gh-pages branch\n",
    "git checkout -b gh-pages\n",
    "\n",
    "# Copy the HTML files from the build directory to the repository\n",
    "cp -r ~/dropbox/1f.ἡἔρις,κ/1.ontology/be/_build/html/* .\n",
    "\n",
    "# Add, commit, and push changes to GitHub\n",
    "git add .\n",
    "git commit -m \"0.0 Update Jupyter Book content\"\n",
    "git push -u origin gh-pages\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to muzaale/fe repository!\"\n",
    "```\n",
    "\n",
    "Note: I've fixed the name of the config file to `_config.yml`, as mentioned in your previous comment.\n",
    "\n",
    "--- \n",
    "\n",
    "```\n",
    "(base) d@Poseidon 1.ontology % ./ip.sh              \n",
    "SSH keys already exist for this repository. Skipping key generation.\n",
    "Running Jupyter-Book v0.15.1\n",
    "Traceback (most recent call last):\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/jupyter_book/cli/main.py\", line 242, in build\n",
    "    parse_toc_yaml(toc)\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sphinx_external_toc/parsing.py\", line 90, in parse_toc_yaml\n",
    "    return parse_toc_data(data)\n",
    "           ^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sphinx_external_toc/parsing.py\", line 100, in parse_toc_data\n",
    "    raise MalformedError(f\"toc is not a mapping: {type(data)}\")\n",
    "sphinx_external_toc.parsing.MalformedError: toc is not a mapping: <class 'list'>\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/bin/jupyter-book\", line 8, in <module>\n",
    "    sys.exit(main())\n",
    "             ^^^^^^\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/click/core.py\", line 1130, in __call__\n",
    "    return self.main(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/click/core.py\", line 1055, in main\n",
    "    rv = self.invoke(ctx)\n",
    "         ^^^^^^^^^^^^^^^^\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/click/core.py\", line 1657, in invoke\n",
    "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
    "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/click/core.py\", line 1404, in invoke\n",
    "    return ctx.invoke(self.callback, **ctx.params)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/click/core.py\", line 760, in invoke\n",
    "    return __callback(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/jupyter_book/cli/main.py\", line 244, in build\n",
    "    _error(\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/jupyter_book/utils.py\", line 48, in _error\n",
    "    raise kind(box)\n",
    "RuntimeError: \n",
    "===============================================================================\n",
    "\n",
    "The Table of Contents file is malformed: toc is not a mapping: <class 'list'>\n",
    "You may need to migrate from the old format, using:\n",
    "\n",
    "        jupyter-book toc migrate /Users/d/Dropbox (Personal)/1f.ἡἔρις,κ/1.ontology/be/_toc.yml -o /Users/d/Dropbox (Personal)/1f.ἡἔρις,κ/1.ontology/be/_toc.yml\n",
    "\n",
    "===============================================================================\n",
    "\n",
    "Cloning into '/Users/d/dropbox/1f.ἡἔρις,κ/1.ontology/fe'...\n",
    "warning: You appear to have cloned an empty repository.\n",
    "Switched to a new branch 'gh-pages'\n",
    "cp: /Users/d/dropbox/1f.ἡἔρις,κ/1.ontology/be/_build/html/*: No such file or directory\n",
    "On branch gh-pages\n",
    "\n",
    "Initial commit\n",
    "\n",
    "nothing to commit (create/copy files and use \"git add\" to track)\n",
    "error: src refspec gh-pages does not match any\n",
    "error: failed to push some refs to 'github.com:muzaale/fe'\n",
    "Jupyter Book content updated and pushed to muzaale/fe repository!\n",
    "(base) d@Poseidon 1.ontology % \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "The error you're experiencing seems to originate from the `_toc.yml` file format. The error message indicates:\n",
    "\n",
    "```\n",
    "sphinx_external_toc.parsing.MalformedError: toc is not a mapping: <class 'list'>\n",
    "```\n",
    "\n",
    "From the `_toc.yml` contents you've shown, the format seems to be a list, which is the default for newer versions of Jupyter Book. This suggests that the version of Jupyter Book you're using expects the older format, where the `_toc.yml` file starts with a dictionary (or \"mapping\") rather than a list.\n",
    "\n",
    "Here's what you can do:\n",
    "\n",
    "1. **Migrate the Table of Contents**  \n",
    "   The error message suggests you may need to migrate the `_toc.yml` file using the following command:  \n",
    "   ```\n",
    "   jupyter-book toc migrate /Users/d/Dropbox (Personal)/1f.ἡἔρις,κ/1.ontology/be/_toc.yml -o /Users/d/Dropbox (Personal)/1f.ἡἔρις,κ/1.ontology/be/_toc.yml\n",
    "   ```\n",
    "\n",
    "2. **Check the Jupyter Book Version**  \n",
    "   Ensure you're using a version of Jupyter Book that's compatible with the `_toc.yml` format you're employing. If you're using an older version of Jupyter Book, consider updating it:\n",
    "   ```\n",
    "   pip install -U jupyter-book\n",
    "   ```\n",
    "\n",
    "3. **Manually Adjust the _toc.yml File**  \n",
    "   If you're sure you're using a version of Jupyter Book that expects the newer format, then your `_toc.yml` appears to be correct. However, if you need the older format, you'll need to nest everything under a `root` key:\n",
    "   ```yaml\n",
    "   root:\n",
    "     - part: Part 1\n",
    "       chapters:\n",
    "         - file: be_0/be_i1_1\n",
    "         ...\n",
    "   ```\n",
    "\n",
    "4. **Ensure Correct Directory Structure**  \n",
    "   Verify that the paths listed in `_toc.yml` (like `be_0/be_i1_1`) correspond to existing files or directories. If the files do not exist, the build process will fail.\n",
    "\n",
    "5. **Errors during Repository Operations**  \n",
    "   The script also seems to have issues during the Git operations. Specifically, it tries to push to a branch that doesn't exist in the repository. Make sure the repository is correctly initialized and the branch you're trying to push to (in this case, `gh-pages`) exists.\n",
    "\n",
    "6. **File Copy Error**  \n",
    "   The script tries to copy files from a directory (`be/_build/html/*`) that may not exist. Ensure that the `jupyter-book build .` command runs successfully before the script attempts to copy files.\n",
    "\n",
    "After making these adjustments, re-run the script and see if it successfully builds and pushes the Jupyter Book.\n",
    "\n",
    "### 887. ip.sh-i4\n",
    "\n",
    "this one actually works & creates a gh-pages branch, whose link otherwises appears as a 404\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# Check if SSH keys already exist, and if not, generate a new one\n",
    "SSH_KEY_PATH=\"$HOME/.ssh/id_befe\"\n",
    "if [ ! -f \"$SSH_KEY_PATH\" ]; then\n",
    "    ssh-keygen -t ed25519 -C \"muzaale@gmail.com\" -f ~/.ssh/id_befe\n",
    "    # Additional commands if needed\n",
    "    cat ~/.ssh/id_befe.pub\n",
    "    # Start the ssh-agent and add the key\n",
    "    eval \"$(ssh-agent -s)\"\n",
    "    ssh-add ~/.ssh/id_befe\n",
    "    # Copy the public key to your clipboard\n",
    "    pbcopy < ~/.ssh/id_befe.pub\n",
    "    echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "else\n",
    "    echo \"SSH keys already exist for this repository. Skipping key generation.\"\n",
    "fi\n",
    "\n",
    "# Define arrays for acts and the number of files for each act\n",
    "acts=(\"be_0\" \"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\" \"be_6\")\n",
    "act_files=(2 3 4 5 6 7 8)\n",
    "\n",
    "# Create Act directories and their corresponding files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    mkdir -p ${acts[$i]}\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        cp ~/dropbox/1f.ἡἔρις,κ/1.ontology/populate_be.ipynb \"${acts[$i]}/be_i$(($i + 1))_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _toc.yml file\n",
    "toc_file=\"_toc.yml\"\n",
    "echo \"format: jb-book\" > $toc_file\n",
    "echo \"root: intro.ipynb\" >> $toc_file\n",
    "echo \"title: Play\" >> $toc_file\n",
    "echo \"parts:\" >> $toc_file\n",
    "\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    echo \"  - caption: Part $(($i + 1))\" >> $toc_file\n",
    "    echo \"    chapters:\" >> $toc_file\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        echo \"      - file: ${acts[$i]}/be_i$(($i + 1))_$j\" >> $toc_file\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _config.yml file\n",
    "config_file=\"_config.yml\"\n",
    "echo \"title: Your Book Title\" > $config_file\n",
    "echo \"author: Your Name\" >> $config_file\n",
    "echo \"logo: logo.png\" >> $config_file\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "cd ..\n",
    "jb build $root_dir\n",
    "git clone https://github.com/muzaale/fe \n",
    "cp -r be/* fe\n",
    "cd fe \n",
    "git add ./*\n",
    "git commit -m \"1.1 Update Jupyter Book content\"\n",
    "chmod 600 ~/.ssh/id_befe\n",
    "git remote -v\n",
    "ssh-add -D\n",
    "git remote set-url origin git@github.com:muzaale/fe\n",
    "ssh-add ~/.ssh/id_befe\n",
    "be\n",
    "git push -u origin main\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to muzaale/fe repository!\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "It looks like you're working with a bash script to create directories and files for a Jupyter Book, configure SSH keys, and push the changes to a GitHub repository. This is quite a comprehensive script!\n",
    "\n",
    "Here's a breakdown of some of the main parts of the output:\n",
    "\n",
    "1. **SSH Key Generation**: Your script first checks whether an SSH key exists, and if not, it generates a new one. In this run, it found the existing keys, so it skipped the generation.\n",
    "\n",
    "2. **Building the Jupyter Book**: You attempted to build the book using Jupyter Book, but an error occurred. The error message indicates that the root file `intro.rst` was not found. You'll need to fix this before proceeding. Ensure that the `intro.rst` file exists at the specified path or update the path accordingly in your configuration.\n",
    "\n",
    "3. **Cloning the Repository**: Your script then cloned the repository `https://github.com/muzaale/fe`, copied the content of the Jupyter Book to the cloned repository, and committed the changes.\n",
    "\n",
    "4. **Pushing the Changes**: You updated the remote URL to use SSH and then pushed the changes to the `main` branch. You also used `ghp-import` to push the build HTML, perhaps to update the GitHub Pages site.\n",
    "\n",
    "5. **Bash Error**: The last line in your script is `be`, which is causing an error because it's not recognized as a command. You'll want to investigate what this line is intended to do and correct it.\n",
    "\n",
    "It seems that you've included the script itself at the end of the output. This gives a clear understanding of what you are trying to accomplish. Your script is setting up directories and files, building a Jupyter Book, cloning a repository, and pushing the updates.\n",
    "\n",
    "Here's what you might want to consider doing next:\n",
    "\n",
    "- Fix the issue with the missing `intro.rst` file and try building the Jupyter Book again.\n",
    "- Investigate the `be` line in your script to determine what it's supposed to do and correct it.\n",
    "- Check the repository to make sure the updates were pushed correctly.\n",
    "- If you continue to encounter problems, breaking the script down into smaller parts and running them individually might make troubleshooting easier.\n",
    "\n",
    "Feel free to provide more specific questions if you need help with any particular part!\n",
    "\n",
    "---\n",
    "\n",
    "Certainly! It looks like there was an error during the Jupyter Book build process, as the root file `intro.rst` was not found. Based on your `_toc.yml` file, it seems like you're referring to an `intro.ipynb` file as the root. You may want to ensure that the file exists.\n",
    "\n",
    "Here's the updated script based on what you shared. I've corrected the root in the `_toc.yml` and updated the process accordingly. If there's an error related to the missing `intro.ipynb` file, you'll want to create or specify that file in the appropriate location.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/Dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# Check if SSH keys already exist, and if not, generate a new one\n",
    "SSH_KEY_PATH=\"$HOME/.ssh/id_befe\"\n",
    "if [ ! -f \"$SSH_KEY_PATH\" ]; then\n",
    "    ssh-keygen -t ed25519 -C \"muzaale@gmail.com\" -f ~/.ssh/id_befe\n",
    "    # Additional commands if needed\n",
    "    cat ~/.ssh/id_befe.pub\n",
    "    # Start the ssh-agent and add the key\n",
    "    eval \"$(ssh-agent -s)\"\n",
    "    ssh-add ~/.ssh/id_befe\n",
    "    # Copy the public key to your clipboard\n",
    "    pbcopy < ~/.ssh/id_befe.pub\n",
    "    echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "else\n",
    "    echo \"SSH keys already exist for this repository. Skipping key generation.\"\n",
    "fi\n",
    "\n",
    "# Define arrays for acts and the number of files for each act\n",
    "acts=(\"be_0\" \"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\" \"be_6\")\n",
    "act_files=(2 3 4 5 6 7 8)\n",
    "\n",
    "# Create Act directories and their corresponding files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    mkdir -p ${acts[$i]}\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        cp ~/Dropbox/1f.ἡἔρις,κ/1.ontology/populate_be.ipynb \"${acts[$i]}/be_i$(($i + 1))_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _toc.yml file\n",
    "toc_file=\"_toc.yml\"\n",
    "echo \"format: jb-book\" > $toc_file\n",
    "echo \"root: intro.ipynb\" >> $toc_file # Make sure this file exists\n",
    "echo \"title: Play\" >> $toc_file\n",
    "echo \"parts:\" >> $toc_file\n",
    "\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    echo \"  - caption: Part $(($i + 1))\" >> $toc_file\n",
    "    echo \"    chapters:\" >> $toc_file\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        echo \"      - file: ${acts[$i]}/be_i$(($i + 1))_$j\" >> $toc_file\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _config.yml file\n",
    "config_file=\"_config.yml\"\n",
    "echo \"title: Your Book Title\" > $config_file\n",
    "echo \"author: Your Name\" >> $config_file\n",
    "echo \"logo: logo.png\" >> $config_file\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "cd ..\n",
    "jb build $root_dir\n",
    "git clone https://github.com/muzaale/fe \n",
    "cp -r be/* fe\n",
    "cd fe \n",
    "git add ./*\n",
    "git commit -m \"1.1 Update Jupyter Book content\"\n",
    "chmod 600 ~/.ssh/id_befe\n",
    "git remote -v\n",
    "ssh-add -D\n",
    "git remote set-url origin git@github.com:muzaale/fe\n",
    "ssh-add ~/.ssh/id_befe\n",
    "git push -u origin main\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to muzaale/fe repository!\"\n",
    "```\n",
    "\n",
    "Please make sure that the `intro.ipynb` file exists or update the `root` entry in `_toc.yml` to point to the correct root file for your book.\n",
    "\n",
    "### 888. ip.sh-i6\n",
    "\n",
    "victory!\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/Dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# Create the intro.ipynb file from populate_be.ipynb\n",
    "cp ~/Dropbox/1f.ἡἔρις,κ/1.ontology/populate_be.ipynb intro.ipynb\n",
    "\n",
    "# Check if SSH keys already exist, and if not, generate a new one\n",
    "SSH_KEY_PATH=\"$HOME/.ssh/id_befe\"\n",
    "if [ ! -f \"$SSH_KEY_PATH\" ]; then\n",
    "    ssh-keygen -t ed25519 -C \"muzaale@gmail.com\" -f ~/.ssh/id_befe\n",
    "    cat ~/.ssh/id_befe.pub\n",
    "    eval \"$(ssh-agent -s)\"\n",
    "    ssh-add ~/.ssh/id_befe\n",
    "    pbcopy < ~/.ssh/id_befe.pub\n",
    "    echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "else\n",
    "    echo \"SSH keys already exist for this repository. Skipping key generation.\"\n",
    "fi\n",
    "\n",
    "# Define arrays for acts and the number of files for each act\n",
    "acts=(\"be_0\" \"be_1\" \"be_2\" \"be_3\" \"be_4\" \"be_5\" \"be_6\")\n",
    "act_files=(2 3 4 5 6 7 8)\n",
    "\n",
    "# Create _toc.yml file\n",
    "toc_file=\"_toc.yml\"\n",
    "echo \"format: jb-book\" > $toc_file\n",
    "echo \"root: intro.ipynb\" >> $toc_file\n",
    "echo \"title: Play\" >> $toc_file\n",
    "echo \"parts:\" >> $toc_file\n",
    "\n",
    "# Create Act directories and their corresponding files\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    mkdir -p ${acts[$i]}\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        cp ~/Dropbox/1f.ἡἔρις,κ/1.ontology/populate_be.ipynb \"${acts[$i]}/be_i$(($i + 1))_$j.ipynb\"\n",
    "    done\n",
    "done\n",
    "\n",
    "for ((i=0; i<${#acts[@]}; i++)); do\n",
    "    echo \"  - caption: Part $(($i + 1))\" >> $toc_file\n",
    "    echo \"    chapters:\" >> $toc_file\n",
    "    for ((j=1; j<=${act_files[$i]}; j++)); do\n",
    "        echo \"      - file: ${acts[$i]}/be_i$(($i + 1))_$j\" >> $toc_file\n",
    "    done\n",
    "done\n",
    "\n",
    "# Create _config.yml file\n",
    "config_file=\"_config.yml\"\n",
    "echo \"title: Your Book Title\" > $config_file\n",
    "echo \"author: Your Name\" >> $config_file\n",
    "echo \"logo: logo.png\" >> $config_file\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "cd ..\n",
    "jb build $root_dir\n",
    "git clone https://github.com/muzaale/fe \n",
    "cp -r be/* fe\n",
    "cd fe \n",
    "git add ./*\n",
    "git commit -m \"1.1 Update Jupyter Book content\"\n",
    "chmod 600 ~/.ssh/id_befe\n",
    "git remote -v\n",
    "ssh-add -D\n",
    "git remote set-url origin git@github.com:muzaale/fe\n",
    "ssh-add ~/.ssh/id_befe\n",
    "git push -u origin main\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to muzaale/fe repository!\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 889. ip.sh-spjd\n",
    "\n",
    "The revolution will be televised!\n",
    "\n",
    "---\n",
    "\n",
    "Certainly! Below is a modified version of the script that generalizes the GitHub username, repository name, and email address by taking them as inputs at the beginning of the script. This way, the script can be used for different users and repositories.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Input GitHub username, repository, and email address\n",
    "read -p \"Enter your GitHub username: \" GITHUB_USERNAME\n",
    "read -p \"Enter your GitHub repository name: \" REPO_NAME\n",
    "read -p \"Enter your email address: \" EMAIL_ADDRESS\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/Dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "mkdir -p $root_dir\n",
    "cd $root_dir\n",
    "\n",
    "# Create the intro.ipynb file from populate_be.ipynb\n",
    "cp ~/Dropbox/1f.ἡἔρις,κ/1.ontology/populate_be.ipynb intro.ipynb\n",
    "\n",
    "# Check if SSH keys already exist, and if not, generate a new one\n",
    "SSH_KEY_PATH=\"$HOME/.ssh/id_befe\"\n",
    "if [ ! -f \"$SSH_KEY_PATH\" ]; then\n",
    "    ssh-keygen -t ed25519 -C \"$EMAIL_ADDRESS\" -f ~/.ssh/id_befe\n",
    "    cat ~/.ssh/id_befe.pub\n",
    "    eval \"$(ssh-agent -s)\"\n",
    "    ssh-add ~/.ssh/id_befe\n",
    "    pbcopy < ~/.ssh/id_befe.pub\n",
    "    echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "else\n",
    "    echo \"SSH keys already exist for this repository. Skipping key generation.\"\n",
    "fi\n",
    "\n",
    "# ... rest of the code is unchanged ...\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "cd ..\n",
    "jb build $root_dir\n",
    "git clone \"https://github.com/$GITHUB_USERNAME/$REPO_NAME\"\n",
    "cp -r be/* $REPO_NAME\n",
    "cd $REPO_NAME\n",
    "git add ./*\n",
    "git commit -m \"1.1 Update Jupyter Book content\"\n",
    "chmod 600 ~/.ssh/id_befe\n",
    "git remote -v\n",
    "ssh-add -D\n",
    "git remote set-url origin \"git@github.com:$GITHUB_USERNAME/$REPO_NAME\"\n",
    "ssh-add ~/.ssh/id_befe\n",
    "git push -u origin main\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to $GITHUB_USERNAME/$REPO_NAME repository!\"\n",
    "```\n",
    "\n",
    "This script will prompt the user to enter their GitHub username, repository name, and email address, and then it will proceed to use those values throughout the script. Make sure that the user has appropriate permissions to the repository, and that the repository exists on GitHub.\n",
    "\n",
    "### 890. spjdworkflow\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Input GitHub username, repository, and email address\n",
    "read -p \"Enter your GitHub username: \" GITHUB_USERNAME\n",
    "read -p \"Enter your GitHub repository name: \" REPO_NAME\n",
    "read -p \"Enter your email address: \" EMAIL_ADDRESS\n",
    "\n",
    "# Set up directories and paths\n",
    "cd ~/Dropbox/1f.ἡἔρις,κ/1.ontology/\n",
    "root_dir=\"be\"\n",
    "\n",
    "# mkdir -p $root_dir\n",
    "# cd $root_dir\n",
    "\n",
    "# Create the intro.ipynb file from populate_be.ipynb\n",
    "# cp ~/Dropbox/1f.ἡἔρις,κ/1.ontology/populate_be.ipynb intro.ipynb\n",
    "\n",
    "# Check if SSH keys already exist, and if not, generate a new one\n",
    "# SSH_KEY_PATH=\"$HOME/.ssh/id_befe\"\n",
    "# if [ ! -f \"$SSH_KEY_PATH\" ]; then\n",
    "#   ssh-keygen -t ed25519 -C \"$EMAIL_ADDRESS\" -f ~/.ssh/id_befe\n",
    "#   cat ~/.ssh/id_befe.pub\n",
    "#   eval \"$(ssh-agent -s)\"\n",
    "#   ssh-add ~/.ssh/id_befe\n",
    "#   pbcopy < ~/.ssh/id_befe.pub\n",
    "#   echo \"SSH public key copied to clipboard. Please add it to your GitHub account's SSH keys.\"\n",
    "# else\n",
    "#   echo \"SSH keys already exist for this repository. Skipping key generation.\"\n",
    "# fi\n",
    "\n",
    "# ... rest of the code is unchanged ...\n",
    "\n",
    "# Build the book with Jupyter Book\n",
    "# cd ..\n",
    "jb build $root_dir\n",
    "# git clone \"https://github.com/$GITHUB_USERNAME/$REPO_NAME\"\n",
    "cp -r be/* $REPO_NAME\n",
    "cd $REPO_NAME\n",
    "git add ./*\n",
    "git commit -m \"spjd workflow\"\n",
    "chmod 600 ~/.ssh/id_befe\n",
    "git remote -v\n",
    "ssh-add -D\n",
    "git remote set-url origin \"git@github.com:$GITHUB_USERNAME/$REPO_NAME\"\n",
    "ssh-add ~/.ssh/id_befe\n",
    "git push -u origin main\n",
    "ghp-import -n -p -f _build/html\n",
    "\n",
    "echo \"Jupyter Book content updated and pushed to $GITHUB_USERNAME/$REPO_NAME repository!\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 891. ideas\n",
    "\n",
    "```{note} 1.1.1 8:55 PM\n",
    "1. Facebook - comparing two girls \n",
    "2. Instagram -number of followers\n",
    "3. TikTok- ability to go viral \n",
    "4. Yafe - how many clinicians endorse \n",
    "5. Philosophe - risk calculator(s)\n",
    "\n",
    "```\n",
    "\n",
    "```{note} 1.1.2\n",
    " \n",
    "1. Clinical question\n",
    "2. Variables \n",
    "3. Data source \n",
    "4. Regression \n",
    "5. WebApp\n",
    "6. Fena hosting \n",
    "7. `Turnaround`: a week \n",
    "8. Demo & workflow using Python/nhanes \n",
    "9. Hopkins doesn’t care for the weights \n",
    "10. Represent public health issue \n",
    "11. After clinical variance \n",
    "12. Hierarchical models \n",
    "13. See fena for\n",
    "14. Details \n",
    "15. \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}